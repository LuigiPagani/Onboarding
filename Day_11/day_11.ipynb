{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Design: Formulate a bandit problem for retrieval in a QA system. Say you have two document indexes (DocsA and DocsB) and also the option to not retrieve at all. How would you set up: contexts (features of the query, like length or topic), actions (which index or none), and reward (e.g. +1 if the answer was correct and -0.1 per document retrieved to penalize cost). Is this reward structure multi-objective (accuracy vs. cost)? How would you incorporate the cost in a bandit reward or would you treat it separately?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Coding: Given a small QA dataset and two retrieval strategies (e.g., keyword search vs. dense embedding search), simulate an adaptive retrieval policy. Implement a simple bandit (like $\\epsilon$-greedy or Thompson Sampling) that chooses strategy per question and receives a reward of 1 if the retrieved set contained the answer. Over many questions, watch the bandit’s strategy selection proportions. Does it learn which strategy is generally better? Now, introduce a context feature: e.g., questions containing dates might do better with keyword search. Modify the bandit to be contextual (e.g. a LinUCB on a feature like “has date or number”). See if it learns a policy: keyword for date queries, dense for others.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023fe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Future Thinking: In Search-R1, an LLM is trained to decide when to call a search API during its reasoning. Why is this a RL problem and not a supervised one? (Hint: the optimal points to call search aren’t known in the training data; the model must discover them by trial and error, guided by a reward like final answer correctness.) If you were to integrate this into Sqwish, what kind of feedback signal could train such behavior? Describe a possible reward function for an LLM agent that can either answer directly or decide to issue a search query and then answer. How would you ensure it doesn’t over-use the search (to minimize latency) or under-use it (and risk being wrong)?\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c2105",
   "metadata": {},
   "source": [
    "The **Search-R1** framework treats the decision of *when* and *how* to search as an RL problem because the optimal search trajectory is not labeled. In supervised fine-tuning, you would need ground-truth sequences of tool calls, which are typically unavailable.\n",
    "\n",
    "RL allows the agent to explore search calls and learn from an outcome-based signal: if the final answer is correct, actions along the trajectory receive positive feedback.\n",
    "\n",
    "A simple reward design could be:\n",
    "\n",
    "- **Outcome reward**: +1 for a correct answer, 0 for an incorrect one.\n",
    "- **Search cost**: penalize each search call (e.g., -0.1 per call) or use a latency-based penalty to discourage over-use.\n",
    "\n",
    "One example total reward is:\n",
    "\n",
    "- If correct: \\(r = 1 - 0.1 \\cdot N\\)\n",
    "- If incorrect: \\(r = -0.1 \\cdot N\\)\n",
    "\n",
    "where \\(N\\) is the number of search calls.\n",
    "\n",
    "By tuning the search-call penalty, the agent can learn to search only when necessary: a high penalty encourages answering from memory, while a low penalty encourages more frequent search. In a production system like Sqwish, you could further incorporate user feedback (thumbs-up/down) as an additional reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250cc7c",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Connection to Context Engineering: Consider ACE's playbook (Day 10) as learned retrieval context, instead of retrieving from a static corpus, the system builds its own knowledge base through experience. How might you combine ACE with traditional RAG? One approach: use RAG for factual, up-to-date information while using an ACE playbook for procedural knowledge (strategies, patterns, failure modes). The playbook could even store meta-knowledge about retrieval itself e.g., \"For financial questions, retrieve from SEC filings first.\" Design a hybrid architecture.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3de17a",
   "metadata": {},
   "source": [
    "Hybrid architecture: use RAG for facts and an ACE playbook for procedures.\n",
    "\n",
    "- RAG: factual grounding (internal docs, web, filings).\n",
    "- ACE playbook: reusable procedures/heuristics (e.g., “SEC first for finance”, “cross-check numerics”).\n",
    "\n",
    "Runtime:\n",
    "\n",
    "1. Extract query features (domain, has numbers/dates, risk, latency budget).\n",
    "2. Retrieve relevant playbook entries (procedure) and retrieve evidence from RAG (facts).\n",
    "3. Router/planner (the “controller” for RAG) chooses a retrieval plan: sources, retrievers, top-k, stop rules, budget.\n",
    "4. Execute the plan, then answer using evidence + the playbook checklist.\n",
    "\n",
    "Learning: log outcomes + cost/latency, update the router policy from rewards (accuracy vs latency), and distill good runs into new/edited playbook rules (generate → reflect → curate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea1c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
