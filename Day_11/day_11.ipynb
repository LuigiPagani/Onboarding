{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Design: Formulate a bandit problem for retrieval in a QA system. Say you have two document indexes (DocsA and DocsB) and also the option to not retrieve at all. How would you set up: contexts (features of the query, like length or topic), actions (which index or none), and reward (e.g. +1 if the answer was correct and -0.1 per document retrieved to penalize cost). Is this reward structure multi-objective (accuracy vs. cost)? How would you incorporate the cost in a bandit reward or would you treat it separately?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Coding: Given a small QA dataset and two retrieval strategies (e.g., keyword search vs. dense embedding search), simulate an adaptive retrieval policy. Implement a simple bandit (like $\\epsilon$-greedy or Thompson Sampling) that chooses strategy per question and receives a reward of 1 if the retrieved set contained the answer. Over many questions, watch the bandit’s strategy selection proportions. Does it learn which strategy is generally better? Now, introduce a context feature: e.g., questions containing dates might do better with keyword search. Modify the bandit to be contextual (e.g. a LinUCB on a feature like “has date or number”). See if it learns a policy: keyword for date queries, dense for others.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1023fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ε-greedy (non-contextual) ===\n",
      "Estimated mean net reward Q: [0.52777778 0.54788584]\n",
      "Action counts: keyword = 270 , dense = 4730\n",
      "Mean success rate: 0.6914\n",
      "Mean net reward  : 0.5468\n",
      "\n",
      "\n",
      "=== LinUCB (contextual, block features) ===\n",
      "Action counts: keyword = 1770 , dense = 3230\n",
      "Mean success rate: 0.7642\n",
      "Mean net reward  : 0.6496\n",
      "\n",
      "--- Conditional policy behavior ---\n",
      "P(keyword | has_number=True ) = 0.9988706945228685\n",
      "P(dense   | has_number=False) = 0.9996903065964695\n",
      "\n",
      "--- Overall selection proportions ---\n",
      "Overall keyword share: 0.354\n",
      "Overall dense share  : 0.646\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Data simulation (Exercise 2)\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "num_questions = 5000\n",
    "has_number = np.random.rand(num_questions) < 0.35  # 35% queries contain numbers/dates\n",
    "\n",
    "# Two retrieval strategies (arms):\n",
    "# 0 = keyword search, 1 = dense search\n",
    "def success_prob(strategy: int, hn: bool) -> float:\n",
    "    if strategy == 0:   # keyword\n",
    "        return 0.78 if hn else 0.45\n",
    "    else:               # dense\n",
    "        return 0.62 if hn else 0.74\n",
    "\n",
    "def retrieval_cost(strategy: int) -> float:\n",
    "    # dense is more expensive (latency/compute), so it must be more accurate to win\n",
    "    return 0.05 if strategy == 0 else 0.15\n",
    "\n",
    "def simulate_outcome(strategy: int, hn: bool):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      success: 1.0 if retrieved set contained the answer else 0.0\n",
    "      net_reward: success - cost  (cost-aware reward; matches the exercise's \"penalize retrieval\" idea)\n",
    "    \"\"\"\n",
    "    p = success_prob(strategy, hn)\n",
    "    success = 1.0 if (np.random.rand() < p) else 0.0\n",
    "    net_reward = success - retrieval_cost(strategy)\n",
    "    return success, net_reward\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Part A: Non-contextual bandit (ε-greedy)\n",
    "# -----------------------------------------\n",
    "epsilon = 0.1\n",
    "K = 2\n",
    "\n",
    "Q = np.zeros(K)         # estimated mean net reward per arm\n",
    "N = np.zeros(K, dtype=int)\n",
    "\n",
    "eg_actions = []\n",
    "eg_successes = []\n",
    "eg_net_rewards = []\n",
    "\n",
    "for t in range(num_questions):\n",
    "    # Warm start: try each arm once\n",
    "    if t < K:\n",
    "        a = t\n",
    "    else:\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(K)\n",
    "        else:\n",
    "            a = int(np.argmax(Q))\n",
    "\n",
    "    s, r = simulate_outcome(a, bool(has_number[t]))\n",
    "    N[a] += 1\n",
    "    Q[a] += (r - Q[a]) / N[a]  # incremental mean update\n",
    "\n",
    "    eg_actions.append(a)\n",
    "    eg_successes.append(s)\n",
    "    eg_net_rewards.append(r)\n",
    "\n",
    "eg_actions = np.array(eg_actions)\n",
    "eg_successes = np.array(eg_successes)\n",
    "eg_net_rewards = np.array(eg_net_rewards)\n",
    "\n",
    "print(\"=== ε-greedy (non-contextual) ===\")\n",
    "print(\"Estimated mean net reward Q:\", Q)\n",
    "print(\"Action counts: keyword =\", np.sum(eg_actions == 0), \", dense =\", np.sum(eg_actions == 1))\n",
    "print(\"Mean success rate:\", float(np.mean(eg_successes)))\n",
    "print(\"Mean net reward  :\", float(np.mean(eg_net_rewards)))\n",
    "print()\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Part B: Contextual bandit (LinUCB)\n",
    "# -----------------------------------\n",
    "# Import the LinUCB implementation from Day 1.\n",
    "# We use block features (same idea as Day 3) so each arm has its own linear model.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(\"/home/luigi/Programming/Onboarding\")\n",
    "sys.path.append(str(REPO_ROOT / \"Day_1\"))\n",
    "from linucb import LinUCBAgent\n",
    "\n",
    "K = 2\n",
    "base_d = 2\n",
    "D = K * base_d  # block-feature dimension\n",
    "\n",
    "linucb = LinUCBAgent(d=D, alpha=0.2, lam=1.0)\n",
    "\n",
    "ucb_actions = []\n",
    "ucb_successes = []\n",
    "ucb_net_rewards = []\n",
    "\n",
    "for t in range(num_questions):\n",
    "    # Base context feature vector: [1, has_number]\n",
    "    x_base = np.array([1.0, 1.0 if has_number[t] else 0.0], dtype=np.float64)\n",
    "\n",
    "    # Block features: each arm gets its own copy of x_base in a disjoint slice.\n",
    "    X = np.zeros((K, D), dtype=np.float64)\n",
    "    for a in range(K):\n",
    "        X[a, a * base_d : (a + 1) * base_d] = x_base\n",
    "\n",
    "    # Warm start: try each arm once\n",
    "    if t < K:\n",
    "        a = t\n",
    "    else:\n",
    "        a = linucb.select_arm(X)\n",
    "\n",
    "    s, r = simulate_outcome(a, bool(has_number[t]))\n",
    "    linucb.update(X[a], r)\n",
    "\n",
    "    ucb_actions.append(a)\n",
    "    ucb_successes.append(s)\n",
    "    ucb_net_rewards.append(r)\n",
    "\n",
    "ucb_actions = np.array(ucb_actions)\n",
    "ucb_successes = np.array(ucb_successes)\n",
    "ucb_net_rewards = np.array(ucb_net_rewards)\n",
    "\n",
    "hn = has_number\n",
    "\n",
    "print(\"\\n=== LinUCB (contextual, block features) ===\")\n",
    "print(\"Action counts: keyword =\", np.sum(ucb_actions == 0), \", dense =\", np.sum(ucb_actions == 1))\n",
    "print(\"Mean success rate:\", float(np.mean(ucb_successes)))\n",
    "print(\"Mean net reward  :\", float(np.mean(ucb_net_rewards)))\n",
    "\n",
    "# Policy diagnostics requested by the exercise\n",
    "print(\"\\n--- Conditional policy behavior ---\")\n",
    "print(\"P(keyword | has_number=True ) =\", float(np.mean(ucb_actions[hn] == 0)))\n",
    "print(\"P(dense   | has_number=False) =\", float(np.mean(ucb_actions[~hn] == 1)))\n",
    "\n",
    "print(\"\\n--- Overall selection proportions ---\")\n",
    "print(\"Overall keyword share:\", float(np.mean(ucb_actions == 0)))\n",
    "print(\"Overall dense share  :\", float(np.mean(ucb_actions == 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Future Thinking: In Search-R1, an LLM is trained to decide when to call a search API during its reasoning. Why is this a RL problem and not a supervised one? (Hint: the optimal points to call search aren’t known in the training data; the model must discover them by trial and error, guided by a reward like final answer correctness.) If you were to integrate this into Sqwish, what kind of feedback signal could train such behavior? Describe a possible reward function for an LLM agent that can either answer directly or decide to issue a search query and then answer. How would you ensure it doesn’t over-use the search (to minimize latency) or under-use it (and risk being wrong)?\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c2105",
   "metadata": {},
   "source": [
    "The **Search-R1** framework treats the decision of *when* and *how* to search as an RL problem because the optimal search trajectory is not labeled. In supervised fine-tuning, you would need ground-truth sequences of tool calls, which are typically unavailable.\n",
    "\n",
    "RL allows the agent to explore search calls and learn from an outcome-based signal: if the final answer is correct, actions along the trajectory receive positive feedback.\n",
    "\n",
    "A simple reward design could be:\n",
    "\n",
    "- **Outcome reward**: +1 for a correct answer, 0 for an incorrect one.\n",
    "- **Search cost**: penalize each search call (e.g., -0.1 per call) or use a latency-based penalty to discourage over-use.\n",
    "\n",
    "One example total reward is:\n",
    "\n",
    "- If correct: \\(r = 1 - 0.1 \\cdot N\\)\n",
    "- If incorrect: \\(r = -0.1 \\cdot N\\)\n",
    "\n",
    "where \\(N\\) is the number of search calls.\n",
    "\n",
    "By tuning the search-call penalty, the agent can learn to search only when necessary: a high penalty encourages answering from memory, while a low penalty encourages more frequent search. In a production system like Sqwish, you could further incorporate user feedback (thumbs-up/down) as an additional reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250cc7c",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Connection to Context Engineering: Consider ACE's playbook (Day 10) as learned retrieval context, instead of retrieving from a static corpus, the system builds its own knowledge base through experience. How might you combine ACE with traditional RAG? One approach: use RAG for factual, up-to-date information while using an ACE playbook for procedural knowledge (strategies, patterns, failure modes). The playbook could even store meta-knowledge about retrieval itself e.g., \"For financial questions, retrieve from SEC filings first.\" Design a hybrid architecture.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3de17a",
   "metadata": {},
   "source": [
    "Hybrid architecture: use RAG for facts and an ACE playbook for procedures.\n",
    "\n",
    "- RAG: factual grounding (internal docs, web, filings).\n",
    "- ACE playbook: reusable procedures/heuristics (e.g., “SEC first for finance”, “cross-check numerics”).\n",
    "\n",
    "Runtime:\n",
    "\n",
    "1. Extract query features (domain, has numbers/dates, risk, latency budget).\n",
    "2. Retrieve relevant playbook entries (procedure) and retrieve evidence from RAG (facts).\n",
    "3. Router/planner (the “controller” for RAG) chooses a retrieval plan: sources, retrievers, top-k, stop rules, budget.\n",
    "4. Execute the plan, then answer using evidence + the playbook checklist.\n",
    "\n",
    "Learning: log outcomes + cost/latency, update the router policy from rewards (accuracy vs latency), and distill good runs into new/edited playbook rules (generate → reflect → curate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea1c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
