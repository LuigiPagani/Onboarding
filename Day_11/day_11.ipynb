{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Design: Formulate a bandit problem for retrieval in a QA system. Say you have two document indexes (DocsA and DocsB) and also the option to not retrieve at all. How would you set up: contexts (features of the query, like length or topic), actions (which index or none), and reward (e.g. +1 if the answer was correct and -0.1 per document retrieved to penalize cost). Is this reward structure multi-objective (accuracy vs. cost)? How would you incorporate the cost in a bandit reward or would you treat it separately?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Coding: Given a small QA dataset and two retrieval strategies (e.g., keyword search vs. dense embedding search), simulate an adaptive retrieval policy. Implement a simple bandit (like $\\epsilon$-greedy or Thompson Sampling) that chooses strategy per question and receives a reward of 1 if the retrieved set contained the answer. Over many questions, watch the bandit’s strategy selection proportions. Does it learn which strategy is generally better? Now, introduce a context feature: e.g., questions containing dates might do better with keyword search. Modify the bandit to be contextual (e.g. a LinUCB on a feature like “has date or number”). See if it learns a policy: keyword for date queries, dense for others.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023fe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Future Thinking: In Search-R1, an LLM is trained to decide when to call a search API during its reasoning. Why is this a RL problem and not a supervised one? (Hint: the optimal points to call search aren’t known in the training data; the model must discover them by trial and error, guided by a reward like final answer correctness.) If you were to integrate this into Sqwish, what kind of feedback signal could train such behavior? Describe a possible reward function for an LLM agent that can either answer directly or decide to issue a search query and then answer. How would you ensure it doesn’t over-use the search (to minimize latency) or under-use it (and risk being wrong)?\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c2105",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5250cc7c",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Connection to Context Engineering: Consider ACE's playbook (Day 10) as learned retrieval context, instead of retrieving from a static corpus, the system builds its own knowledge base through experience. How might you combine ACE with traditional RAG? One approach: use RAG for factual, up-to-date information while using an ACE playbook for procedural knowledge (strategies, patterns, failure modes). The playbook could even store meta-knowledge about retrieval itself e.g., \"For financial questions, retrieve from SEC filings first.\" Design a hybrid architecture.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3de17a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
