{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7eed45",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18663c",
   "metadata": {},
   "source": [
    "Conceptual: Why does a static preference dataset become insufficient as a policy improves? Explain using an example: initial model outputs are poor, so human preferences cover only easy mistakes; once the model stops making those, the old data is less relevant. How does online DPO address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb28f9",
   "metadata": {},
   "source": [
    "## Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8485ca",
   "metadata": {},
   "source": [
    "A static dataset is **tied to the behavior of the policy that generated it**. As the policy improves, two things happen:\n",
    "\n",
    "1. **The dataset stops matching the policy’s error profile (distribution shift).**\n",
    "   Early on, the model makes *obvious* mistakes, so humans mostly compare **bad vs. decent** outputs. After training, the policy rarely produces the “bad” outputs anymore, so those comparisons are no longer representative of what the policy currently does.\n",
    "\n",
    "2. **The comparisons become weak training signal (saturation).**\n",
    "   DPO updates are driven by the *margin* between preferred and rejected:\n",
    "   $$\n",
    "   \\Delta = \\big(\\log \\pi(y_w|x)-\\log \\pi(y_l|x)\\big) - \\big(\\log \\pi_{\\text{ref}}(y_w|x)-\\log \\pi_{\\text{ref}}(y_l|x)\\big).\n",
    "   $$\n",
    "   If the improved policy already strongly prefers (y_w) over (y_l) on the old dataset, (\\Delta) is large, the logistic loss saturates, and gradients become small. So the old dataset gives diminishing returns.\n",
    "\n",
    "\n",
    "Online DPO fixes the mismatch by making the data **on-policy (or near on-policy)** each iteration:\n",
    "\n",
    "* **Generate candidates from the current policy** (often with some exploration to get diversity).\n",
    "\n",
    "* **Collect fresh preferences** on *these* candidates.\n",
    "\n",
    "* **Update with DPO on the new comparisons**, typically with **anchoring/regularization** to prevent drift.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a240406",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff73d87",
   "metadata": {},
   "source": [
    "Coding: Implement a simple simulation of OFS-DPO. Use two copies of a model (e.g., two sets of parameters representing fast and slow). At each iteration: have the fast model generate an output for a query, label its quality with a simulated metric (e.g., a known reward function or a proxy judge), update fast model via DPO loss, and occasionally copy fast weights to slow. Monitor a metric (like the difference between fast and slow policy outputs or rewards over time) to verify that fast adapts quickly and slow provides a stabilizing anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b0716",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2edfa2",
   "metadata": {},
   "source": [
    "### OFS-DPO (toy) simulation\n",
    "\n",
    "Two policies:\n",
    "- **fast**: updated online from preferences\n",
    "- **slow**: lags behind and stabilizes (copied from fast every `sync_every` steps)\n",
    "\n",
    "We’ll track **average reward** and **fast–slow divergence** to see fast adapt quickly while slow provides an anchor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce853bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reproducible toy environment\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "n_queries = 8\n",
    "n_actions = 6\n",
    "d = 12\n",
    "\n",
    "phi = rng.normal(size=(n_queries, n_actions, d))\n",
    "w_true = rng.normal(size=d)\n",
    "reward = phi @ w_true\n",
    "reward = reward / np.std(reward)\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - z.max()\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum()\n",
    "\n",
    "\n",
    "def probs(theta, x):\n",
    "    return softmax(phi[x] @ theta)\n",
    "\n",
    "\n",
    "def expected_reward(theta):\n",
    "    return float(np.mean([probs(theta, x) @ reward[x] for x in range(n_queries)]))\n",
    "\n",
    "\n",
    "def avg_kl(theta_p, theta_q, eps=1e-12):\n",
    "    kls = []\n",
    "    for x in range(n_queries):\n",
    "        p = np.clip(probs(theta_p, x), eps, 1.0)\n",
    "        q = np.clip(probs(theta_q, x), eps, 1.0)\n",
    "        kls.append(np.sum(p * (np.log(p) - np.log(q))))\n",
    "    return float(np.mean(kls))\n",
    "\n",
    "\n",
    "def sample_two(theta, x):\n",
    "    p = probs(theta, x)\n",
    "    y1, y2 = rng.choice(n_actions, size=2, replace=False, p=p)\n",
    "    return int(y1), int(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFS-DPO loop: fast updates every step; slow syncs periodically\n",
    "beta = 2.0\n",
    "lr = 0.1\n",
    "steps = 3000\n",
    "sync_every = 250\n",
    "log_every = 25\n",
    "noise_std = 0.15  # proxy judge noise\n",
    "\n",
    "theta_fast = np.zeros(d)\n",
    "theta_slow = theta_fast.copy()\n",
    "\n",
    "logs = {\"step\": [], \"r_fast\": [], \"r_slow\": [], \"kl_fs\": [], \"theta_gap\": []}\n",
    "\n",
    "for t in range(1, steps + 1):\n",
    "    x = int(rng.integers(n_queries))\n",
    "\n",
    "    # fast generates two candidates for the same query\n",
    "    y1, y2 = sample_two(theta_fast, x)\n",
    "\n",
    "    # simulated preference judge\n",
    "    r1 = float(reward[x, y1] + rng.normal(scale=noise_std))\n",
    "    r2 = float(reward[x, y2] + rng.normal(scale=noise_std))\n",
    "    y_w, y_l = (y1, y2) if r1 >= r2 else (y2, y1)\n",
    "\n",
    "    # DPO-style anchored update (reference = slow)\n",
    "    dphi = phi[x, y_w] - phi[x, y_l]\n",
    "    m = float(dphi @ (theta_fast - theta_slow))\n",
    "    bm = np.clip(beta * m, -50, 50)\n",
    "    w = 1.0 / (1.0 + np.exp(bm))  # sigmoid(-beta*m)\n",
    "    theta_fast += lr * beta * w * dphi\n",
    "\n",
    "    # occasional slow sync\n",
    "    if t % sync_every == 0:\n",
    "        theta_slow = theta_fast.copy()\n",
    "\n",
    "    if t % log_every == 0:\n",
    "        logs[\"step\"].append(t)\n",
    "        logs[\"r_fast\"].append(expected_reward(theta_fast))\n",
    "        logs[\"r_slow\"].append(expected_reward(theta_slow))\n",
    "        logs[\"kl_fs\"].append(avg_kl(theta_fast, theta_slow))\n",
    "        logs[\"theta_gap\"].append(float(np.linalg.norm(theta_fast - theta_slow)))\n",
    "\n",
    "print(\n",
    "    f\"final  r_fast={logs['r_fast'][-1]:.3f}  r_slow={logs['r_slow'][-1]:.3f}  \"\n",
    "    f\"KL(f||s)={logs['kl_fs'][-1]:.3f}  ||θf-θs||={logs['theta_gap'][-1]:.3f}\"\n",
    ")\n",
    "\n",
    "# Plot if available (otherwise the print above is enough)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "    ax[0].plot(logs[\"step\"], logs[\"r_fast\"], label=\"fast\")\n",
    "    ax[0].plot(logs[\"step\"], logs[\"r_slow\"], label=\"slow\")\n",
    "    ax[0].set_ylabel(\"avg expected reward\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(logs[\"step\"], logs[\"kl_fs\"], label=\"KL(fast||slow)\")\n",
    "    ax[1].plot(logs[\"step\"], logs[\"theta_gap\"], label=\"||θ_fast-θ_slow||\")\n",
    "    ax[1].set_xlabel(\"step\")\n",
    "    ax[1].set_ylabel(\"fast–slow gap\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"plot skipped:\", type(e).__name__, str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a78441",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646eed1",
   "metadata": {},
   "source": [
    "Discussion: The fast-slow approach mimics having an exploitative agent and a conservative baseline. In a production system like Sqwish, how might we implement this practically? (Hint: The “fast” could be a live updated prompt policy, and the “slow” could be a periodically retrained baseline that prevents the prompt strategy from drifting too far and causing bad experiences.) Propose a mechanism for deciding when to sync the slow model with the fast one (e.g., based on performance plateau or time interval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d8cbe",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700c4ce",
   "metadata": {},
   "source": [
    "\n",
    "### Practical fast/slow implementation (production)\n",
    "\n",
    "- **Fast (online, gradient-free policy layer)**\n",
    "  - Treat **fast** as a live-updated *policy artifact* (e.g., prompt template).\n",
    "  - Update it continuously from product signals (thumbs up/down, completions, revenue, etc.) using **gradient-free** methods (contextual bandits, Bayesian optimization).\n",
    "  - Wrap it in **guardrails** (policy constraints, hard limits, monitoring).\n",
    "\n",
    "- **Evaluate fast vs. slow using two feedback channels**\n",
    "  - **Outcome metrics**: conversion/retention/revenue, task success, latency/cost, complaints.\n",
    "  - **Preference data**: thumbs + pairwise comparisons.\n",
    "    - Show **A = fast** vs **B = slow**.\n",
    "  - **Experiment design**: continuous A/B, with minimum sample sizes + statistical significance, plus segment checks (new vs power users, languages, affluent vs thrifty).\n",
    "\n",
    "- **Promotion criteria (fast → candidate slow refresh)**\n",
    "  - **Wins primary KPI(s)** with statistical confidence.\n",
    "  - **Not worse on key segments** / worst-case slices.\n",
    "  - **Passes safety**: automated checks + human review on sampled traffic.\n",
    "  - **Bounded drift**: fast stays within a defined divergence from slow.\n",
    "\n",
    "- **Sync (train a new slow from fast on high-quality labeled data)**\n",
    "  - Build a *high-quality* training set from production traffic:\n",
    "    - **Thumbs-up / explicit positive feedback**.\n",
    "    - **Outcome wins**: sessions linked to success (e.g., good revenue/conversion/retention, high task success, low complaint rates).\n",
    "    - **Audited answers**: human-reviewed samples.\n",
    "  - Convert that into preference/distillation data:\n",
    "    - **Preference pairs for DPO**: for the same context `x`, compare `y_fast` vs `y_slow` (or other candidates) and keep pairs where `y_fast` is preferred.\n",
    "    - **Format**: `(x, y_fast, y_slow)` plus any additional trusted human-labeled pairs.\n",
    "  - Train the **new slow** using **both**:\n",
    "    - **DPO** on the preference pairs (so slow learns *what to prefer*).\n",
    "    - **Logprob (logit) distillation from fast**: train slow to match fast’s next-token logprobs.\n",
    "  - Stabilize the update to avoid regressions:\n",
    "    - **Replay** older preference data / critical behaviors to avoif forgetting.\n",
    "    - **KL regularization** to the previous slow (or a reference model) to bound drift.\n",
    "\n",
    "- **Release loop (how slow gets updated safely)**\n",
    "  1. **Canary rollout** the new slow (**slow+1**) to a small slice of traffic.\n",
    "  2. **Monitor KPIs**: outcome KPIs, preference win-rate (head-to-head vs the old slow), and safety incidents—broken down by key segments and high-risk slices.\n",
    "  3. **Promote or rollback**: if it holds, make **slow+1** the new slow and reset fast’s baseline to it; otherwise roll back and keep iterating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e37cb",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8aa79",
   "metadata": {},
   "source": [
    "Discussion (Context vs. Weight Adaptation): The fast-slow paradigm updates model weights online. An alternative is updating contexts while keeping weights frozen: ACE (Day 10) does exactly this with evolving playbooks. Compare: weight-based (changes behavior fundamentally, requires gradients, risk of forgetting) vs. context-based (faster, interpretable, limited by context window, no forgetting). When might you prefer each? Consider: (1) dramatic domain shifts, (2) incorporating a single new policy, (3) regulated industries requiring auditability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770aebf",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535c21d",
   "metadata": {},
   "source": [
    "In the Sqwish context, fast context updates are the safest and fastest way to adapt to live feedback.\n",
    "\n",
    "Let's analyze these three scenarios:\n",
    "\n",
    "- **A single new policy or rule**: Prefer fast context updates so the policy is explicit, versioned, and reversible. Distill into weights only if it becomes core behavior.\n",
    "- **Regulated industries / auditability**: Prefer fast context updates for traceability, such as change logs, policy text, and approvals. Weight updates are less auditable and can introduce tricky second- and third-order effects, so they require stricter governance, replay, and safety reviews.\n",
    "- **Out-of-distribution traffic**: Prefer slow weight updates when traffic moves into new domains, for example private enterprise data or novel task types, where the base model underperforms. Use fast context updates as a temporary bridge while collecting data for a stable retrain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
