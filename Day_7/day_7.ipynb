{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7eed45",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18663c",
   "metadata": {},
   "source": [
    "Conceptual: Why does a static preference dataset become insufficient as a policy improves? Explain using an example: initial model outputs are poor, so human preferences cover only easy mistakes; once the model stops making those, the old data is less relevant. How does online DPO address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb28f9",
   "metadata": {},
   "source": [
    "## Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8485ca",
   "metadata": {},
   "source": [
    "A static dataset is **tied to the behavior of the policy that generated it**. As the policy improves, two things happen:\n",
    "\n",
    "1. **The dataset stops matching the policy’s error profile (distribution shift).**\n",
    "   Early on, the model makes *obvious* mistakes, so humans mostly compare **bad vs. decent** outputs. After training, the policy rarely produces the “bad” outputs anymore, so those comparisons are no longer representative of what the policy currently does.\n",
    "\n",
    "2. **The comparisons become weak training signal (saturation).**\n",
    "   DPO updates are driven by the *margin* between preferred and rejected:\n",
    "   $$\n",
    "   \\Delta = \\big(\\log \\pi(y_w|x)-\\log \\pi(y_l|x)\\big) - \\big(\\log \\pi_{\\text{ref}}(y_w|x)-\\log \\pi_{\\text{ref}}(y_l|x)\\big).\n",
    "   $$\n",
    "   If the improved policy already strongly prefers (y_w) over (y_l) on the old dataset, (\\Delta) is large, the logistic loss saturates, and gradients become small. So the old dataset gives diminishing returns.\n",
    "\n",
    "\n",
    "Online DPO fixes the mismatch by making the data **on-policy (or near on-policy)** each iteration:\n",
    "\n",
    "* **Generate candidates from the current policy** (often with some exploration to get diversity).\n",
    "\n",
    "* **Collect fresh preferences** on *these* candidates.\n",
    "\n",
    "* **Update with DPO on the new comparisons**, typically with **anchoring/regularization** to prevent drift.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a240406",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff73d87",
   "metadata": {},
   "source": [
    "Coding: Implement a simple simulation of OFS-DPO. Use two copies of a model (e.g., two sets of parameters representing fast and slow). At each iteration: have the fast model generate an output for a query, label its quality with a simulated metric (e.g., a known reward function or a proxy judge), update fast model via DPO loss, and occasionally copy fast weights to slow. Monitor a metric (like the difference between fast and slow policy outputs or rewards over time) to verify that fast adapts quickly and slow provides a stabilizing anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b0716",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce853bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a78441",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646eed1",
   "metadata": {},
   "source": [
    "Discussion: The fast-slow approach mimics having an exploitative agent and a conservative baseline. In a production system like Sqwish, how might we implement this practically? (Hint: The “fast” could be a live updated prompt policy, and the “slow” could be a periodically retrained baseline that prevents the prompt strategy from drifting too far and causing bad experiences.) Propose a mechanism for deciding when to sync the slow model with the fast one (e.g., based on performance plateau or time interval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d8cbe",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700c4ce",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e37cb",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8aa79",
   "metadata": {},
   "source": [
    "Discussion (Context vs. Weight Adaptation): The fast-slow paradigm updates model weights online. An alternative is updating contexts while keeping weights frozen: ACE (Day 10) does exactly this with evolving playbooks. Compare: weight-based (changes behavior fundamentally, requires gradients, risk of forgetting) vs. context-based (faster, interpretable, limited by context window, no forgetting). When might you prefer each? Consider: (1) dramatic domain shifts, (2) incorporating a single new policy, (3) regulated industries requiring auditability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770aebf",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535c21d",
   "metadata": {},
   "source": [
    ".."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
