{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95ffdfb0",
      "metadata": {},
      "source": [
        "# Capstone Project\n",
        "\n",
        "## Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc124173",
      "metadata": {},
      "source": [
        "**Objective:** Synthesize everything you've learned by designing and implementing a mini version of Sqwish's optimization engine. This capstone project will have you create a simulated environment and then build an agent that optimizes in that environment, incorporating bandits, delayed rewards, safety checks, and evaluation in one end-to-end prototype.\n",
        "\n",
        "**Project Brief:** *E-commerce Model Router.* You will simulate an e-commerce website where a model is chosen to generate product descriptions (generation is abstracted away), and the goal is to maximize conversion (purchase) while respecting cost. Three different models (of varying cost and quality) are available. Users have different preferences. You'll build a contextual bandit agent to route the models optimally.\n",
        "\n",
        "**Environment Setup:**\n",
        "\n",
        "- **User Context:** Define a user persona feature (e.g. budget_sensitive vs quality_seeker) and a product category feature. These together form the context $x$.\n",
        "- **Arms/Actions:** Three model choices for generating the description: *Model A* (cheap & concise), *Model B* (moderate), *Model C* (expensive & detailed). You can also allow the prompt to vary or other actions, but at minimum choosing the model is the action.\n",
        "- **Hidden Reward Function:** Simulate probability of conversion as a function of context and model. For example: budget_sensitive users convert better with concise Model A (perhaps they don't like fluff), quality_seekers convert better with detailed Model C. You can fabricate this mapping, e.g., P(buy | x, A) = 0.05 normally, but 0.15 if user is budget_sensitive; P(buy | x, C) = 0.05 normally, but 0.15 if user is quality_seeker, etc. The idea is each model is optimal for a certain segment. Conversion is binary (success/fail).\n",
        "- **Cost Model:** Assign a \"cost\" to using each model (e.g. A costs $0.01, B $0.02, C $0.10 per description). This will be used in evaluating the profit.\n",
        "\n",
        "**Agent Requirements:**\n",
        "\n",
        "- Use a **Contextual Bandit algorithm** (Thompson Sampling or LinUCB recommended) to learn over interactions which model works best for which context. The agent will make a choice each round (given context, pick model) and observe a stochastic conversion after a delay.\n",
        "- **Delayed Feedback:** Assume conversions are observed after a delay. Update the bandit when the real conversion arrives, using profit (conversion value minus cost) as the reward.\n",
        "- **Off-Policy Evaluation:** Before fully trusting your learned policy, use an offline evaluation. For example, have the agent do an initial random policy for 1000 interactions to gather a log. Then when your bandit policy is learned, use **IPS or Doubly Robust** on that log to estimate the conversion rate of the bandit policy *without* deploying it. Compare this estimate to the actual performance when you do run the bandit live in the simulator. This checks your OPE integration.\n",
        "- **Safety Constraint:** Implement a simple safety rule in the simulator (for instance, Model C might occasionally produce an unsafe word). If that happens, the user instantly doesn't buy and is unhappy. Ensure your agent either learns to avoid that or you add a filter. (This can be simulated by saying: with small probability, Model C outputs something disallowed, which always results in no conversion; the agent could learn that risk or you can explicitly penalize it.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e890ee",
      "metadata": {},
      "source": [
        "## Implementation Choices\n",
        "\n",
        "### Goals\n",
        "- Compare three bandit policies: LinUCB, Thompson Sampling, and epsilon-greedy.\n",
        "- Train on delayed conversion feedback with a safety gate and costs.\n",
        "- Estimate offline value with Doubly Robust OPE and compare to actual.\n",
        "\n",
        "### Design Choices\n",
        "- Context: continuous embedding (dim=8) plus categorical persona/category features.\n",
        "- Delay model: exponential delay (mean=50) to mimic long-tail conversions.\n",
        "- Safety: Model C has 2% unsafe injection; unsafe always yields no conversion.\n",
        "- Costs/value: A/B/C costs = 0.01/0.02/0.10, conversion value = 1.0.\n",
        "- OPE: Doubly Robust using logistic regression for conversion probability.\n",
        "\n",
        "### Code Layout\n",
        "- `Day_15/sim.py`: environment, contexts, conversion model, delay, costs, safety.\n",
        "- `Day_15/agents.py`: LinUCB (imported), Thompson Sampling, epsilon-greedy, block features.\n",
        "- `Day_15/ope.py`: DR estimator + logistic regression reward model.\n",
        "- `Day_15/run.py`: end-to-end training, OPE, evaluation, plots.\n",
        "\n",
        "### How the Run Works\n",
        "1) Log N random interactions for OPE.\n",
        "2) Train each agent for the remaining rounds using delayed profit feedback (conversion value minus cost).\n",
        "3) Evaluate each learned policy on fresh rounds (actual conversion + profit).\n",
        "4) Use DR OPE on the log to estimate conversion/profit and compare to actual.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bf36106",
      "metadata": {},
      "source": [
        "### OPE: Logistic Reward Model + Doubly Robust Estimator\n",
        "- **Logistic regression reward model:** We fit a binary classifier on logged data to predict conversion from (context, action). This gives a probability estimate q_hat(x, a) = P(convert | x, a), which we use as a smooth, low-variance model of reward.\n",
        "- **Doubly Robust (DR) estimate:** DR combines the model prediction with an inverse-propensity correction from the log.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab545ec",
      "metadata": {},
      "source": [
        "### Context + Preference Modeling\n",
        "- Each round samples a context with: (1) an 8-dim continuous embedding `embed`, (2) a persona id `persona`, and (3) a category id `category`.\n",
        "- `embed` is a synthetic latent feature vector; `embed_w` contains per-model weights that score those features.\n",
        "- `persona_aff` is a lookup table: `persona_aff[persona, a]` adds a persona-specific bias for model `a`.\n",
        "- `category_aff` is a lookup table: `category_aff[category, a]` adds a category-specific bias for model `a`.\n",
        "- For each model/arm `a`, we compute a logit score $s_a = base[a] + embed_w[a]^T\\,embed + persona_aff[persona, a] + category_aff[category, a]$.\n",
        "- Conversion probability is $P(\\text{convert}\\mid x,a)=\\sigma(s_a)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c39896",
      "metadata": {},
      "source": [
        "### Data Labels + Delayed Feedback\n",
        "- Labels used in the simulator are defined in `Day_15/sim.py`: `PERSONA_NAMES`, `CATEGORY_NAMES`, and `MODEL_NAMES`.\n",
        "- Unsafe events are injected for Model C with probability `unsafe_rate`; unsafe always yields no conversion.\n",
        "- True conversion is delayed: when an action is taken at time $t$, its conversion outcome is delivered at $t + d$ with d ~ Exponential(mean=50).\n",
        "- During training, we update the bandit when the conversion arrives, using profit (conversion value minus cost).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "962ecc51",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "run_experiment() got an unexpected keyword argument 'plot_title'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDay_15\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[1;32m     13\u001b[0m base_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, log_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, eval_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     14\u001b[0m                    plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m summary_delay, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelay=50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbase_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m summary_nodelay, _ \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39mrun_experiment(delay_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, plot_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_delay=0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_kwargs)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-strategy head-to-head (delay=50 vs no delay=0)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: run_experiment() got an unexpected keyword argument 'plot_title'"
          ]
        }
      ],
      "source": [
        "# Compare delayed feedback vs no delay\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "here = Path.cwd()\n",
        "if (here / 'run.py').exists():\n",
        "    sys.path.insert(0, str(here.parent))\n",
        "else:\n",
        "    sys.path.insert(0, str(here))\n",
        "\n",
        "from Day_15 import run\n",
        "\n",
        "base_kwargs = dict(rounds=5000, log_rounds=1000, eval_rounds=1000, seed=0, epsilon=0.1,\n",
        "                   plot=True)\n",
        "\n",
        "summary_delay, _ = run.run_experiment(delay_mean=50.0, plot_title='delay=50', **base_kwargs)\n",
        "summary_nodelay, _ = run.run_experiment(delay_mean=0.0, plot_title='no_delay=0', **base_kwargs)\n",
        "\n",
        "print(\"Per-strategy head-to-head (delay=50 vs no delay=0)\")\n",
        "\n",
        "for name in summary_delay.keys():\n",
        "    d = summary_delay[name]\n",
        "    n = summary_nodelay[name]\n",
        "    print(\"\")\n",
        "    print(name)\n",
        "    header = f\"{'metric':<12} {'delay=50':>10} {'no_delay=0':>12}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    rows = [\n",
        "        (\"DR conv\", d.dr_conversion, n.dr_conversion),\n",
        "        (\"DR profit\", d.dr_profit, n.dr_profit),\n",
        "        (\"Eval conv\", d.actual_conversion, n.actual_conversion),\n",
        "        (\"Eval profit\", d.actual_profit, n.actual_profit),\n",
        "        (\"Eval unsafe\", d.actual_unsafe, n.actual_unsafe),\n",
        "    ]\n",
        "    for label, dv, nv in rows:\n",
        "        print(f\"{label:<12} {dv:>10.3f} {nv:>12.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2861c16e",
      "metadata": {},
      "source": [
        "### Delay Setting Note\n",
        "- `delay_mean = 0` means no delay: conversion outcomes are applied immediately, and training uses the real conversion reward.\n",
        "- `delay_mean > 0` uses an exponential delay with that mean; training updates only when conversions arrive.\n",
        "- Evaluation uses immediate conversions (no delay) for a clean readout.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fc4dfe",
      "metadata": {},
      "source": [
        "## More simulations for statisctical significance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "209628a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average over 10 seeds (delay=50 vs no delay=0)\n",
            "\n",
            "linucb\n",
            "metric        delay=50 mean  delay=50 std  no_delay mean  no_delay std\n",
            "----------------------------------------------------------------------\n",
            "DR conv               0.655         0.031          0.650         0.028\n",
            "DR profit             0.616         0.032          0.613         0.028\n",
            "Eval conv             0.661         0.023          0.656         0.020\n",
            "Eval profit           0.622         0.023          0.619         0.018\n",
            "Eval unsafe           0.006         0.002          0.005         0.003\n",
            "\n",
            "thompson\n",
            "metric        delay=50 mean  delay=50 std  no_delay mean  no_delay std\n",
            "----------------------------------------------------------------------\n",
            "DR conv               0.655         0.021          0.656         0.022\n",
            "DR profit             0.614         0.022          0.616         0.023\n",
            "Eval conv             0.652         0.021          0.653         0.014\n",
            "Eval profit           0.611         0.019          0.614         0.015\n",
            "Eval unsafe           0.006         0.002          0.005         0.003\n",
            "\n",
            "epsilon_greedy\n",
            "metric        delay=50 mean  delay=50 std  no_delay mean  no_delay std\n",
            "----------------------------------------------------------------------\n",
            "DR conv               0.646         0.039          0.656         0.032\n",
            "DR profit             0.611         0.037          0.618         0.030\n",
            "Eval conv             0.652         0.022          0.648         0.023\n",
            "Eval profit           0.617         0.021          0.610         0.024\n",
            "Eval unsafe           0.005         0.003          0.006         0.002\n"
          ]
        }
      ],
      "source": [
        "# Average over multiple seeds\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "here = Path.cwd()\n",
        "if (here / 'run.py').exists():\n",
        "    sys.path.insert(0, str(here.parent))\n",
        "else:\n",
        "    sys.path.insert(0, str(here))\n",
        "\n",
        "from Day_15 import run\n",
        "\n",
        "base_kwargs = dict(rounds=5000, log_rounds=1000, eval_rounds=1000, epsilon=0.1,\n",
        "                   plot=False)\n",
        "\n",
        "seeds = list(range(10))\n",
        "metrics = [\n",
        "    ('DR conv', 'dr_conversion'),\n",
        "    ('DR profit', 'dr_profit'),\n",
        "    ('Eval conv', 'actual_conversion'),\n",
        "    ('Eval profit', 'actual_profit'),\n",
        "    ('Eval unsafe', 'actual_unsafe'),\n",
        "]\n",
        "\n",
        "summary_delay, _ = run.run_experiment(delay_mean=50.0, seed=seeds[0], **base_kwargs)\n",
        "strategies = list(summary_delay.keys())\n",
        "results = {\n",
        "    name: {metric: {'delay': [], 'nodelay': []} for _, metric in metrics}\n",
        "    for name in strategies\n",
        "}\n",
        "\n",
        "for seed in seeds:\n",
        "    summary_delay, _ = run.run_experiment(delay_mean=50.0, seed=seed, **base_kwargs)\n",
        "    summary_nodelay, _ = run.run_experiment(delay_mean=0.0, seed=seed, **base_kwargs)\n",
        "    for name in strategies:\n",
        "        d = summary_delay[name]\n",
        "        n = summary_nodelay[name]\n",
        "        for _, metric in metrics:\n",
        "            results[name][metric]['delay'].append(getattr(d, metric))\n",
        "            results[name][metric]['nodelay'].append(getattr(n, metric))\n",
        "\n",
        "print(f\"Average over {len(seeds)} seeds (delay=50 vs no delay=0)\")\n",
        "\n",
        "for name in strategies:\n",
        "    print(\"\")\n",
        "    print(name)\n",
        "    header =  (\n",
        "        f\"{'metric':<12} {'delay=50 mean':>14} {'delay=50 std':>13} \"\n",
        "        f\"{'no_delay mean':>14} {'no_delay std':>13}\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for label, metric in metrics:\n",
        "        d_vals = np.asarray(results[name][metric]['delay'], dtype=float)\n",
        "        n_vals = np.asarray(results[name][metric]['nodelay'], dtype=float)\n",
        "        print(\n",
        "            f\"{label:<12} {d_vals.mean():>14.3f} {d_vals.std(ddof=1):>13.3f} \"\n",
        "            f\"{n_vals.mean():>14.3f} {n_vals.std(ddof=1):>13.3f}\"\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
