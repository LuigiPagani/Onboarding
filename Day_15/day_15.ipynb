{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95ffdfb0",
      "metadata": {},
      "source": [
        "# Capstone Project\n",
        "\n",
        "## Overview\n",
        "This notebook runs the Day 15 capstone simulator: an e-commerce description optimizer\n",
        "using a contextual bandit (LinUCB, Thompson Sampling, and epsilon-greedy) trained on\n",
        "an LLM-judge proxy reward with delayed conversion feedback. We evaluate actual\n",
        "conversion/profit, apply a safety threshold on unsafe content, and estimate policy\n",
        "value offline using Doubly Robust OPE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc124173",
      "metadata": {},
      "source": [
        "*Objective:** Synthesize everything you’ve learned by designing and implementing a mini version of Sqwish’s optimization engine. This capstone project will have you create a simulated environment and then build an agent that optimizes in that environment, incorporating bandits, proxy rewards, and OPE. It’s a chance to put it all together: bandit algorithms, reward modeling, safety checks, and evaluation in one end-to-end prototype.\n",
        "\n",
        "**Project Brief:** *E-commerce Description Optimizer.* You will simulate an e-commerce website where an LLM generates product descriptions for users, and the goal is to maximize conversion (purchase) while respecting cost. Three different LLMs (of varying cost and quality) are available. Users have different preferences. You’ll build a contextual bandit agent to route and prompt the LLMs optimally.\n",
        "\n",
        "**Environment Setup:**\n",
        "\n",
        "- **User Context:** Define a user persona feature (e.g. budget_sensitive vs quality_seeker) and a product category feature. These together form the context $x$.\n",
        "- **Arms/Actions:** Three LLM choices for generating the description: *Model A* (cheap & concise), *Model B* (moderate), *Model C* (expensive & detailed). You can also allow the prompt to vary or other actions, but at minimum choosing the model is the action.\n",
        "- **Hidden Reward Function:** Simulate probability of conversion as a function of context and model. For example: budget_sensitive users convert better with concise Model A (perhaps they don’t like fluff), quality_seekers convert better with detailed Model C. You can fabricate this mapping, e.g., $P(\\text{buy}|x,\\text{A}) = 0.05$ normally, but $0.15$ if user is budget_sensitive; $P(\\text{buy}|x,\\text{C}) = 0.05$ normally, but $0.15$ if user is quality_seeker, etc. The idea is each model is optimal for a certain segment. Conversion is binary (success/fail).\n",
        "- **Cost Model:** Assign a “cost” to using each model (e.g. A costs  $\\$0.01$, B $\\$0.02$, C $\\$0.10$ per description). This will be used in evaluating the profit.\n",
        "\n",
        "**Agent Requirements:**\n",
        "\n",
        "- Use a **Contextual Bandit algorithm** (Thompson Sampling or LinUCB recommended) to learn over interactions which model works best for which context. The agent will make a choice each round (given context, pick model), observe a stochastic reward (1 if conversion happened, 0 if not).\n",
        "- Incorporate a **Proxy Reward Model (LLM Judge)**: To make it interesting, assume conversion events are rare (maybe users purchase much later). So instead, introduce an immediate proxy reward - e.g., an LLM that scores the description’s “persuasiveness” from 0 to 1. The bandit will train on this proxy reward every round (since conversion is delayed), but you will later evaluate on actual conversion. The proxy should be correlated with conversion but not perfect, to simulate reality.\n",
        "- **Off-Policy Evaluation:** Before fully trusting your learned policy, use an offline evaluation. For example, have the agent do an initial random policy for 1000 interactions to gather a log. Then when your bandit policy is learned, use **IPS or Doubly Robust** on that log to estimate the conversion rate of the bandit policy *without* deploying it. Compare this estimate to the actual performance when you do run the bandit live in the simulator. This checks your OPE integration.\n",
        "- **Safety Constraint:** Implement a simple safety rule in the simulator (for instance, Model C might occasionally produce an unsafe word). If that happens, the user instantly doesn’t buy and is unhappy. Ensure your agent either learns to avoid that or you add a filter. (This can be simulated by saying: with small probability, Model C outputs something disallowed, which always results in no conversion; the agent could learn that risk or you can explicitly penalize it.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff37c0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the capstone from the Day_15 directory\n",
        "!python run.py --rounds 5000 --log-rounds 1000 --eval-rounds 1000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e890ee",
      "metadata": {},
      "source": [
        "## Implementation Notes (README-style)\n",
        "\n",
        "### Goals\n",
        "- Build the Day 15 capstone simulator only (no extra days).\n",
        "- Compare three bandit policies: LinUCB, Thompson Sampling, and epsilon-greedy.\n",
        "- Use a real LLM judge (OpenAI `gpt-4.1-nano`) for proxy rewards.\n",
        "- Include delayed conversion feedback + noise, plus a safety gate.\n",
        "- Estimate offline value with Doubly Robust OPE and compare to actual.\n",
        "\n",
        "### Key Design Choices\n",
        "- Context: continuous embedding (dim=8) plus categorical persona/category features.\n",
        "- Delay model: exponential delay (mean=50) to mimic long-tail conversions.\n",
        "- Proxy noise: Gaussian noise (std=0.2) on the judge score.\n",
        "- Safety: LLM judge returns `unsafe_score`; unsafe if >= 0.5. Model C has 2% unsafe injection.\n",
        "- Costs/value: A/B/C costs = 0.01/0.02/0.10, conversion value = 1.0.\n",
        "- OPE: Doubly Robust using logistic regression for conversion probability.\n",
        "\n",
        "### Code Layout\n",
        "- `Day_15/sim.py`: environment, contexts, conversion model, delay, costs, safety.\n",
        "- `Day_15/agents.py`: LinUCB (imported), Thompson Sampling, epsilon-greedy, block features.\n",
        "- `Day_15/judge.py`: OpenAI judge call returning `{proxy_score, unsafe_score}`.\n",
        "- `Day_15/ope.py`: DR estimator + logistic regression reward model.\n",
        "- `Day_15/run.py`: end-to-end training, OPE, evaluation, plots.\n",
        "\n",
        "### How the Run Works\n",
        "1) Log 1,000 random interactions for OPE.\n",
        "2) Train each agent for the remaining rounds using proxy reward.\n",
        "3) Evaluate each learned policy on fresh rounds (actual conversion + profit).\n",
        "4) Use DR OPE on the log to estimate conversion/profit and compare to actual.\n",
        "\n",
        "### Plots\n",
        "- Cumulative avg proxy reward\n",
        "- Cumulative conversion rate\n",
        "- Cumulative avg profit\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
