{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Design: Propose a logging schema for Sqwish’s online learning system. It should record everything needed to reproduce training and do later analysis. For each user request, what would you log? (Think: a unique request ID, user context features, chosen prompt/model, all model outputs maybe, any user click or outcome, timestamps, the probability or propensity of the chosen action if using a stochastic policy, etc.). Write a structured list of fields and justify each (why is it needed? e.g. propensity is needed for IPS in OPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562da46a",
   "metadata": {},
   "source": [
    "## Solution: Logging schema (concise)\n",
    "\n",
    "### Decision event (one row per request)\n",
    "- `request_id`, `timestamp_ms`, `session_id`, `user_id_hash` (join keys: link this decision to later rewards/errors)\n",
    "- `trace_id` (distributed tracing: follow the request across services and break down latency)\n",
    "- `context_features`, `feature_version` (exact inputs the policy used at decision time)\n",
    "- `candidate_actions`, `chosen_action`, `action_params` (what was available + what we did)\n",
    "- `logging_policy_id`, `logging_policy_version`, `propensity` (needed for IPS/DR OPE)\n",
    "- `exploration_state` (epsilon/UCB alpha/TS params; why it explored)\n",
    "- `latency_ms_total`, `latency_ms_by_stage`, `token_usage`, `estimated_cost_usd`, `error_flags`\n",
    "- `safety_labels`, `safety_score`, `safety_model_version`, `mitigation_taken`\n",
    "\n",
    "### Reward/outcome event (separate row, can arrive later)\n",
    "- `request_id` (join key: attach this outcome back to the original decision)\n",
    "- `reward_timestamp_ms` (when the outcome was observed)\n",
    "- `reward_type`, `reward_value` (what happened + numeric value)\n",
    "- `reward_observation_window` (what window you waited for, e.g. 24h conversion)\n",
    "- `attribution_model_version` (if credit assignment/labeling uses a model)\n",
    "\n",
    "### Minimal template\n",
    "```json\n",
    "{\n",
    "  \"request_id\": \"...\",\n",
    "  \"timestamp_ms\": 0,\n",
    "  \"session_id\": \"...\",\n",
    "  \"user_id_hash\": \"...\",\n",
    "  \"context_features\": {\"...\": \"...\"},\n",
    "  \"feature_version\": \"...\",\n",
    "  \"candidate_actions\": [\"...\"],\n",
    "  \"chosen_action\": \"...\",\n",
    "  \"action_params\": {\"...\": \"...\"},\n",
    "  \"logging_policy_id\": \"...\",\n",
    "  \"logging_policy_version\": \"...\",\n",
    "  \"propensity\": 0.0,\n",
    "  \"latency_ms_total\": 0,\n",
    "  \"token_usage\": {\"prompt\": 0, \"completion\": 0},\n",
    "  \"estimated_cost_usd\": 0.0,\n",
    "  \"safety_labels\": [],\n",
    "  \"error_flags\": []\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Analysis: You have deployed a bandit that optimizes prompts. After a week, you examine results and see overall user satisfaction went up 5%. However, for new users (first-time visitors) satisfaction dropped. How would you investigate this? Outline an experiment or analysis using logs to diagnose why the policy might be underperforming for new users (maybe it over-explored or didn’t personalize properly for cold-start users). What changes to the algorithm could you consider (e.g. epsilon-greedy for new users until enough data)?\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b5061",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Coding: Using an open bandit dataset (e.g. the Open Bandit Pipeline’s logged data if available, or simulate one), perform an off-policy evaluation of a hypothetical new policy. For example, use logged data from a uniform random policy on a classification task as bandit feedback. Define a new deterministic policy (like always choose arm 1 for certain feature values and arm 2 otherwise). Use IPS and Doubly Robust to estimate the new policy’s reward from the logs. Compare that to the actual reward if you run the new policy on the dataset (if ground truth available). This exercise solidifies understanding of OPE’s value and limitations (if the policy is very different, IPS variance will be high).\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae870c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45327de1",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Monitoring: List 3 metrics you would put on a dashboard for the live Sqwish system and why. For example: “Cumulative regret” - to watch if the learning is improving over a non-learning baseline; “10th percentile reward” - to ensure we’re not badly serving a subset of users even if average looks good; “Unsafe response count per 1000 interactions” - to catch any increase in policy violations. Explain briefly how each metric helps ensure the system is healthy.\n",
    "\n",
    "## Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
