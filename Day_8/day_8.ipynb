{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa7930c",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2834b",
   "metadata": {},
   "source": [
    "Coding: Build a simple Pairwise Judgment Evaluator. Use an API like GPT-5 to compare two responses. Implement a swap test: for a given pair (response A vs B), have the judge pick a winner; then present the responses in reversed order (B vs A) and get a winner. If the judge is inconsistent (prefers A then B), record this as a potential bias instance and default to “tie” or require human review. Test this on a few known cases (you can fabricate scenarios, like A is longer but correct, B is shorter but slightly less verbose, etc.). Measure the agreement rate of the judge with itself under swapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815c6c1",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdc60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Induction vs proof: A then B -> A\n",
      "0.999...: A then B -> A\n",
      "Derivative of |x| at 0: A then B -> A\n",
      "Verbose vs concise (both correct): B then B -> TIE (bias -> tie)\n",
      "Agreement rate: 75% (3/4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ModuleNotFoundError:\n",
    "    raise ModuleNotFoundError(\"Please install the 'openai' Python package.\")\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"\"  # set your key here as a string \n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is not set.\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-4.1-nano\"\n",
    "SYSTEM = (\n",
    "    \"You are a strict, fair judge of response quality. Prioritize correctness, \"\n",
    "    \"then clarity and concision.\"\n",
    ")\n",
    "\n",
    "\n",
    "def judge_pair(prompt, a, b):\n",
    "    user = f\"\"\"Prompt:\n",
    "{prompt}\n",
    "\n",
    "Response A:\n",
    "{a}\n",
    "\n",
    "Response B:\n",
    "{b}\n",
    "\n",
    "Return only one token: A, B, or TIE.\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = client.responses.create(\n",
    "            model=MODEL,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            max_output_tokens=200,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"OpenAI API call failed. Common causes: invalid API key, no network, or wrong model name.\\n\"\n",
    "            f\"Underlying error: {type(e).__name__}: {e}\"\n",
    "        ) from e\n",
    "\n",
    "    text = ((resp.output_text) or \"\").strip().upper()\n",
    "    for token in (\"A\", \"B\", \"TIE\"):\n",
    "        if text.startswith(token):\n",
    "            return token\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Judge returned an unexpected output (expected exactly: A, B, or TIE).\\n\"\n",
    "        f\"Raw judge output: {resp.output_text!r}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def swap_test(prompt, a, b):\n",
    "    r1 = judge_pair(prompt, a, b)\n",
    "    r2 = judge_pair(prompt, b, a)\n",
    "    consistent = (r1, r2) in {(\"A\", \"B\"), (\"B\", \"A\"), (\"TIE\", \"TIE\")}\n",
    "    final = \"TIE\" if not consistent else r1\n",
    "    return {\"first\": r1, \"second\": r2, \"consistent\": consistent, \"final\": final}\n",
    "\n",
    "\n",
    "cases = [\n",
    "    {\n",
    "        \"name\": \"Induction vs proof\",\n",
    "        \"prompt\": \"If all swans you have seen are white, does that prove all swans are white? Answer in one sentence.\",\n",
    "        \"A\": \"No. That is inductive evidence, not a proof.\",\n",
    "        \"B\": \"Yes. Observing enough swans proves it.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"0.999...\",\n",
    "        \"prompt\": \"Is 0.999... equal to 1? Briefly justify.\",\n",
    "        \"A\": \"Yes. It is the limit of 0.9, 0.99, 0.999, ... which equals 1.\",\n",
    "        \"B\": \"No. It is slightly less than 1.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Derivative of |x| at 0\",\n",
    "        \"prompt\": \"Does f(x)=|x| have a derivative at x=0? Answer concisely.\",\n",
    "        \"A\": \"No. The left and right derivatives are -1 and 1.\",\n",
    "        \"B\": \"Yes. The derivative at 0 is 0.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Verbose vs concise (both correct)\",\n",
    "        \"prompt\": \"In one sentence, describe what a binary search does.\",\n",
    "        \"A\": \"Binary search works on sorted data by repeatedly halving the search interval and checking the middle element to find a target, which is efficient for large lists.\",\n",
    "        \"B\": \"It finds a target in a sorted list by repeatedly halving the search range.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "for c in cases:\n",
    "    r = swap_test(c[\"prompt\"], c[\"A\"], c[\"B\"])\n",
    "    r[\"name\"] = c[\"name\"]\n",
    "    results.append(r)\n",
    "    flag = \" (bias -> tie)\" if not r[\"consistent\"] else \"\"\n",
    "    print(f\"{c['name']}: {r['first']} then {r['second']} -> {r['final']}{flag}\")\n",
    "\n",
    "agree = sum(r[\"consistent\"] for r in results)\n",
    "rate = agree / len(results)\n",
    "print(f\"Agreement rate: {rate:.0%} ({agree}/{len(results)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec285ceb",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9731d66",
   "metadata": {},
   "source": [
    "Analysis: You have a small set of prompts where you also collected human evaluations of outputs. Use this as a calibration set for your LLM-judge. For each prompt, you have the judge’s preferred answer and the human’s preferred answer. How would you estimate the judge’s error rates (false positive/negative) from this? Outline how to adjust the judge’s scores statistically so that, say, a 70% win rate reported by the judge comes with a confidence interval for the true human preference rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b65a9",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d496f6",
   "metadata": {},
   "source": [
    "Start with a human-labeled **gold set**, then split it into two parts:\n",
    "\n",
    "- **Few-shot subset**: examples you include in the judge prompt.\n",
    "- **Holdout subset**: examples the judge never sees.\n",
    "\n",
    "On the holdout, compare judge vs human to estimate error rates (e.g., judge picks A when human prefers B):\n",
    "\n",
    "- $FP = P(\\text{judge picks A} \\mid \\text{human prefers B})$\n",
    "- $FN = P(\\text{judge picks B} \\mid \\text{human prefers A})$\n",
    "\n",
    "If the judge reports a win rate $p_j = P(\\text{judge picks A})$ on unlabeled prompts, relate it to the true human preference rate $p_h = P(\\text{human prefers A})$ via:\n",
    "\n",
    "$$\n",
    " p_j = p_h(1 - FN) + (1 - p_h)FP\n",
    "$$\n",
    "\n",
    "Solve for $p_h$:\n",
    "\n",
    "$$\n",
    " p_h = \\frac{p_j - FP}{1 - FP - FN}\n",
    "$$\n",
    "\n",
    "Then clip $p_h$ to $[0, 1]$ if needed.\n",
    "\n",
    "For uncertainty, Wilson/binomial confidence intervals are fast, closed-form, and good for small $n$ under i.i.d. Bernoulli assumptions; bootstrap confidence intervals resample the holdout set and are more flexible, but need more data/compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ea5c4",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c36c5",
   "metadata": {},
   "source": [
    "Discussion: Imagine our Sqwish system using an LLM judge to decide which of two prompts led to a better answer (when user feedback is not immediately available). What are the risks if we blindly trust the LLM judge’s verdicts? List at least two failure modes (e.g. the judge might prefer flowery language even if factual accuracy suffers). How can the team mitigate these? (Think in terms of periodic human audits, calibration as above, or mixing in some known test queries with correct answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2191a",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633dfdd0",
   "metadata": {},
   "source": [
    "**Failure modes**\n",
    "\n",
    "- **Style/length bias**: the judge prefers verbosity or polished prose over factual accuracy.\n",
    "- **Order bias**: the verdict changes based on which answer is shown first, or on superficial formatting differences (a swap test can reveal this inconsistency).\n",
    "\n",
    "**Mitigations**\n",
    "\n",
    "- **Periodic audits**: regularly sample judgments for human review.\n",
    "- **Fixed gold set**: maintain a stable set of benchmark queries and track judge performance over time to detect drift.\n",
    "- **Cross-judge checks**: occasionally compare the production judge against a very strong reasoning model (but more expensive) reasoning model as a proxy signal, and recalibrate when disagreement/error rates rise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
