{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa7930c",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2834b",
   "metadata": {},
   "source": [
    "Coding: Build a simple Pairwise Judgment Evaluator. Use an API like GPT-5 to compare two responses. Implement a swap test: for a given pair (response A vs B), have the judge pick a winner; then present the responses in reversed order (B vs A) and get a winner. If the judge is inconsistent (prefers A then B), record this as a potential bias instance and default to “tie” or require human review. Test this on a few known cases (you can fabricate scenarios, like A is longer but correct, B is shorter but slightly less verbose, etc.). Measure the agreement rate of the judge with itself under swapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815c6c1",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec285ceb",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9731d66",
   "metadata": {},
   "source": [
    "Analysis: You have a small set of prompts where you also collected human evaluations of outputs. Use this as a calibration set for your LLM-judge. For each prompt, you have the judge’s preferred answer and the human’s preferred answer. How would you estimate the judge’s error rates (false positive/negative) from this? Outline how to adjust the judge’s scores statistically so that, say, a 70% win rate reported by the judge comes with a confidence interval for the true human preference rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b65a9",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ea5c4",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c36c5",
   "metadata": {},
   "source": [
    "Discussion: Imagine our Sqwish system using an LLM judge to decide which of two prompts led to a better answer (when user feedback is not immediately available). What are the risks if we blindly trust the LLM judge’s verdicts? List at least two failure modes (e.g. the judge might prefer flowery language even if factual accuracy suffers). How can the team mitigate these? (Think in terms of periodic human audits, calibration as above, or mixing in some known test queries with correct answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd2191a",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
