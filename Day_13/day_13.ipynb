{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Application Design: Suppose our reward in Sqwish has multiple components (user satisfaction + some revenue metric), but we also have a constraint: e.g., “Don’t use more than X tokens on average” (to control cost) or “Avoid any response that violates content policy (hate, self-harm, etc.) with probability > 0.001”. Choose one such constraint and outline how you would enforce it during learning. Would you filter out unsafe outputs in data? Use a penalty in the reward (like negative reward for unsafe outcomes)? Use a Lagrange multiplier that dynamically adjusts the weight of the safety penalty until the model meets the criterion? Describe the approach and why it’s effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6280573",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Math: Write down the Lagrangian $\\mathcal{L}(\\pi, \\lambda) = \\mathbb{E}[R(\\pi)] - \\lambda (\\mathbb{E}[C(\\pi)] - \\epsilon)$ for a simple constrained bandit where $C(\\pi)$ is the expected constraint metric (say latency or a risk of violation). Derive the policy gradient that includes the constraint via $\\lambda$. Interpret $\\lambda$ in this context (hint: if $\\lambda$ is high, violating the constraint is very costly to reward - it will push the policy to sacrifice primary reward to satisfy the constraint). How does algorithms like CPO or TRPO ensure the constraint is approximately satisfied at each step?\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b5061",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Coding/Analysis: Consider a simplified RePO scenario: You have a classifier that can detect unsafe content in an output with some probability. You incorporate a “rectification” where any output that is flagged as unsafe gets a big negative reward (or is filtered out entirely). Simulate this: take a language model (could be a small one or even a stub function) that sometimes produces a forbidden word. Train a policy (even via simple trial-and-error adjustment) to maximize a reward for helpfulness minus a huge penalty for the forbidden word. Show that over iterations, the forbidden word usage drops to near zero - the policy learns to avoid it, even if that means slightly less reward in other areas. This demonstrates constrained optimization at work.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
