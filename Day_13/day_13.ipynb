{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Application Design: Suppose our reward in Sqwish has multiple components (user satisfaction + some revenue metric), but we also have a constraint: e.g., “Don’t use more than X tokens on average” (to control cost) or “Avoid any response that violates content policy (hate, self-harm, etc.) with probability > 0.001”. Choose one such constraint and outline how you would enforce it during learning. Would you filter out unsafe outputs in data? Use a penalty in the reward (like negative reward for unsafe outcomes)? Use a Lagrange multiplier that dynamically adjusts the weight of the safety penalty until the model meets the criterion? Describe the approach and why it’s effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6280573",
   "metadata": {},
   "source": [
    "## Picked constraint\n",
    "\n",
    "**Safety:** $\\Pr[\\text{policy violation}] \\le \\varepsilon = 0.001$.\n",
    "\n",
    "## Approach: Primal–dual (Lagrangian) constrained RL + filtering\n",
    "\n",
    "1. **Define cost**\n",
    "\n",
    "   * $c(x,y)=1$ if output violates policy, else $0$.\n",
    "\n",
    "2. **Constrained objective**\n",
    "\n",
    "   * Maximizing business reward $r(x,y)$ subject to $\\mathbb{E}[c]\\le \\varepsilon$.\n",
    "   * Using Lagrangian: optimize $\\mathbb{E}[r(x,y) - \\lambda c(x,y)]$, with $\\lambda \\ge 0$.\n",
    "\n",
    "3. **Training loop**\n",
    "\n",
    "   * Sample prompts $x$, generate $y \\sim \\pi_\\theta(\\cdot|x)$.\n",
    "   * Score: reward $r$ (satisfaction + revenue) and cost $c$ (safety classifier + rules + audits).\n",
    "   * **Policy update**: optimize a *constrained* RL objective like PPO on the **penalized reward** $r - \\lambda c$.\n",
    "     - Separately, add a **KL-to-reference** term for *stability*, e.g. maximize $(r - \\lambda c) - \\beta\\,\\mathrm{KL}(\\pi_\\theta\\,\\|\\,\\pi_\\text{ref})$.\n",
    "   * **Dual update** (enforces the constraint level):\n",
    "\n",
    "     $$\n",
    "     \\lambda \\leftarrow \\big[\\lambda + \\eta(\\widehat{\\mathbb{E}}[c]-\\varepsilon)\\big]_+\n",
    "     $$\n",
    "\n",
    "     If violations exceed $\\varepsilon$, $\\lambda$ increases to enforce the constraint. If violations are below $\\varepsilon$ on average, the update pushes $\\lambda$ down (but only **down to 0** because of $[\\cdot]_+$), giving the policy more slack to prioritize reward when the constraint is already satisfied.\n",
    "\n",
    "4. **Data handling (filtering)**\n",
    "\n",
    "   * **Offline data curation (SFT / preference datasets, before PPO)**:\n",
    "     - Drop examples where the supervised target output is unambiguously unsafe (e.g., explicit hate/self-harm instructions), to avoid training the model to imitate violations.\n",
    "     - Keep borderline/ambiguous prompts, but relabel them with safe targets (e.g., refusal + brief safe alternative / policy-compliant guidance), so the model learns correct behavior in “gray areas”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Math: Write down the Lagrangian $\\mathcal{L}(\\pi, \\lambda) = \\mathbb{E}[R(\\pi)] - \\lambda (\\mathbb{E}[C(\\pi)] - \\epsilon)$ for a simple constrained bandit where $C(\\pi)$ is the expected constraint metric (say latency or a risk of violation). Derive the policy gradient that includes the constraint via $\\lambda$. Interpret $\\lambda$ in this context (hint: if $\\lambda$ is high, violating the constraint is very costly to reward - it will push the policy to sacrifice primary reward to satisfy the constraint). How does algorithms like CPO or TRPO ensure the constraint is approximately satisfied at each step?\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b5061",
   "metadata": {},
   "source": [
    "### Lagrangian\n",
    "\n",
    "For a (contextual) bandit with prompt/context $x$, action $a\\sim\\pi_\\theta(\\cdot\\mid x)$, reward $r(x,a)$, and cost $c(x,a)$, the constrained problem is:\n",
    "\n",
    "$$\n",
    "\\max_{\\pi}\\; \\mathbb{E}[r]\\quad\\text{s.t.}\\quad \\mathbb{E}[c]\\le \\epsilon.\n",
    "$$\n",
    "\n",
    "The Lagrangian is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\pi,\\lambda)=\\mathbb{E}_{x,\\,a\\sim\\pi}[\\,r(x,a)\\,]-\\lambda\\big(\\mathbb{E}_{x,\\,a\\sim\\pi}[\\,c(x,a)\\,]-\\epsilon\\big),\\qquad \\lambda\\ge 0.\n",
    "$$\n",
    "\n",
    "### Policy gradient (primal update)\n",
    "\n",
    "Rewrite the objective as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\lambda)=\\mathbb{E}_{x,\\,a\\sim\\pi_\\theta}[\\,r(x,a)-\\lambda c(x,a)\\,]+\\lambda\\epsilon.\n",
    "$$\n",
    "\n",
    "The term $\\lambda\\epsilon$ does not depend on $\\theta$, so it drops out of the $\\theta$-gradient.\n",
    "\n",
    "Let $f(x,a)=r(x,a)-\\lambda c(x,a)$. For a fixed $x$ (discrete actions), write the expectation explicitly:\n",
    "\n",
    "$$\n",
    "J_x(\\theta)=\\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid x)}[f(x,a)]\n",
    "=\\sum_a \\pi_\\theta(a\\mid x)\\,f(x,a).\n",
    "$$\n",
    "\n",
    "Differentiate:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J_x(\\theta)=\\sum_a \\nabla_\\theta \\pi_\\theta(a\\mid x)\\,f(x,a).\n",
    "$$\n",
    "\n",
    "Now use the log-derivative trick:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\pi_\\theta(a\\mid x)\n",
    "=\\pi_\\theta(a\\mid x)\\,\\nabla_\\theta \\log \\pi_\\theta(a\\mid x).\n",
    "$$\n",
    "\n",
    "Substitute and convert back to an expectation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J_x(\\theta)=\\sum_a \\pi_\\theta(a\\mid x)\\,\\nabla_\\theta \\log \\pi_\\theta(a\\mid x)\\,f(x,a)\n",
    "=\\mathbb{E}_{a\\sim\\pi_\\theta}\\big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid x)\\,f(x,a)\\big].\n",
    "$$\n",
    "\n",
    "Finally average over $x$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}(\\theta,\\lambda)\n",
    "=\\mathbb{E}_{x,\\,a\\sim\\pi_\\theta}\\big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid x)\\,\\big(r(x,a)-\\lambda c(x,a)\\big)\\big].\n",
    "$$\n",
    "\n",
    "So the constraint appears exactly as an extra penalty term in the “advantage”: $r-\\lambda c$.\n",
    "\n",
    "### Dual update + interpretation of $\\lambda$\n",
    "\n",
    "The dual variable $\\lambda$ is the “price” of violating the constraint.\n",
    "\n",
    "- If $\\lambda$ is large, violations are very expensive, so the policy trades off reward to reduce cost.\n",
    "- If the constraint is slack (cost well below $\\epsilon$), the optimal $\\lambda$ tends to $0$ (complementary slackness).\n",
    "\n",
    "A standard projected dual ascent step is:\n",
    "\n",
    "$$\n",
    "\\lambda \\leftarrow [\\lambda + \\eta(\\widehat{\\mathbb{E}}[c]-\\epsilon)]_+.\n",
    "$$\n",
    "\n",
    "### How TRPO / CPO keep constraints approximately satisfied\n",
    "\n",
    "Let $\\theta_k$ be the current parameters and $s=\\theta-\\theta_k$.\n",
    "\n",
    "**Local approximations (orders):**\n",
    "\n",
    "- Reward improvement: first-order (linear) around $\\theta_k$.\n",
    "- KL constraint: second-order (quadratic) around $\\theta_k$.\n",
    "\n",
    "Concretely,\n",
    "\n",
    "$$\n",
    "J_R(\\theta_k+s)\\approx J_R(\\theta_k)+g^\\top s,\\qquad\n",
    "\\bar{\\mathrm{KL}}(\\theta_k,\\theta_k+s)\\approx \\tfrac12 s^\\top H s,\n",
    "$$\n",
    "\n",
    "where $g\\approx \\nabla_\\theta J_R(\\theta_k)$ and $H$ is the Fisher information matrix (the local metric induced by KL).\n",
    "\n",
    "**TRPO (trust region; enforces KL only):**\n",
    "\n",
    "$$\n",
    "\\max_{s}\\; g^\\top s\n",
    "\\quad\\text{s.t.}\\quad\n",
    "\\tfrac12 s^\\top H s\\le \\delta.\n",
    "$$\n",
    "\n",
    "Equivalent Lagrangian/KKT form gives $s^\\star \\propto H^{-1}g$ with scaling to satisfy the constraint:\n",
    "\n",
    "$$\n",
    "\\tilde s = H^{-1}g\\;\\;\\text{(computed via conjugate gradient)},\\qquad\n",
    "s = \\alpha\\,\\tilde s,\\;\n",
    "\\alpha=\\sqrt{\\frac{2\\delta}{\\tilde s^\\top H\\tilde s}}.\n",
    "$$\n",
    "\n",
    "**Explicit enforcement:** backtracking line search on $\\alpha$ until\n",
    "\n",
    "$$\n",
    "\\bar{\\mathrm{KL}}(\\pi_{\\theta_k}\\,\\|\\,\\pi_{\\theta_k+s})\\le \\delta\n",
    "$$\n",
    "\n",
    "(and the surrogate improves). TRPO does not directly impose a cost constraint.\n",
    "\n",
    "**CPO (adds a linearized cost constraint; enforces KL + cost):**\n",
    "\n",
    "Cost is constrained via a first-order approximation:\n",
    "\n",
    "$$\n",
    "J_C(\\theta_k+s)\\approx J_C(\\theta_k)+b^\\top s,\\qquad b\\approx \\nabla_\\theta J_C(\\theta_k).\n",
    "$$\n",
    "\n",
    "Local constrained step:\n",
    "\n",
    "$$\n",
    "\\max_{s}\\; g^\\top s\n",
    "\\quad\\text{s.t.}\\quad\n",
    "J_C(\\theta_k)+b^\\top s\\le \\epsilon,\n",
    "\\quad\n",
    "\\tfrac12 s^\\top H s\\le \\delta.\n",
    "$$\n",
    "\n",
    "**Explicit enforcement:** solve the QP’s KKT system (often via conjugate gradient), then do line search and **check both constraints**:\n",
    "\n",
    "$$\n",
    "\\bar{\\mathrm{KL}}(\\pi_{\\theta_k}\\,\\|\\,\\pi_{\\theta_k+s})\\le \\delta,\n",
    "\\qquad\n",
    "\\widehat{J_C}(\\theta_k+s)\\le \\epsilon.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Coding/Analysis: Consider a simplified RePO scenario: You have a classifier that can detect unsafe content in an output with some probability. You incorporate a “rectification” where any output that is flagged as unsafe gets a big negative reward (or is filtered out entirely). Simulate this: take a language model (could be a small one or even a stub function) that sometimes produces a forbidden word. Train a policy (even via simple trial-and-error adjustment) to maximize a reward for helpfulness minus a huge penalty for the forbidden word. Show that over iterations, the forbidden word usage drops to near zero - the policy learns to avoid it, even if that means slightly less reward in other areas. This demonstrates constrained optimization at work.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae870c",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Setup\n",
    "\n",
    "- A tiny policy produces either a **safe** completion or an **unsafe** completion that contains a forbidden token.\n",
    "- The safety cost depends only on the output here: $c(x,y)=\\mathbf{1}\\{\\text{FORBIDDEN appears in }y\\}$.\n",
    "- Use a rectified reward (helpfulness minus a large safety penalty):\n",
    "\n",
    "$$\n",
    "R(x,y)=r_{\\text{help}}(x,y)-\\beta\\,c(x,y),\\qquad \\beta\\gg 0.\n",
    "$$\n",
    "\n",
    "### REINFORCE update used in the code\n",
    "\n",
    "We maximize $J(\\theta)=\\mathbb{E}_{y\\sim\\pi_\\theta(\\cdot\\mid x)}[R(x,y)]$. Using the log-derivative trick:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)=\\mathbb{E}\\big[\\nabla_\\theta \\log \\pi_\\theta(y\\mid x)\\,(R-b)\\big]\n",
    "$$\n",
    "\n",
    "where $b$ is a baseline (we use an EMA of $R$) to reduce variance.\n",
    "\n",
    "In this toy, the policy is Bernoulli with logit $w$ and $p_{\\text{unsafe}}=\\sigma(w)$. If $u\\in\\{0,1\\}$ indicates whether the sampled output was unsafe, then:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\log \\pi(u)=u-p_{\\text{unsafe}},\\qquad\n",
    "w \\leftarrow w + \\eta\\,(R-b)\\,(u-p_{\\text{unsafe}}).\n",
    "$$\n",
    "\n",
    "Goal: show that as training proceeds, $p_{\\text{unsafe}}$ (and the observed unsafe rate) drop toward zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 simulation: RePO-style rectification via a big safety penalty\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "forbidden_token = \"FORBIDDEN\"\n",
    "\n",
    "# \"Tiny LM\" policy parameterization: p_unsafe = sigmoid(w)\n",
    "# unsafe => output contains forbidden token\n",
    "w = 0.0  # initial logit => p_unsafe ~ 0.5\n",
    "\n",
    "beta = 6.0          # safety penalty strength (rectifier)\n",
    "lr = 0.05           # policy learning rate\n",
    "baseline_ema = 0.0  # running baseline for variance reduction\n",
    "baseline_mom = 0.9\n",
    "\n",
    "T = 3000\n",
    "unsafe_rates = []\n",
    "avg_rewards = []\n",
    "ps = []\n",
    "\n",
    "# Helpful-but-unsafe completion gives slightly higher raw helpfulness,\n",
    "# so the policy would like it unless rectified by the safety penalty.\n",
    "r_help_safe = 1.0\n",
    "r_help_unsafe = 1.2\n",
    "\n",
    "unsafe_count = 0\n",
    "reward_sum = 0.0\n",
    "\n",
    "for t in range(1, T + 1):\n",
    "    # policy\n",
    "    p_unsafe = 1.0 / (1.0 + np.exp(-w))\n",
    "    unsafe = rng.random() < p_unsafe\n",
    "\n",
    "    # generate text\n",
    "    if unsafe:\n",
    "        y = f\"Here is the answer: {forbidden_token}\"\n",
    "        r_help = r_help_unsafe\n",
    "    else:\n",
    "        y = \"Here is the answer: (safe)\"\n",
    "        r_help = r_help_safe\n",
    "\n",
    "    # safety classifier (deterministic)\n",
    "    true_unsafe = forbidden_token in y\n",
    "    c = 1.0 if true_unsafe else 0.0\n",
    "\n",
    "    # rectified reward\n",
    "    R = r_help - beta * c\n",
    "\n",
    "    # REINFORCE gradient for Bernoulli-logit policy: d/dw log p(y) = (unsafe - p_unsafe)\n",
    "    grad_logp = (1.0 if unsafe else 0.0) - p_unsafe\n",
    "\n",
    "    # baseline\n",
    "    baseline_ema = baseline_mom * baseline_ema + (1.0 - baseline_mom) * R\n",
    "    adv = R - baseline_ema\n",
    "\n",
    "    # ascent\n",
    "    w = w + lr * adv * grad_logp\n",
    "\n",
    "    # logging\n",
    "    unsafe_count += int(true_unsafe)\n",
    "    reward_sum += R\n",
    "\n",
    "    if t % 50 == 0:\n",
    "        unsafe_rates.append(unsafe_count / t)\n",
    "        avg_rewards.append(reward_sum / t)\n",
    "        ps.append(p_unsafe)\n",
    "\n",
    "# plots\n",
    "xs = np.arange(len(unsafe_rates)) * 50\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(13, 3.2))\n",
    "ax[0].plot(xs, unsafe_rates)\n",
    "ax[0].set_title(\"Observed unsafe rate (FORBIDDEN in output)\")\n",
    "ax[0].set_xlabel(\"iteration\")\n",
    "ax[0].set_ylim(0, 1)\n",
    "\n",
    "ax[1].plot(xs, avg_rewards)\n",
    "ax[1].set_title(\"Average rectified reward\")\n",
    "ax[1].set_xlabel(\"iteration\")\n",
    "\n",
    "ax[2].plot(xs, ps)\n",
    "ax[2].set_title(\"Policy: p(unsafe output)\")\n",
    "ax[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial p_unsafe ~ {0.5:.3f}  |  Final p_unsafe ~ {ps[-1]:.6f}\")\n",
    "print(f\"Final unsafe rate (true) ~ {unsafe_rates[-1]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
