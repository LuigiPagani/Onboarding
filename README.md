Welcome to Sqwish where we build the **autopilot for AI products**. Over the next 15 days, you‚Äôll undertake a rigorous, hands-on journey from foundational theories of bandits and reinforcement learning to cutting-edge techniques in prompt optimization, multi-objective routing, and safe online learning. By Day 15, you‚Äôll be fully equipped to contribute to Sqwish‚Äôs real-time outcome optimization stack. Each day below includes top-tier readings (with accessible links) and challenging exercises to solidify both your theoretical understanding and practical skills.

## **Week 1: Bandit Foundations & Black-Box Optimization**

### **Day 1: Contextual Bandits - Fundamentals & Regret Analysis**

**Objective:** Grasp the core framework of **contextual bandits** and the exploration-exploitation trade-off. Learn to derive regret bounds and implement classic algorithms like LinUCB from scratch. This forms the bedrock for how Sqwish dynamically chooses prompts, models, or contexts in real time.

**Reading List:**

- **Bandit Basics:** [*Bandit Algorithms*](https://tor-lattimore.com/downloads/book/book.pdf) (Lattimore & Szepesv√°ri, 2020) - Chapters 18-20 on stochastic contextual bandits (derivations of confidence bounds and regret).
- **LinUCB Paper:** Li et al.¬†(2010), [*A Contextual-Bandit Approach to Personalized News Article Recommendation*](https://arxiv.org/abs/1003.0146) - Introduction of **LinUCB**, a seminal algorithm for contextual bandits.
- **Classic Theory:** Auer et al.¬†(2002), [*Finite-time Analysis of the Multiarmed Bandit Problem*](https://link.springer.com/article/10.1023/A:1013689704352) - UCB1 algorithm and regret proof (a foundational result in bandit theory).
- **Key Concepts:** Action-Value Uncertainty, Upper Confidence Bound (UCB), Thompson Sampling (intro), Cumulative Regret, Exploration vs.¬†Exploitation.
- **Exercises:**
- *Coding:* Implement a **LinUCBAgent** class in Python. Use efficient updates (e.g.¬†Sherman-Morrison formula) to achieve $O(d^2)$ per-step updates instead of naive $O(d^3)$. Test it on a simulation with a few arms and measure regret.
- *Theory:* Derive the regret bound $R_T = O(d\sqrt{T})$ for a linear bandit. Explain the role of confidence ellipsoids in the proof. Why does higher dimension $d$ worsen the worst-case regret?
- *Application:* Consider a simplified Sqwish scenario with 3 prompt variants. Formulate it as a contextual bandit: define the context (e.g.¬†user embedding), actions (prompts), and reward (e.g.¬†conversion vs no conversion). How would UCB decide which prompt to show early on versus after gathering data?

**Sqwish Connection:** Every user query is a one-shot decision for us - which prompt or model or context or tool to use. That‚Äôs a contextual bandit problem. Mastering bandit algorithms means understanding how Sqwish learns to serve the best configurations while still exploring new ones in production.

### **Day 2: Scaling Up - High-Dimensional & Continuous Action Bandits**

**Objective:** Tackle the practical challenges when contexts or actions don‚Äôt fit neat small-scale settings. Production systems use high-dimensional embeddings and sometimes *continuous* action spaces (e.g.¬†tuning a temperature hyperparameter). Learn techniques for scaling linear bandits and handling infinitely many actions.

**Reading List:**

- **High-Dim Bandits:** [*Scalable LinUCB: Low-Rank Design Matrix Updates for Large Action Spaces*](https://arxiv.org/abs/2501.00000) (2025) - Introduces matrix sketching to maintain a low-rank approximation of the covariance matrix, cutting per-step complexity.
- **Kernel Approximation:** Jun et al.¬†(2020), [*Efficient and Robust High-Dimensional Linear Contextual Bandits*](https://arxiv.org/abs/2007.10093) - Proposes **Spectral Covariance** (SCFD) to keep top-$k$ singular vectors and adjust for discarded variance, ensuring UCB optimism remains valid.
- **Continuous Actions:** Krishnamurthy et al.¬†(2019), [*Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting*](https://arxiv.org/abs/1902.01520) - Framework for infinite arms using Lipschitz assumptions and kernel-based smoothing. Presents the **CATS** algorithm that selects near-optimal continuous actions via adaptive discretization.
- **Key Concepts:** Curse of Dimensionality, Matrix Sketching (Frequent Directions), Low-Rank Approximation, **Continuous Action Bandits**, Smoothing Kernels, CATS Algorithm, Policy Gradient vs.¬†Bandit approach to continuous optimization.

**Exercises:**

- *Coding:* Extend your LinUCB implementation to a **SketchedLinUCB**. Use a rank-$r$ matrix approximation (e.g.¬†Frequent Directions) to update an approximate covariance matrix quickly. Evaluate its performance vs.¬†full LinUCB on a context dimension $d=1024$ with 10 arms. Plot latency vs.¬†regret as you vary the sketch rank - this illustrates the trade-off (Pareto frontier) between computational efficiency and decision quality.
- *Application:* Implement a simple **continuous bandit** using a smoothing approach. For example, optimize an LLM‚Äôs **temperature** setting (a continuous action in [0,1]) to maximize a reward signal (like user rating). Use a grid of possible temperatures and apply UCB with a Gaussian kernel around tried values. How does it choose diverse temperatures initially and then focus near the best one?
- *Theory:* Explain how a smoothing kernel helps a continuous bandit algorithm. In particular, describe how the kernel in CATS bridges discrete and continuous actions by generalizing the notion of ‚Äúnearby‚Äù actions having similar rewards. What assumptions are needed (e.g.¬†Lipschitz continuity), and how do they reflect in regret bounds?

**Sqwish Connection:** Our system often optimizes continuous parameters on the fly - think of dynamically adjusting an LLM‚Äôs top-p or temperature to balance creativity vs.¬†accuracy. Techniques like CATS allow us to treat these as arms in a bandit, so we can fine-tune generation style in real-time rather than hardcoding settings.

### **Day 3: Advanced Exploration - Thompson Sampling & Beyond**

**Objective:** Delve into stochastic exploration strategies and other advanced bandit techniques. Thompson Sampling (TS) offers a Bayesian, randomized alternative to UCB that often performs better in practice. Understand its behavior, regret analysis, and specialized bandit variants that reduce variance (like action-centering). This will build intuition for handling uncertainty in more complex RL scenarios.

**Reading List:**

- **Thompson Sampling Tutorial:** Russo et al.¬†(2018), [*Tutorial on Thompson Sampling*](https://arxiv.org/abs/1707.02038). Covers the probability-matching principle and connections between TS and UCB.
- **Regret Bounds for TS:** Agrawal & Goyal (2013), [*Thompson Sampling for Contextual Bandits*](https://arxiv.org/abs/1209.3352) - Proves a $O(d\sqrt{T})$ regret bound for linear TS, showing TS can match UCB‚Äôs order of performance.
- **Action-Centered Bandits:** Greenewald et al.¬†(2017), [*Action-Centered Contextual Bandits*](https://arxiv.org/abs/1706.04687). Introduces modeling reward as $r(x,a) = f(x) + \delta(x,a)$ to reduce variance - useful when user context dominates outcomes and actions have small incremental effects.
- **Key Concepts:** Thompson Sampling (Probability Matching), Posterior Sampling, Beta/Bernoulli bandit (simple case), Bayesian vs Frequentist regret analysis, **Delayed Feedback** handling, Action-Centering for low-variance reward estimation.

**Exercises:**

- *Math:* Why is Thompson Sampling asymptotically optimal for the simple multi-armed (Bernoulli) bandit? Outline the argument that TS achieves logarithmic regret in that setting. Then discuss why proving similar guarantees for contextual TS was historically challenging (hint: TS‚Äôs random exploration is harder to analyze than UCB‚Äôs deterministic optimism).
- *Coding:* Implement **Linear Thompson Sampling** using a Bayesian linear regression approach. Assume a Gaussian prior on $\theta$ for each arm and update posteriors with incoming rewards. Test it vs.¬†LinUCB on a synthetic contextual bandit where rewards have a 100-step delay (i.e., you only observe reward 100 time steps after the action). Which algorithm‚Äôs performance degrades more, and why?
- *Analysis:* In an **action-centered bandit** formulation, explain how separating $f(x)$ (context baseline) from $\delta(x,a)$ (action effect) can improve learning. If one arm is overall best for all users but another arm is slightly better for a small segment, how would an action-centered approach help detect the latter? What does this imply for personalization in Sqwish‚Äôs setting (e.g.¬†one prompt is usually best, but a different prompt works better for a niche user group)?

**Sqwish Connection:** Many of our optimizations involve uncertainty - e.g.¬†not knowing how a new prompt variant will perform. Thompson Sampling‚Äôs randomized exploration is particularly useful in non-stationary and delayed-feedback situations that occur in RLHF loops or when optimizing long-horizon user engagement. Also, by focusing on *action effects*, we ensure our algorithms can catch subtle improvements (like a special prompt that only matters for a subset of users) without getting lost in the noise of dominant context signals.

### **Day 4: Off-Policy Evaluation (OPE) - Counterfactual Policy Testing**

**Objective:** Learn how to evaluate a new policy using **historical data** *without deploying it*. Off-policy evaluation is critical for safe experimentation: before we let a new prompt-selection or model-routing strategy control real user traffic, we estimate its performance using logs from the current system. Mastering OPE methods (and their bias-variance trade-offs) will enable you to validate improvements offline.

**Reading List:**

- **OPE Fundamentals:** Dud√≠k et al.¬†(2011), [*Doubly Robust Policy Evaluation and Learning*](https://arxiv.org/abs/1103.4601). Introduces **Inverse Propensity Scoring (IPS)** and **Doubly Robust (DR)** estimators for unbiased value estimation using logging policies.
- **Modern OPE & Diagnostics:** Wang et al.¬†(2017), [*Optimal Off-Policy Evaluation with the SWITCH Estimator*](https://arxiv.org/abs/1612.01205). Proposes the **SWITCH estimator**, which adaptively combines IPS and model-based estimates to control variance.
- **Practical Example:** Kiyohara et al.¬†(2025), [*Prompt Optimization with Logged Bandit Data*](https://arxiv.org/abs/2501.00000) (selected sections) - Demonstrates OPE in the context of prompt selection for LLMs using real logged interactions, highlighting common pitfalls like covariate shift.
- **Key Concepts:** **Inverse Propensity Score (IPS)** weighting, Self-Normalized IPS, Doubly Robust Estimator, Reward Model (Direct Method), **SWITCH / WDR** (Weighted Doubly Robust) technique, High-variance bias, Importance Weight truncation.

**Exercises:**

- *Design:* Suppose our current system uses an $\epsilon$-greedy policy for model selection (mostly choosing GPT-5, occasionally a cheaper model). Define a logging schema for each interaction: what context, action, and probability information must we log to enable unbiased OPE of a new routing policy? Outline the data structure clearly.
- *Coding:* Implement **IPS and Doubly Robust** estimators for off-policy policy value. Use a synthetic logged dataset (e.g., generated by a known policy on a multi-armed bandit) and a candidate target policy. Compare their estimates to the ground-truth value. Then implement the **SWITCH estimator**: for each instance, decide to use IPS or a model prediction based on whether the importance weight is below a threshold. Show that SWITCH yields lower Mean Squared Error than plain IPS when the target policy is significantly different from the logging policy.
- *Theory:* Explain why off-policy evaluation is fundamentally hard when the new policy differs a lot from the logging policy (the ‚Äúcoverage‚Äù problem). What is **propensity overfitting** and how do techniques like weight clipping or SWITCH address it? Discuss how OPE at Sqwish helps us be **safe** - e.g.¬†detecting if a proposed change (like more aggressive exploration) could tank reward for a subset of users **before** we deploy it live.

**Sqwish Connection:** In a live system that learns from user feedback, we can‚Äôt recklessly try arbitrary new strategies without first estimating their effects. OPE is our safety net. For example, if we devise a new prompt generation policy, we run it through logged data via IPS/DR to ensure it likely improves conversion or quality **without** negatively impacting certain user groups. Only then do we roll it out cautiously. This keeps our optimization agile but safe and evidence-driven.

## **Week 2: Adapting LLMs - Reward Models, RLHF, and Prompt Optimization**

### **Day 5: Reinforcement Learning from Human Feedback (RLHF)**

**Objective:** Understand the standard RLHF pipeline for aligning LLMs with human preferences, and the challenges therein. We will dissect the classic approach using Proximal Policy Optimization (PPO), which fine-tunes an LLM with a learned reward model. Grasp why PPO-based RLHF can be unstable and resource-intensive, motivating newer approaches.

**Reading List:**

- **RLHF Overview:** Ouyang et al.¬†(2022), [*Training language models to follow instructions (InstructGPT)*](https://arxiv.org/abs/2203.02155) - Section on human feedback training loop. Real-world success of RLHF with PPO for LLM alignment.
- **PPO Tricks:** ICLR Blog (2022), [*The 37 Implementation Details of Proximal Policy Optimization*](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) - A detailed guide on the many practical knobs in PPO (clipping, normalization, etc.), highlighting why tuning PPO for LLMs is non-trivial.
- **RLHF vs.¬†DPO:** Xu et al.¬†(2024), [*Is DPO Superior to PPO for LLM Alignment?*](https://arxiv.org/abs/2404.10719) - A study comparing PPO-based RLHF with the emerging Direct Preference Optimization method, previewing tomorrow‚Äôs topic.
- **Key Concepts:** Reward Modeling (training a scalar reward $r_\phi(x,y)$ from human preference data), **KL-penalty** in PPO objective, Policy vs.¬†Value networks, **Generalized Advantage Estimation (GAE)**, Instability issues (mode collapse, reward hacking).

**Exercises:**

- *Understanding:* Recreate the **RLHF loop** in your own words: first supervised fine-tune (SFT) a model, then train a reward model on comparisons, then run PPO. Why is the KL divergence term (keeping $\pi_\theta$ close to a reference model $\pi_{\text{ref}}$) crucial? What happens if $\beta$ (the KL penalty coefficient) is set too low or zero?
- *Coding:* Implement the **Generalized Advantage Estimator (GAE)** for PPO. Given a sequence of rewards and value estimates, code the calculation of advantages with a decay parameter $\lambda$. Verify your implementation on a small synthetic sequence by comparing to the definition.
- *Experiment:* (Thought experiment or optional coding) Consider an LLM fine-tuned with PPO on a reward model that highly values verbosity. Describe how the generated outputs might drift if $\beta$ is not high enough. How would you detect *reward hacking* in practice (e.g., the model finds loopholes in the reward)?

**Sqwish Connection:** While we do not fine-tune base LLM weights in production, RLHF concepts are foundational for how we treat *prompts and outputs*. We often use **LLM-based reward models** (like an LLM-as-judge) to score outputs when real user reward is delayed or sparse. Understanding PPO-based RLHF gives insight into how optimizing against a proxy can go awry and why careful tuning (or alternative methods) are needed to keep the model‚Äôs behavior aligned with user intent.

### **Day 6: Direct Preference Optimization (DPO)**

**Objective:** Master **Direct Preference Optimization**, a cutting-edge alternative to PPO that sidesteps the need for a separate value model by turning the RL problem into a *logistic regression on preferences*. Learn the derivation of the DPO loss from first principles and why it tends to be more stable and efficient for aligning LLMs with human preferences.

<aside>
üí°

**First, a stepping stone: GRPO.** What if PPO didn't need a critic? **Group Relative Policy Optimization** (DeepSeek, 2024) samples multiple responses per prompt and compares them *relative to each other*: $\hat{A}_i = (r_i - \bar{r})/\sigma$. No value network, 50% less memory. The punchline: when group size = 2, GRPO reduces to contrastive preference learning: mathematically close to DPO (Wu et al., 2025, "It Takes Two"). This tells us preference-based and reward-based RL are deeply connected. DPO takes this further by removing the reward model entirely.

</aside>

**Reading List:**

- **GRPO:** Shao et al. (2024), [DeepSeekMath: Pushing the Limits of Mathematical Reasoning](https://arxiv.org/abs/2402.03300) - Section 3 introduces GRPO. Also see Wu et al. (2025), [It Takes Two: Your GRPO Is Secretly DPO](https://arxiv.org/abs/2510.00977) for the theoretical connection.
- **DPO Introduction:** Rafailov et al.¬†(2023), [*Direct Preference Optimization*](https://arxiv.org/abs/2305.18290) - Original paper proposing DPO. Focus on Sections 1-3 explaining how DPO reparametrizes the RLHF objective.
- **Theoretical Insights:** Azar et al.¬†(2024), [*A Theoretical Perspective on Preference Optimization*](https://arxiv.org/abs/2405.17230) - Connects DPO to general preference-based learning and bandit algorithms. Offers deeper insight into why DPO works and how it relates to other approaches like PPO or supervised fine-tuning.
- **Comparison Study:** (Revisit) Xu et al.¬†(2024), [*Is DPO Superior to PPO for LLM Alignment?*](https://arxiv.org/abs/2404.10719) - particularly the sections analyzing where DPO outperforms PPO in practice.
- **Key Concepts:** KL-regularized RL objective, **Boltzmann Optimal Policy** (optimal $\pi^*(y|x) \propto \pi_{\text{ref}}(y|x)\exp(r(x,y)/\beta)$), Bradley-Terry model for pairwise preferences, DPO loss function (logistic loss comparing preferred vs.¬†dispreferred outputs), Implicit reward in DPO.

**Exercises:**

- *Derivation:* Starting from the optimal policy form $\pi^*(y|x)$ with a KL penalty, derive step-by-step how inserting this into a pairwise preference likelihood leads to the DPO loss. Show how the unknown true reward $r(x,y)$ cancels out, yielding a purely probability-based objective. This derivation cements the intuition that DPO ‚Äúmakes the reward model unnecessary.‚Äù
- *Math:* Prove that the DPO loss is convex in the model‚Äôs logit difference (assume a fixed reference $\pi_{\text{ref}}$). Why is this convexity an appealing property compared to the non-convex PPO objective? (Hint: Logistic loss is convex.)
- *Coding:* Implement the DPO loss function and a simple gradient ascent on it for a toy problem. For example, let $\pi_{\text{ref}}$ be a fixed distribution over two outcomes, and simulate some ‚Äúhuman‚Äù preference data. Show that training $\pi_\theta$ with DPO reliably increases probability of the preferred outcome. Compare this to a PPO-style training on the same data and note any differences in stability (e.g., PPO might oscillate without careful hyperparameters).

**Sqwish Connection:** DPO gives us a powerful tool to quickly align models with new feedback. At Sqwish, we can interpret an online learning loop as continuously generating preferences (A vs B outcomes) - DPO allows us to update the policy (which might be ‚Äúwhich prompt to choose‚Äù rather than model weights) efficiently. It‚Äôs like doing RLHF but treating it as a simpler supervised update. This is especially useful in an online setting where data is streaming and we need stable, fast convergence.

### **Day 7: Online & Fast Adaptation - Iterative DPO and Beyond**

**Objective:** Transition from offline preference optimization to *online, iterative* updates suitable for a live system. Learn how to continuously update a policy with new feedback while avoiding catastrophic forgetting. Specifically, study techniques like **Online DPO** and the ‚ÄúFast-Slow‚Äù two-speed models approach that stabilize training on non-stationary data.

**Reading List:**

- **Online DPO:** Ramamurthy et al.¬†(2024), [*Online Direct Preference Optimization with Fast-Slow Chasing*](https://arxiv.org/abs/2406.13548) - Presents the **OFS-DPO** algorithm that uses two models: a fast adapter and a slow stabilizer.
- **Iterative Feedback Learning:** Wu et al.¬†(2024), [*Iterative Preference Learning from Human Feedback: Bridging Theory and Practice*](https://arxiv.org/abs/2312.11456) - Discusses how to iteratively collect new preferences and update, with attention to distribution shift and sample efficiency.
- **Experience from RLHF:** (Blog) *OpenAI: Iterative Approaches in RLHF* - Insights on how policies are periodically re-generated to gather fresh comparison data. Optional but useful for practical perspective.
- **Key Concepts:** **Distribution Shift** in iterative policy improvement (policy‚Äôs own improved outputs differ from original data), Online exploration for new feedback, **Fast-Slow Model (OFS-DPO)** - maintaining a quickly adapting ‚Äúfast‚Äù policy and a slowly moving ‚Äúreference‚Äù to regularize it, Intraspecific competition (fast vs.¬†slow) to prevent drift.

**Exercises:**

- *Conceptual:* Why does a static preference dataset become insufficient as a policy improves? Explain using an example: initial model outputs are poor, so human preferences cover only easy mistakes; once the model stops making those, the old data is less relevant. How does *online DPO* address this?
- *Coding:* Implement a simple simulation of **OFS-DPO**. Use two copies of a model (e.g., two sets of parameters representing fast and slow). At each iteration: have the fast model generate an output for a query, label its quality with a simulated metric (e.g., a known reward function or a proxy judge), update fast model via DPO loss, and occasionally copy fast weights to slow. Monitor a metric (like the difference between fast and slow policy outputs or rewards over time) to verify that fast adapts quickly and slow provides a stabilizing anchor.
- *Discussion:* The fast-slow approach mimics having an exploitative agent and a conservative baseline. In a production system like Sqwish, how might we implement this practically? (Hint: The ‚Äúfast‚Äù could be a live updated prompt policy, and the ‚Äúslow‚Äù could be a periodically retrained baseline that prevents the prompt strategy from drifting too far and causing bad experiences.) Propose a mechanism for deciding when to sync the slow model with the fast one (e.g., based on performance plateau or time interval).
- *Discussion (Context vs. Weight Adaptation):* The fast-slow paradigm updates model *weights* online. An alternative is updating *contexts* while keeping weights frozen: ACE (Day 10) does exactly this with evolving playbooks. Compare: **weight-based** (changes behavior fundamentally, requires gradients, risk of forgetting) vs. **context-based** (faster, interpretable, limited by context window, no forgetting). When might you prefer each? Consider: (1) dramatic domain shifts, (2) incorporating a single new policy, (3) regulated industries requiring auditability.

**Sqwish Connection:** Our optimization loop is **continuous** - the system is always trying new tweaks and learning from user clicks, ratings, or other signals. Techniques like Online DPO ensure we can adapt on the fly without losing the lessons of the past. The Fast-Slow paradigm in particular is very relevant: it‚Äôs like having an experimental wing of our algorithm trying bold changes (fast) while another part ensures we don‚Äôt stray beyond proven boundaries (slow). This keeps our real-time learning *agile yet safe*.

### **Day 8: LLM-as-a-Judge - Reward Models and Calibration**

**Objective:** Address a unique challenge in our domain: often there is **no immediate numeric reward** for an LLM‚Äôs output (e.g.¬†how do you quantify answer quality on the fly?). We use LLM-based judges as proxy reward providers. Learn how to design these judge models, understand their biases, and apply statistical calibration so their evaluations can be trusted for optimization.

**Reading List:**

- **LLM Judges Pitfalls:** OpenAI (2023), [*How to Correctly Report LLM-as-a-Judge Evaluations*](https://arxiv.org/abs/2406.12624) - Identifies biases like position bias and verbosity bias when using one LLM to grade another‚Äôs output. Discusses statistical techniques to adjust for these biases.
- **Evaluating the Evaluators:** Chan et al.¬†(2024), [*Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges*](https://arxiv.org/abs/2406.12624) - Analyzes reliability of various LLM judges, and where they can be fooled or skewed. Proposes some calibration strategies.
- **Practical Guide:** N. Islam (2024), [*Scaling Evaluation with LLM Judges - Our Approach and Findings*](https://www.anthropic.com/research/evaluating-ai-systems) - A Medium blog detailing a framework for using pairwise comparisons and consistency checks (like swap tests) to improve judge reliability.
- **Key Concepts:** **LLM-as-a-Judge Biases** (e.g.¬†always preferring longer answers - verbosity bias, or preferring whichever answer is listed first - position bias), **Swap Test** for consistency (flip A/B order and expect the judge‚Äôs preference to also flip; if not, treat as noise), Sensitivity/Specificity of a judge and how to calibrate it as a noisy classifier.

**Exercises:**

- *Coding:* Build a simple **Pairwise Judgment Evaluator**. Use an API like GPT-5 to compare two responses. Implement a **swap test**: for a given pair (response A vs B), have the judge pick a winner; then present the responses in reversed order (B vs A) and get a winner. If the judge is inconsistent (prefers A then B), record this as a potential bias instance and default to ‚Äútie‚Äù or require human review. Test this on a few known cases (you can fabricate scenarios, like A is longer but correct, B is shorter but slightly less verbose, etc.). Measure the **agreement rate** of the judge with itself under swapping.
- *Analysis:* You have a small set of prompts where you also collected **human evaluations** of outputs. Use this as a calibration set for your LLM-judge. For each prompt, you have the judge‚Äôs preferred answer and the human‚Äôs preferred answer. How would you estimate the judge‚Äôs error rates (false positive/negative) from this? Outline how to adjust the judge‚Äôs scores statistically so that, say, a 70% win rate reported by the judge comes with a confidence interval for the true human preference rate.
- *Discussion:* Imagine our Sqwish system using an LLM judge to decide which of two prompts led to a better answer (when user feedback is not immediately available). What are the risks if we blindly trust the LLM judge‚Äôs verdicts? List at least two failure modes (e.g.¬†the judge might prefer flowery language even if factual accuracy suffers). How can the team mitigate these? (Think in terms of periodic human audits, calibration as above, or mixing in some known test queries with correct answers.)

**Sqwish Connection:** In live optimization, we often need a quick proxy for ‚Äúwas this output good?‚Äù LLM judges fill that role by providing a learned notion of quality or preference. But an uncalibrated judge can mislead the optimizer - for example, it might systematically prefer one style of answer that users actually don‚Äôt. By carefully calibrating judges and checking their consistency, we ensure our reward signals (whether they‚Äôre from humans or AI judges) truly reflect what we want to optimize. This is crucial for avoiding optimization in the wrong direction.

## **Week 3: System Optimization - Routing, Prompt Engineering, and Multi-Objective Trade-offs**

### **Day 9: Dynamic Model Routing - FrugalGPT and Beyond**

**Objective:** Learn how to build an intelligent router that chooses between multiple LLMs for a given request to optimize cost and performance. We‚Äôll cover **LLM cascades** (e.g.¬†try cheap models first, escalate to expensive ones only if needed) and using preference-trained routers to decide which model can handle a query. This is central to Sqwish‚Äôs cost-efficiency strategy.

**Reading List:**

- **LLM Cascades:** Chen et al.¬†(2023), [*FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance*](https://arxiv.org/abs/2305.05176). Introduces the idea of cascading models from cheap to expensive, achieving up to 98% cost reduction by only calling big models when necessary. Key for understanding cost-quality trade-offs.
- **Learning to Route:** Ramaswamy et al.¬†(2025), [*RouteLLM: Learning to Route LLMs from Preference Data*](https://arxiv.org/abs/2406.18665). Shows how to train a classifier (router) on logged comparisons to predict which model will perform best for a given input. Achieved 2-4√ó cost savings at equal quality by routing queries intelligently.
- **Multi-Model RL:** Zhang et al.¬†(2025), [*Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via RL*](https://arxiv.org/abs/2503.00000). (Optional frontier reading) Uses reinforcement learning to train an agent that can route a query through *multiple* models over multiple steps, learning an optimal sequence (like first ask a small model, then a big one to refine, etc.).
- **Key Concepts:** **LLM Cascade** (progressive model escalation), Routing as a Contextual Bandit (context = query features, arms = model choices, reward = e.g.¬†accuracy minus cost), Cost-Quality Thresholding (only use expensive model if cheap model‚Äôs confidence < threshold), Training a Router via **preference modeling** (e.g.¬†using past data where one model‚Äôs answer was preferred over another‚Äôs for similar queries).

**Exercises:**

- *Coding:* Simulate a **FrugalGPT cascade**. Assume you have two models: A (cheap, 80% accuracy) and B (expensive, 90% accuracy). Define a simple confidence heuristic for model A (e.g.¬†length of answer or presence of a certain keyword). Implement a policy that calls A, checks confidence; if confident, use A‚Äôs answer, if not, call B. Generate a dataset of queries with ‚Äúground truth‚Äù answers and simulate the cascade, measuring overall accuracy and cost. Compare this to always using B and always using A. Show how varying the confidence threshold produces a **Pareto curve of cost vs.¬†accuracy**.
- *Coding:* Train a basic **router model**. Using an open dataset like *LMSYS Chatbot Arena* results, extract features (possibly the user query text or embedding) and labels (which model among a pair won). Train a classifier (e.g.¬†a small BERT or even logistic regression on embedding features) to predict if a cheaper model‚Äôs output will be rated as good as GPT-5‚Äôs. Then evaluate: for new queries, use the classifier‚Äôs prediction to decide routing (cheap vs.¬†expensive). How much cost can you cut while maintaining quality above a threshold?
- *Application:* In production, routing decisions might also consider *latency*. Discuss how you would extend the bandit formulation to include latency constraints. (Hint: this becomes a **multi-objective** problem - you might impose a penalty on using a slower model. How could you incorporate that? Perhaps as a negative reward for exceeding a latency target, or as an additional arm that is ‚Äúdo nothing and respond with an apology‚Äù if all models are too slow.) Outline an approach and any algorithms that could handle such constrained bandit problems.

**Sqwish Connection:** We promise our customers ‚Äúthe right model for the job‚Äù - that means sometimes a quick answer from a smaller model is good enough (saving cost and time), and other times we pull out the big guns (GPT-5 class) for complex queries. The routing logic that decides this is powered by bandit algorithms and learned predictors. By mastering model routing, you‚Äôll directly contribute to the part of our stack that *simultaneously optimizes quality, cost, and latency* - essentially a live multi-objective optimization with each request.

### **Day 10: Autonomous Prompt Optimization - DSPy, Evolutionary Strategies, and More**

**Objective:** Move beyond manual prompt engineering. Learn about **programmatic frameworks** and algorithms that automatically optimize prompts and multi-step LLM workflows. We will explore *DSPy* (a system that treats prompts as tunable parameters) and methods like **TextGrad** (differentiating through text generation) and **GEPA/EvoPrompt** (using evolutionary algorithms to improve prompts). By the end, you should know how to set up an automated prompt optimizer that can outperform human-crafted prompts.

**Reading List:**

- **Prompt Pipeline Optimization:** Khattab et al.¬†(2023), [*DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines*](https://arxiv.org/abs/2310.03714). Shows how to define a multi-stage LLM pipeline in a declarative way and let a compiler (‚ÄúTeleprompter‚Äù) optimize its prompts/demonstrations to maximize an objective. Demonstrates huge gains (25%+ improvement) over few-shot baselines by automatic prompt tuning.
- **Textual Backpropagation:** Weng et al.¬†(2024), [*TextGrad: Automatic ‚ÄúDifferentiation‚Äù via Text*](https://arxiv.org/abs/2406.07496). Introduces a technique to propagate errors backward through a chain of text-generation steps (e.g.¬†if final answer is wrong, adjust earlier prompt text). It treats text edits like gradient steps, bridging RL and direct editing.
- **Evolutionary Prompt Search:** Agrawal et al.¬†(2025), [*GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning*](https://arxiv.org/abs/2507.19457) and Guo et al.¬†(2024), [*EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers*](https://arxiv.org/abs/2309.08532). These methods use **LLMs themselves as mutation/crossover operators** to evolve better prompts generation after generation. Notably, GEPA uses LLM-generated *reflections* on prompt failures to propose improvements, achieving >10% quality gains with 35√ó fewer trials than RL; EvoPrompt shows up to 25% improvement on BigBench Hard tasks by combining LLMs with genetic algorithms.
- **Agentic Context Engineering:** Zhang et al. (2025), [Agentic Context Engineering (ACE)](https://arxiv.org/abs/2510.04618). Treats contexts as **evolving playbooks** rather than static prompts. Uses a Generator-Reflector-Curator architecture with incremental delta updates to prevent context collapse (where iterative rewriting erodes detailed knowledge). Achieves +10.6% on agent benchmarks with 86.9% lower adaptation latency than alternatives. Open-source at [github.com/ace-agent/ace](http://github.com/ace-agent/ace).
- **Key Concepts:** **Language Model Programs (LMPs)**, TelePrompters (automated prompt tuners in DSPy), Few-Shot Example Bootstrapping, **Genetic Algorithms** for prompts (population, mutation, crossover), Fitness Functions for prompts (e.g.¬†exact match on a validation set), **Pareto Optimization** in GEPA (combining prompts that excel on different metrics into a Pareto frontier). **Evolving Playbooks** (contexts as growing knowledge bases vs. concise instructions), **Generator-Reflector-Curator** (three-role agentic system: Generator produces trajectories, Reflector extracts lessons, Curator synthesizes delta updates), **Context Collapse Prevention** (structured incremental updates vs. monolithic rewriting that loses detail), **Offline vs. Online Context Adaptation** (system prompt optimization before deployment vs. test-time memory updates).

**Exercises:**

- *Coding:* Use **DSPy** (or a simplified version if DSPy isn‚Äôt accessible) to optimize a multi-step QA pipeline. For example, pipeline: (1) retrieve relevant text from a small corpus, (2) ask LLM to answer question given retrieved text. Define the metric as accuracy of answer. Let the system tune the retrieval prompt and answer prompt. Observe what changes it makes (e.g.¬†does it add ‚ÄúLet‚Äôs think step by step‚Äù automatically?). Report the before vs after performance.
- *Coding:* Implement a simple version of **EvoPrompt**. Represent a prompt as a list of tokens or words. Define two evolutionary operators: *mutate* (randomly replace or insert a word) and *crossover* (swap a segment between two prompts). Use an LLM (or a heuristic function) to evaluate fitness (e.g.¬†BLEU score or any task-specific score) of prompts. Start with a few initial prompts and run a few generations of evolution. Did the prompts improve? This could be done on a trivial task (like prompt an LLM to output a specific keyword - evolve prompts to maximize the occurrence of that keyword in the response).
- *Theory:* Compare **reinforcement learning vs.¬†evolutionary search** for prompt optimization. If our ‚Äúpolicy‚Äù is the prompt text and the ‚Äúenvironment‚Äù gives a reward (quality score), RL would tweak the prompt based on gradient of reward (if possible) or black-box optimization. Evolutionary methods like GEPA/EvoPrompt treat it like a search over strings. List pros and cons of each: e.g., RL (with methods like RLPrompt or policy gradients) can directly optimize an objective but may get stuck in local optima or require many samples; evolutionary approaches are more global and can incorporate heuristic knowledge (via LLM reflections in GEPA) but might be slower if search space is huge. In practice, why might GEPA‚Äôs ability to incorporate **natural language reflections** be advantageous in prompt tuning?
- *Coding (ACE):* Implement a simplified **ACE-style context evolution** system. Create a playbook with sections: STRATEGIES, CODE_SNIPPETS, PITFALLS. Each entry has an ID, helpful/harmful counters, and content. For a simple task (e.g., math problems), implement: (1) **Generator** produces a solution noting which playbook entries helped/hurt, (2) **Reflector** extracts a lesson from success/failure, (3) **Curator** converts the lesson to a structured bullet, checks for duplicates, and merges with counters. Run 20-30 iterations. Compare against a baseline that concatenates all lessons without structure. Does the structured approach prevent context collapse and preserve early lessons?

**Sqwish Connection:** Our platform continuously **learns better prompts** for each stage of an AI workflow - whether it‚Äôs retrieving knowledge or formatting an answer. We rely on automated prompt optimization to do in hours what would take humans weeks of trial-and-error. By using frameworks like DSPy and algorithms like EvoPrompt/GEPA, we can rapidly adapt prompt strategies to new domains or user behaviors. For instance, if we notice our answer prompts aren‚Äôt eliciting the desired level of politeness or accuracy, the system can autonomously fine-tune those prompts (through evolution or gradient-free search) to improve, as evidenced by 10-20% gains in research. This keeps Sqwish-driven applications constantly improving on the fly, a key competitive edge.

ACE is particularly relevant here: unlike methods that compress prompts (losing domain heuristics), ACE builds **evolving playbooks** that accumulate strategies, code patterns, and failure modes over time. The Generator-Reflector-Curator loop mirrors Sqwish's optimization cycle. Try, learn, consolidate. With 86.9% lower adaptation latency than alternatives, ACE enables real-time self-improvement without human intervention, exactly what enterprise customers need for domain-specific expertise that grows with each interaction.

### **Day 11: Optimizing Retrieval-Augmented Generation (RAG) with Bandits**

**Objective:** Extend optimization to **retrieval-augmented generation** pipelines. Many LLM applications first retrieve documents then generate answers (think open-book QA). Here, we tackle how to optimize the *retrieval component* using bandit and RL methods: deciding which documents to retrieve, how many, or even whether to retrieve at all. You‚Äôll learn to treat the retrieval step as part of the decision-making loop, not a fixed process.

**Reading List:**

- **Adaptive Retrieval:** Qin et al.¬†(2025), [*MBA-RAG: A Bandit Approach for Adaptive Retrieval-Augmented Generation*](https://arxiv.org/abs/2412.01572) - Proposes using multi-armed bandits to dynamically choose retrieval strategies per query. For instance, decide between using a vector search vs.¬†keyword search, or retrieving 5 documents vs.¬†10, based on the query complexity.
- **LLM-Guided Search:** Xie et al.¬†(2025), [*Search-R1: Training LLMs to Reason and Leverage Search Engines with RL*](https://arxiv.org/abs/2503.09516). Trains an LLM agent via reinforcement learning to issue search queries (as actions) and integrate results into its answer. Illustrates a more **agentive approach** where the LLM decides *when and what* to retrieve in multiple steps.
- **Offline Bandit Data for Prompting:** (Reference) Kiyohara et al.¬†(2025), [*Prompt Optimization with Logged Bandit Data*](https://arxiv.org/abs/2501.00000) - previously read in Day 4 - section on using **logged data** to optimize exemplar selection, which is analogous to using logs to optimize retrieval choices. *(Review this if needed for context.)*
- **Key Concepts:** Nonstationary Document Relevance (some queries need lots of context, others none), **Multi-armed bandit for retrieval** (arms could be: retrieve nothing, retrieve from source A, retrieve from source B, etc.), **Reward design** for RAG (final answer quality vs.¬†cost of retrieval), Off-policy learning from logs (using prior QA logs to train retrieval policies), **LLM-Agent RL** (LLM taking actions like a sequence of tool uses, optimized via policy gradient or DPO).

**Exercises:**

- *Design:* Formulate a **bandit problem for retrieval** in a QA system. Say you have two document indexes (DocsA and DocsB) and also the option to not retrieve at all. How would you set up: contexts (features of the query, like length or topic), actions (which index or none), and reward (e.g.¬†+1 if the answer was correct and -0.1 per document retrieved to penalize cost). Is this reward structure multi-objective (accuracy vs.¬†cost)? How would you incorporate the cost in a bandit reward or would you treat it separately?
- *Coding:* Given a small QA dataset and two retrieval strategies (e.g., keyword search vs.¬†dense embedding search), simulate an **adaptive retrieval policy**. Implement a simple bandit (like $\epsilon$-greedy or Thompson Sampling) that chooses strategy per question and receives a reward of 1 if the retrieved set contained the answer. Over many questions, watch the bandit‚Äôs strategy selection proportions. Does it learn which strategy is generally better? Now, introduce a context feature: e.g., questions containing dates might do better with keyword search. Modify the bandit to be contextual (e.g.¬†a LinUCB on a feature like ‚Äúhas date or number‚Äù). See if it learns a *policy*: keyword for date queries, dense for others.
- *Future Thinking:* In Search-R1, an LLM is trained to decide *when to call a search API* during its reasoning. Why is this a RL problem and not a supervised one? (Hint: the optimal points to call search aren‚Äôt known in the training data; the model must *discover* them by trial and error, guided by a reward like final answer correctness.) If you were to integrate this into Sqwish, what kind of **feedback signal** could train such behavior? Describe a possible reward function for an LLM agent that can either answer directly or decide to issue a search query and then answer. How would you ensure it doesn‚Äôt over-use the search (to minimize latency) or under-use it (and risk being wrong)?
- *Connection to Context Engineering:* Consider ACE's playbook (Day 10) as **learned retrieval context,** instead of retrieving from a static corpus, the system builds its own knowledge base through experience. How might you combine ACE with traditional RAG? One approach: use RAG for factual, up-to-date information while using an ACE playbook for procedural knowledge (strategies, patterns, failure modes). The playbook could even store *meta-knowledge* about retrieval itself e.g., "For financial questions, retrieve from SEC filings first." Design a hybrid architecture.

**Sqwish Connection:** Many of our enterprise users have private knowledge bases or require factually correct answers. We incorporate retrieval (RAG) to ground LLM responses. But how many docs to fetch? Which search method to use? These are decisions that a one-size-fits-all approach can‚Äôt optimize. By treating retrieval as part of the learning loop, Sqwish can adapt: e.g., for straightforward questions, skip retrieval to save time; for complex ones, pull more context. The result is a smarter system that knows when to ‚Äúopen the book‚Äù and when it can answer from memory, thereby balancing accuracy with efficiency in a data-driven way.

### **Day 12: Reasoning Strategies & Meta-Optimization of Compute**

**Objective:** Explore frontier ideas on optimizing **inference-time computation** and reasoning. Modern research treats the decision of ‚Äúhow much to think‚Äù or ‚Äúwhich reasoning path to take‚Äù as part of the optimization problem. You‚Äôll learn about using Meta-RL to control test-time computation (like deciding how many reasoning steps to run) and about **risk-sensitive objectives** that prioritize achieving at least one very good outcome (useful in code gen or math problem solving). We also introduce **multi-objective optimization** explicitly here - balancing quality, cost, latency - as a unifying theme.

**Reading List:**

- **Adaptive Compute (Meta-RL):** Setlur et al.¬†(2025), [*Optimizing LLM Test-Time Compute Involves Solving a Meta-RL Problem*](https://blog.ml.cmu.edu/2025/01/08/optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem/) (CMU ML Blog) and the paper: Qu et al.¬†(2025), [*Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning*](https://arxiv.org/abs/2503.07572). Conceptualizes test-time reasoning (like deciding whether to use Chain-of-Thought, how many steps of self-refinement, etc.) as an **outer-loop RL** problem. An agent (controller) decides how much computation to expend on a query to maximize final reward per token used.
- **Risk-Seeking RL:** Jiang et al.¬†(2025), [*Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models*](https://arxiv.org/abs/2509.24261) - Introduces optimizing *metrics like pass@k* (at least one solution correct in k tries) rather than expected accuracy. Uses CVaR (Conditional Value at Risk) or other risk-sensitive criteria to make the policy aim for occasionally great answers rather than consistently average ones, which is useful in scenarios like code generation where one correct answer is enough.
- **Multi-Objective Evolution:** Liu et al.¬†(2023), [*Large Language Model for Multiobjective Evolutionary Optimization*](https://arxiv.org/abs/2310.12541) - Demonstrates using LLMs to help find a **Pareto front** of solutions optimizing multiple objectives simultaneously. (Read for inspiration on how LLMs might assist in multi-objective tuning tasks.)
- **Key Concepts:** **Meta-optimization** (treating the choice of how to solve a problem as an RL problem itself), Test-time compute budget (e.g.¬†decision to do additional reasoning steps if needed), **Risk-sensitive reward** (e.g.¬†giving high reward if at least one attempt succeeds, zero if all fail, which encourages taking bold chances rather than optimizing for average), **Pareto optimality** (a set of solutions where you can‚Äôt improve one objective without worsening another), Scalarization vs.¬†Pareto approach (combining objectives into one vs.¬†maintaining a frontier).

**Exercises:**

- *Conceptual:* Imagine an LLM-based agent that can either answer immediately or invoke a step-by-step reasoning chain (taking more time). Formulate a **Meta-RL** setup: states might include intermediate reasoning results or uncertainties, actions are ‚Äúcontinue reasoning‚Äù or ‚Äústop and output answer‚Äù. What would a reasonable reward be? Perhaps +1 for a correct answer minus a penalty for each step used. Discuss how you would train this (maybe simulate simple math problems of varying difficulty, where continuing reasoning helps for hard ones but is wasteful for easy ones). How is this meta-level decision similar to a bandit? (Hint: each query presents a trade-off between using more resource vs.¬†risk of being wrong, and you have to learn a policy that maps query features to an appropriate compute allocation.)
- *Math/Analysis:* In a **risk-sensitive objective**, say we want to maximize the probability of getting *at least one* correct solution in 3 tries (like pass@3 for coding). If the model‚Äôs per-try success probability is $p$ when optimizing for expected success, it might settle at $p=0.5$ each try (so expected 0.5, and pass@3 ~ 1 - 0.5^3 = 0.875). But a risk-seeking policy might prefer a strategy that yields $p=0.2$ on each try but occasionally a near-perfect attempt, resulting in one attempt being correct with higher probability (this is hypothetical). How would you formalize the reward for pass@3? (One way: reward = 1 if any of the 3 attempts is correct, 0 otherwise.) Why is this a non-linear, non-additive reward that standard RL would struggle with? What algorithms or approaches can handle this (hint: transform it into an auxiliary MDP or use CVaR-based optimization)?
- *Coding (advanced, optional):* Given two objectives, e.g., **Quality** and **Latency**, implement a simple **multi-objective bandit** simulation. Arm A gives high quality (reward1 ~ 0.9) at high latency (reward2 ~ -1.0 second), Arm B gives moderate quality (0.7) at low latency (-0.2). Run a bandit algorithm that tries to maximize a *weighted sum* of these rewards, varying the weight between quality vs.¬†latency. Show how the chosen arm shifts as you change the weight. Then compute the Pareto-optimal set of the two arms (in this case, both might be Pareto-optimal if one is better in quality and the other in latency). Discuss how in a real system we might maintain a *set* of configurations (prompts/models) that lie on the Pareto frontier of (quality, cost, latency), and possibly choose among them based on real-time user preferences or contexts (for instance, a user on a slow device might implicitly prefer faster responses over slightly higher quality).

**Sqwish Connection:** At Sqwish‚Äôs cutting edge, we‚Äôre not just optimizing *what* the model says, but *how* the model thinks. If a user question is easy, we want to answer instantly. If it‚Äôs hard, maybe the system should autonomously decide to do extra reasoning or tool use - effectively ‚Äúthink harder‚Äù - to get it right. This is meta-optimization of the inference process itself. Similarly, our mission is inherently multi-objective: every deployment balances quality, cost, and latency. By understanding Pareto frontiers and risk-sensitive rewards, you‚Äôll be able to design systems that find the *sweet spots* - e.g., a slight sacrifice in latency might yield huge quality gains, which is worth it until a certain point. These advanced concepts ensure we push the envelope of performance while respecting practical constraints, delivering an AI autopilot that‚Äôs not just smart, but also efficient and aligned with user needs.

### **Day 13: Safety Constraints and Safe Optimization**

**Objective:** Focus on the **safety aspects** of online optimization. In real applications, we must optimize rewards **without** crossing certain lines (e.g., generating disallowed content, leaking private info, or tanking user trust). Learn about formulating constraints in RL (Constrained MDPs), techniques like Lagrangian relaxation to enforce them, and specific methods in the LLM context like Safe RLHF and Rectified Policy Optimization (RePO) that aim to keep models within ethical and policy bounds while maximizing utility.

**Reading List:**

- **Safe RLHF:** Bai et al.¬†(2024), [*Safe Reinforcement Learning from Human Feedback*](https://arxiv.org/abs/2506.08266) (OpenReview). Proposes applying **constrained policy optimization** to RLHF, with an explicit constraint (e.g.¬†toxicity must remain below a threshold). Introduces a Lagrange multiplier that adjusts the reward function to heavily penalize unsafe outputs, ensuring the final policy meets safety targets.
- **Rectified Policy Optimization:** Peng et al.¬†(2024), [*Enhancing Safety in RLHF via Rectified Policy Optimization (RePO)*](https://rlj.cs.umass.edu/2025/papers/RLJ_RLC_2025_62.pdf). Augments the training objective with a *rectifier* term: essentially an additional penalty for any action predicted unsafe by a classifier, applied at the **prompt level** to pre-filter potentially unsafe completions. This yields a policy that is proactively constrained.
- **Policy Gradient with Constraints:** Achiam et al.¬†(2017), [*Constrained Policy Optimization*](https://arxiv.org/abs/1705.10528) (background reading, classic in safe RL) - Presents a method to enforce a constraint on expected cost (safety) during policy updates. Good for understanding how theoretical guarantees can be provided for constraint satisfaction.
- **Key Concepts:** **Constrained MDP** (optimize $E[R]$ subject to $E[\text{Cost}] \le \epsilon$), Lagrangian methods (introduce $\lambda$ and optimize $R - \lambda \cdot \text{Cost}$ until constraint satisfied), **Shielding** (preventing certain actions entirely), Safe RLHF specifics (human feedback includes flags for disallowed content, incorporate those into reward shaping or constraints), **RePO** details - adding a term that basically says ‚Äúif this output would trigger the safety filter, hugely downgrade its score‚Äù thus rectifying the policy to avoid that region.

**Exercises:**

- *Application Design:* Suppose our reward in Sqwish has multiple components (user satisfaction + some revenue metric), but we also have a **constraint**: e.g., ‚ÄúDon‚Äôt use more than X tokens on average‚Äù (to control cost) or ‚ÄúAvoid any response that violates content policy (hate, self-harm, etc.) with probability > 0.001‚Äù. Choose one such constraint and outline how you would enforce it during learning. Would you filter out unsafe outputs in data? Use a penalty in the reward (like negative reward for unsafe outcomes)? Use a Lagrange multiplier that dynamically adjusts the weight of the safety penalty until the model meets the criterion? Describe the approach and why it‚Äôs effective.
- *Math:* Write down the Lagrangian $\mathcal{L}(\pi, \lambda) = \mathbb{E}[R(\pi)] - \lambda (\mathbb{E}[C(\pi)] - \epsilon)$ for a simple constrained bandit where $C(\pi)$ is the expected constraint metric (say latency or a risk of violation). Derive the policy gradient that includes the constraint via $\lambda$. Interpret $\lambda$ in this context (hint: if $\lambda$ is high, violating the constraint is very costly to reward - it will push the policy to sacrifice primary reward to satisfy the constraint). How does algorithms like CPO or TRPO ensure the constraint is approximately satisfied at each step?
- *Coding/Analysis:* Consider a simplified **RePO** scenario: You have a classifier that can detect unsafe content in an output with some probability. You incorporate a ‚Äúrectification‚Äù where any output that is flagged as unsafe gets a big negative reward (or is filtered out entirely). Simulate this: take a language model (could be a small one or even a stub function) that sometimes produces a forbidden word. Train a policy (even via simple trial-and-error adjustment) to maximize a reward for helpfulness minus a huge penalty for the forbidden word. Show that over iterations, the forbidden word usage drops to near zero - the policy learns to avoid it, even if that means slightly less reward in other areas. This demonstrates constrained optimization at work.

**Sqwish Connection:** Trust and safety are paramount. Imagine our agent finds a prompt that really drives engagement - great! - but maybe it does so with an inappropriate tone or by exploiting user biases. We *cannot* allow that. So in our optimization loop, every policy update or new experiment must respect constraints (legal, ethical, client-specific). The reading on Safe RLHF and RePO reflects exactly what we aim for: **maximize good outcomes while never crossing the guardrails**. By incorporating safety constraints into the training objective, either as hard filters or soft penalties, we ensure Sqwish‚Äôs ‚Äúautopilot‚Äù not only achieves results but does so responsibly. As you contribute, you‚Äôll be expected to build these safety considerations into every algorithm - whether it‚Äôs bandits with cost constraints or RL with penalty terms for policy violations.

### **Day 14: Evaluation & Monitoring in Production - Closing the Loop**

**Objective:** Learn about the **production ML systems** aspects: once we deploy our agents, how do we continuously monitor and evaluate them? This day is about building the right logging, testing, and evaluation pipelines. We cover advanced off-policy evaluation techniques like **SWITCH** (revisiting Day 4 in context of real data) and also consider evaluation beyond averages (e.g.¬†fairness, subgroup analysis). Essentially, how do we verify our online learning system is performing well and safely *in the wild*?

**Reading List:**

- **Technical Debt & Monitoring:** Sculley et al.¬†(2015), [*Hidden Technical Debt in Machine Learning Systems*](https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html) - Classic paper describing how most issues in ML systems arise not from the model itself, but from the surrounding infrastructure (data pipelines, monitoring, reproducibility). Emphasizes need for robust logging and validation as systems evolve.
- **A/B Testing at Scale:** Agarwal et al.¬†(2016), [*The Decision Service: A/B Testing at Scale*](https://arxiv.org/abs/1606.03966) - Early description of Microsoft‚Äôs contextual-bandit service for automating experimentation. Provides insight into how one can replace traditional A/B tests with continuously learning bandits, and the infrastructure required (e.g.¬†logging every decision and reward).
- **Subgroup Evaluation:** Netflix Tech Blog (2025), [Heterogeneous Treatment Effects at Netflix](https://netflixtechblog.medium.com/heterogeneous-treatment-effects-at-netflix-da5c3dd58833) - How Netflix analyzes when A/B test wins on average might harm specific user segments. Also see: Meta (2023), [Practical Policy Optimization with Personalized Experimentation](https://export.arxiv.org/pdf/2303.17648v1.pdf) - Describes how standard experimentation doesn't work well when treatment effects vary across users. The lesson: always evaluate not just aggregate reward, but also by segments (to detect if an optimization inadvertently hurts a minority of cases).
- **Key Concepts:** **Data Logging Schema** (what to log for each interaction: context, action, probabilities, reward, etc.), Real-time Monitoring Metrics (reward trends, regret, safety incident counts), **Off-Policy Evaluation in Production** (using logged data to estimate if a proposed change would help, before deploying it - ties back to Day 4/14 content), **Canary deployments** (try new policy on small % of traffic and compare), **Bias & Fairness checks** (evaluate performance for different user segments to ensure no regression in any critical segment), Feedback loops and non-stationarity detection (monitor if reward distribution shifts indicating user behavior change).

**Exercises:**

- *Design:* Propose a **logging schema** for Sqwish‚Äôs online learning system. It should record everything needed to reproduce training and do later analysis. For each user request, what would you log? (Think: a unique request ID, user context features, chosen prompt/model, all model outputs maybe, any user click or outcome, timestamps, the probability or propensity of the chosen action if using a stochastic policy, etc.). Write a structured list of fields and justify each (why is it needed? e.g.¬†propensity is needed for IPS in OPE).
- *Analysis:* You have deployed a bandit that optimizes prompts. After a week, you examine results and see overall user satisfaction went up 5%. However, for new users (first-time visitors) satisfaction dropped. How would you investigate this? Outline an experiment or analysis using logs to diagnose why the policy might be underperforming for new users (maybe it over-explored or didn‚Äôt personalize properly for cold-start users). What changes to the algorithm could you consider (e.g.¬†epsilon-greedy for new users until enough data)?
- *Coding:* Using an open bandit dataset (e.g.¬†the Open Bandit Pipeline‚Äôs logged data if available, or simulate one), perform an **off-policy evaluation** of a hypothetical new policy. For example, use logged data from a uniform random policy on a classification task as bandit feedback. Define a new deterministic policy (like always choose arm 1 for certain feature values and arm 2 otherwise). Use IPS and Doubly Robust to estimate the new policy‚Äôs reward from the logs. Compare that to the actual reward if you run the new policy on the dataset (if ground truth available). This exercise solidifies understanding of OPE‚Äôs value and limitations (if the policy is very different, IPS variance will be high).
- *Monitoring:* List 3 metrics you would put on a dashboard for the live Sqwish system and why. For example: ‚ÄúCumulative regret‚Äù - to watch if the learning is improving over a non-learning baseline; ‚Äú10th percentile reward‚Äù - to ensure we‚Äôre not badly serving a subset of users even if average looks good; ‚ÄúUnsafe response count per 1000 interactions‚Äù - to catch any increase in policy violations. Explain briefly how each metric helps ensure the system is healthy.

**Sqwish Connection:** This final preparatory day is about being a responsible engineer. Building a clever algorithm isn‚Äôt enough - we must **close the loop** with evaluation and monitoring. Sqwish‚Äôs value prop is continuous improvement, which means we are effectively always running experiments in production. By logging every micro-experiment and regularly evaluating, we can prove to ourselves and our customers that things are getting better (and catch it quickly if not!). For instance, we use OPE to validate changes offline, then deploy carefully, and watch a live dashboard of key metrics (conversion, latency, any safety flags) like a hawk. You‚Äôll be involved in creating those feedback loops - ensuring that when our autopilot takes the controls, we have full visibility and confidence in its behavior.

### **Day 15: Capstone Project - Build the ‚ÄúSqwish Simulator‚Äù**

**Objective:** Synthesize everything you‚Äôve learned by designing and implementing a mini version of Sqwish‚Äôs optimization engine. This capstone project will have you create a simulated environment and then build an agent that optimizes in that environment, incorporating bandits, proxy rewards, and OPE. It‚Äôs a chance to put it all together: bandit algorithms, reward modeling, safety checks, and evaluation in one end-to-end prototype.

**Project Brief:** *E-commerce Description Optimizer.* You will simulate an e-commerce website where an LLM generates product descriptions for users, and the goal is to maximize conversion (purchase) while respecting cost. Three different LLMs (of varying cost and quality) are available. Users have different preferences. You‚Äôll build a contextual bandit agent to route and prompt the LLMs optimally.

**Environment Setup:**

- **User Context:** Define a user persona feature (e.g.¬†budget_sensitive vs quality_seeker) and a product category feature. These together form the context $x$.
- **Arms/Actions:** Three LLM choices for generating the description: *Model A* (cheap & concise), *Model B* (moderate), *Model C* (expensive & detailed). You can also allow the prompt to vary or other actions, but at minimum choosing the model is the action.
- **Hidden Reward Function:** Simulate probability of conversion as a function of context and model. For example: budget_sensitive users convert better with concise Model A (perhaps they don‚Äôt like fluff), quality_seekers convert better with detailed Model C. You can fabricate this mapping, e.g., $P(\text{buy}|x,\text{A}) = 0.05$ normally, but $0.15$ if user is budget_sensitive; $P(\text{buy}|x,\text{C}) = 0.05$ normally, but $0.15$ if user is quality_seeker, etc. The idea is each model is optimal for a certain segment. Conversion is binary (success/fail).
- **Cost Model:** Assign a ‚Äúcost‚Äù to using each model (e.g.¬†A costs  $\$0.01$, B $\$0.02$, C $\$0.10$ per description). This will be used in evaluating the profit.

**Agent Requirements:**

- Use a **Contextual Bandit algorithm** (Thompson Sampling or LinUCB recommended) to learn over interactions which model works best for which context. The agent will make a choice each round (given context, pick model), observe a stochastic reward (1 if conversion happened, 0 if not).
- Incorporate a **Proxy Reward Model (LLM Judge)**: To make it interesting, assume conversion events are rare (maybe users purchase much later). So instead, introduce an immediate proxy reward - e.g., an LLM that scores the description‚Äôs ‚Äúpersuasiveness‚Äù from 0 to 1. The bandit will train on this proxy reward every round (since conversion is delayed), but you will later evaluate on actual conversion. The proxy should be correlated with conversion but not perfect, to simulate reality.
- **Off-Policy Evaluation:** Before fully trusting your learned policy, use an offline evaluation. For example, have the agent do an initial random policy for 1000 interactions to gather a log. Then when your bandit policy is learned, use **IPS or Doubly Robust** on that log to estimate the conversion rate of the bandit policy *without* deploying it. Compare this estimate to the actual performance when you do run the bandit live in the simulator. This checks your OPE integration.
- **Safety Constraint:** Implement a simple safety rule in the simulator (for instance, Model C might occasionally produce an unsafe word). If that happens, the user instantly doesn‚Äôt buy and is unhappy. Ensure your agent either learns to avoid that or you add a filter. (This can be simulated by saying: with small probability, Model C outputs something disallowed, which always results in no conversion; the agent could learn that risk or you can explicitly penalize it.)

**Milestones:**

- *Simulation & Random Policy:* Build the environment and run a random model selection for a few hundred rounds to ensure your simulation is working (you should see, e.g., quality_seekers converting more when Model C is chosen, etc., in the data).
- *Implement Bandit Agent:* Hook up a LinUCB or Thompson Sampling that updates on the proxy reward signal each round. Run it for, say, 1000 rounds. Track the cumulative regret or reward. You should see it start to favor the right model for each context (check the model selection frequencies by context type to see if it matches the hidden optimal mapping).
- *Evaluate:* Calculate the **actual conversion rate** achieved by your agent in the latter part of the simulation (when it has learned). Also calculate the total ‚ÄúProfit‚Äù = (conversion_rate * value_per_conversion - average_cost_per_interaction). Compare this to two baselines: always use cheapest Model A, and always use best-quality Model C. Ideally, your agent should outperform both in profit, by using the expensive model only when it‚Äôs worth it.
- *OPE check:* Using the initial random log data, perform an off-policy evaluation of your learned policy. Report the IPS estimate of conversion vs.¬†the actual conversion your simulation observed with the policy. They should be in the same ballpark, validating your OPE approach (and if not, discuss why - e.g., off-policy might struggle if policy is very deterministic or radically different from logging policy).
- *Documentation:* Summarize your findings. Include a plot of the learning curve (cumulative reward or regret over time) to show the bandit learning. Also include any interesting observation, like ‚ÄúThe proxy LLM judge had a bias which the bandit exploited - it started writing longer descriptions to please the judge, which mostly helped conversions but sometimes not.‚Äù This kind of insight is gold.
- **Stretch goals (if time):** Integrate a **meta-decision**: allow the agent to sometimes choose ‚Äúno generation‚Äù or a very short template if it thinks none of the models would do well (simulating abstaining to save cost on low-value interactions). Or, try a small **meta-learning** twist: after training on one simulated e-commerce category, quickly adapt the agent to a new category with slightly different user behavior using a small number of trials (testing few-shot adaptation).

**Sqwish Connection:** This project is essentially a microcosm of Sqwish‚Äôs production system: multiple models, real-time decisions, balancing reward vs.¬†cost, using proxy signals (LLM judge) when ground truth is delayed, and evaluating everything rigorously before and after deployment. By completing this, you‚Äôll demonstrate end-to-end understanding - from theoretical algorithm to practical implementation and evaluation. In fact, we often use simulators like this internally to validate research ideas before trying them live. Your final deliverable (the Jupyter notebook with results and proposal) could even seed a new experiment in our actual system. Congratulations in advance - this is the culmination of an intense onboarding, and the start of your journey to push the frontier of online LLM optimization at Sqwish!

**Congratulations on completing the 15-day Sqwish Research Onboarding!** Over the past 2 weeks, you‚Äôve built up from fundamentals to frontiers. You‚Äôve read and implemented cutting-edge techniques from bandits and black-box optimization to RLHF, preference learning, prompt evolution, and safe RL. Most importantly, you‚Äôve seen how it all connects in a continuous learning system.

By now, you should be ready to **think like a Sqwish researcher-engineer**: always consider the real-world objectives (business metrics), the constraints (cost, latency, safety), and the data feedback loops. You are equipped to design new algorithms or adapt known ones to our unique setting of *online outcome optimization*. The journey doesn‚Äôt end here - indeed, many open research questions remain (as you saw in Day 15 readings like Casper et al.‚Äôs survey). But you‚Äôre now prepared to tackle them and contribute to our mission of an AI autopilot that learns *safely, efficiently, and autonomously*. We‚Äôre excited to see you apply this knowledge on the job. *Welcome aboard, and let‚Äôs build the future of adaptive AI systems together!* Test
