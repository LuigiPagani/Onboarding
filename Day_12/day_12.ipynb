{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94ab38f",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3c7aa",
   "metadata": {},
   "source": [
    "Conceptual: Imagine an LLM-based agent that can either answer immediately or invoke a step-by-step reasoning chain (taking more time). Formulate a Meta-RL setup: states might include intermediate reasoning results or uncertainties, actions are “continue reasoning” or “stop and output answer”. What would a reasonable reward be? Perhaps +1 for a correct answer minus a penalty for each step used. Discuss how you would train this (maybe simulate simple math problems of varying difficulty, where continuing reasoning helps for hard ones but is wasteful for easy ones). How is this meta-level decision similar to a bandit? (Hint: each query presents a trade-off between using more resource vs. risk of being wrong, and you have to learn a policy that maps query features to an appropriate compute allocation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafd6cd",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6280573",
   "metadata": {},
   "source": [
    "In a meta‑reinforcement learning formulation, the agent’s state would encompass features of the current query and the partial reasoning state, such as the question’s complexity and any intermediate conclusions or uncertainty estimates generated so far. The action space consists of two discrete choices: either continue the chain-of-thought, incurring additional reasoning steps, or stop and output the current best answer. A sensible reward signal would grant a positive reward for producing a correct answer and subtract a cost proportional to the number of reasoning steps used, reflecting the compute budget. To train such a policy, one could simulate tasks of varying difficulty, where shallow reasoning suffices for easy cases but deeper reasoning is required for hard ones. By sampling many tasks, the agent learns to map problem features to appropriate compute allocation decisions, updating its policy via gradient-based reinforcement learning to maximise expected reward. This meta-level choice parallels a bandit problem: each query offers an unknown payoff distribution over compute and correctness, and the agent must learn a strategy that balances the benefit of exploring further reasoning against the risk and cost of being wrong or wasting steps.\n",
    ", or to stop early— hen a question is too difficult or effectively impossible, so it doesn’t waste tokens, for example  trying to solve the Navier–Stokes equations ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71a2b4",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Math/Analysis: In a risk-sensitive objective, say we want to maximize the probability of getting at least one correct solution in 3 tries (like pass@3 for coding). If the model’s per-try success probability is $p$ when optimizing for expected success, it might settle at $p=0.5$ each try (so expected 0.5, and pass@3 ~ 1 - 0.5^3 = 0.875). But a risk-seeking policy might prefer a strategy that yields $p=0.2$ on each try but occasionally a near-perfect attempt, resulting in one attempt being correct with higher probability (this is hypothetical). How would you formalize the reward for pass@3? (One way: reward = 1 if any of the 3 attempts is correct, 0 otherwise.) Why is this a non-linear, non-additive reward that standard RL would struggle with? What algorithms or approaches can handle this (hint: transform it into an auxiliary MDP or use CVaR-based optimization)?\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652b94e",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Coding (advanced, optional): Given two objectives, e.g., Quality and Latency, implement a simple multi-objective bandit simulation. Arm A gives high quality (reward1 ~ 0.9) at high latency (reward2 ~ -1.0 second), Arm B gives moderate quality (0.7) at low latency (-0.2). Run a bandit algorithm that tries to maximize a weighted sum of these rewards, varying the weight between quality vs. latency. Show how the chosen arm shifts as you change the weight. Then compute the Pareto-optimal set of the two arms (in this case, both might be Pareto-optimal if one is better in quality and the other in latency). Discuss how in a real system we might maintain a set of configurations (prompts/models) that lie on the Pareto frontier of (quality, cost, latency), and possibly choose among them based on real-time user preferences or contexts (for instance, a user on a slow device might implicitly prefer faster responses over slightly higher quality).\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
