{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba9c19a",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8bb81",
   "metadata": {},
   "source": [
    "Use DSPy (or a simplified version if DSPy isn’t accessible) to optimize a multi-step QA pipeline. For example, pipeline: (1) retrieve relevant text from a small corpus, (2) ask LLM to answer question given retrieved text. Define the metric as accuracy of answer. Let the system tune the retrieval prompt and answer prompt. Observe what changes it makes (e.g. does it add “Let’s think step by step” automatically?). Report the before vs after performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47be01",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5dabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import dspy\n",
    "from openai import OpenAI\n",
    "\n",
    "# Cheaper embeddings are fine for synthetic corpora; switch to -large if you want max recall.\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "# DSPy v3.x uses LiteLLM-style names: \"provider/model\"\n",
    "LM_MODEL = \"openai/gpt-4.1-nano\"\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing OPENAI_API_KEY. In a notebook, set it with `%env OPENAI_API_KEY=...` \"\n",
    "        \"or export it in your shell before starting Jupyter.\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# DSPy v3.x\n",
    "lm = dspy.LM(model=LM_MODEL, max_tokens=128)\n",
    "\n",
    "# Configure DSPy to use the LLM\n",
    "if hasattr(dspy, \"settings\"):\n",
    "    dspy.settings.configure(lm=lm)\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5fcd214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs=60 qa_pairs=120 train=96 dev=24 TOP_K=3\n"
     ]
    }
   ],
   "source": [
    "TOP_K = 3\n",
    "NUM_DOCS = 60\n",
    "QA_PER_DOC = 2\n",
    "TRAIN_FRAC = 0.8\n",
    "\n",
    "_rng = random.Random(42)\n",
    "\n",
    "_adjs = [\"Aurora\",\"Nimbus\",\"Orion\",\"Kestrel\",\"Zephyr\",\"Raven\",\"Maple\",\"Helix\",\"Osprey\",\"Forge\",\"Slate\",\"Cedar\",\"Lumen\",\"Delta\",\"Vesta\",\"Atlas\",\"Nova\",\"Redstone\",\"Aster\",\"Northbridge\"]\n",
    "_nouns = [\"Project\",\"Protocol\",\"Battery\",\"Clinic\",\"Drone\",\"Study\",\"Sensor\",\"API\",\"Route\",\"Compiler\",\"Festival\",\"Satellite\",\"Plant\",\"Library\"]\n",
    "_people = [\"Mara Ortiz\",\"Jun Park\",\"Amina Khan\",\"Elena Petrov\",\"Sam Rivera\",\"Noah Chen\",\"Ivy Patel\",\"Luca Rossi\",\"Fatima Ali\",\"Owen Brooks\",\"Hana Suzuki\",\"Diego Silva\"]\n",
    "_cities = [\"Portland\",\"Austin\",\"Berlin\",\"Toronto\",\"Lisbon\",\"Oslo\",\"Seoul\",\"Kyoto\",\"Nairobi\",\"Lima\",\"Dublin\",\"Prague\"]\n",
    "\n",
    "\n",
    "def make_name(kind):\n",
    "    a=_rng.choice(_adjs)\n",
    "    b=_rng.choice(_adjs)\n",
    "    # encourage shared tokens to make lexical matching harder\n",
    "    if _rng.random() < 0.35:\n",
    "        b = a\n",
    "    return f\"{a} {kind} {b}\" if kind in {\"Project\",\"Study\",\"Route\"} else f\"{a} {b} {kind}\"\n",
    "\n",
    "\n",
    "def generate_doc(doc_id):\n",
    "    kind = _rng.choice(_nouns)\n",
    "\n",
    "    if kind == \"Project\":\n",
    "        name = make_name(\"Project\")\n",
    "        lead = _rng.choice(_people)\n",
    "        budget = round(_rng.uniform(1.5, 9.5), 1)\n",
    "        hq = _rng.choice(_cities)\n",
    "        year = _rng.randint(2017, 2024)\n",
    "        text = (\n",
    "            f\"{name}'s lead engineer is {lead}. \"\n",
    "            f\"The project budget was {budget} million dollars. \"\n",
    "            f\"The project started in {year} and is headquartered in {hq}.\"\n",
    "        )\n",
    "        qas = [\n",
    "            (f\"Who is the lead engineer for {name}?\", lead),\n",
    "            (f\"What was the budget for {name}?\", f\"{budget} million dollars\"),\n",
    "            (f\"Where is {name} headquartered?\", hq),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Battery\":\n",
    "        name = make_name(\"Battery\")\n",
    "        cap = round(_rng.uniform(2.0, 9.0), 1)\n",
    "        mins = _rng.randint(12, 38)\n",
    "        pct = _rng.choice([70, 75, 80, 85])\n",
    "        text = (\n",
    "            f\"The {name} has a capacity of {cap} kWh and charges to {pct} percent in {mins} minutes.\"\n",
    "        )\n",
    "        qas = [\n",
    "            (f\"What is the capacity of the {name}?\", f\"{cap} kWh\"),\n",
    "            (f\"How long does the {name} take to reach {pct} percent?\", f\"{mins} minutes\"),\n",
    "            (f\"To what percent does the {name} charge in {mins} minutes?\", f\"{pct} percent\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Clinic\":\n",
    "        name = make_name(\"Clinic\")\n",
    "        system = _rng.choice([\"Atlas\",\"Aster\",\"Nova\",\"Vesta\",\"Redstone\"])\n",
    "        city = _rng.choice(_cities)\n",
    "        year = _rng.randint(2016, 2024)\n",
    "        text = f\"The {name} runs on the {system} scheduling system. The clinic opened in {city} in {year}.\"\n",
    "        qas = [\n",
    "            (f\"Which scheduling system does the {name} use?\", system),\n",
    "            (f\"In which city did the {name} open?\", city),\n",
    "            (f\"What year did the {name} open?\", str(year)),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Drone\":\n",
    "        name = make_name(\"Drone\")\n",
    "        speed = _rng.choice([120, 130, 140, 150, 160])\n",
    "        end = _rng.choice([42, 50, 55, 60, 68])\n",
    "        text = f\"The {name}'s top speed is {speed} kilometers per hour. Its endurance is {end} minutes.\"\n",
    "        qas = [\n",
    "            (f\"What is the top speed of the {name}?\", f\"{speed} kilometers per hour\"),\n",
    "            (f\"What is the endurance of the {name}?\", f\"{end} minutes\"),\n",
    "            (f\"How long is the endurance of the {name}?\", f\"{end} minutes\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Protocol\":\n",
    "        name = make_name(\"Protocol\")\n",
    "        key = _rng.choice([128, 192, 256, 384])\n",
    "        year = _rng.randint(2015, 2023)\n",
    "        text = f\"The {name} encrypts data using a {key}-bit key. It was ratified in {year}.\"\n",
    "        qas = [\n",
    "            (f\"What key size does the {name} use for encryption?\", f\"{key}-bit\"),\n",
    "            (f\"In what year was the {name} ratified?\", str(year)),\n",
    "            (f\"Which year was the {name} ratified?\", str(year)),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Study\":\n",
    "        name = make_name(\"Study\")\n",
    "        sessions = _rng.choice([8, 10, 12, 14, 16])\n",
    "        improve = _rng.choice([12, 15, 18, 21, 24])\n",
    "        text = (\n",
    "            f\"In the {name}, participants completed {sessions} sessions. \"\n",
    "            f\"The primary outcome improved by {improve} percent.\"\n",
    "        )\n",
    "        qas = [\n",
    "            (f\"How many sessions were completed in the {name}?\", str(sessions)),\n",
    "            (f\"By what percent did the primary outcome improve in the {name}?\", f\"{improve} percent\"),\n",
    "            (f\"What was the percent improvement in the {name}?\", f\"{improve} percent\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Sensor\":\n",
    "        name = make_name(\"Sensor\")\n",
    "        material = _rng.choice([\"sapphire\",\"quartz\",\"ceramic\",\"glass\"])\n",
    "        diam = _rng.choice([7, 8, 9, 10, 11])\n",
    "        text = f\"The {name} uses a {material} lens. The lens diameter is {diam} millimeters.\"\n",
    "        qas = [\n",
    "            (f\"What material is the {name}'s lens made of?\", material),\n",
    "            (f\"What is the diameter of the {name}'s lens?\", f\"{diam} millimeters\"),\n",
    "            (f\"How wide is the {name}'s lens diameter?\", f\"{diam} millimeters\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"API\":\n",
    "        name = make_name(\"API\")\n",
    "        limit = _rng.choice([60, 90, 120, 150, 200])\n",
    "        outs = _rng.choice([\n",
    "            \"JSON and CSV\",\n",
    "            \"JSON and XML\",\n",
    "            \"CSV and Parquet\",\n",
    "            \"JSON and YAML\",\n",
    "        ])\n",
    "        text = f\"The {name} has a default rate limit of {limit} requests per minute. It supports {outs} outputs.\"\n",
    "        qas = [\n",
    "            (f\"What is the default rate limit of the {name}?\", f\"{limit} requests per minute\"),\n",
    "            (f\"Which outputs does the {name} support?\", outs),\n",
    "            (f\"What outputs does the {name} support?\", outs),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Route\":\n",
    "        name = make_name(\"Route\")\n",
    "        km = _rng.choice([420, 480, 540, 610, 690])\n",
    "        day = _rng.choice([\"Mondays\",\"Tuesdays\",\"Wednesdays\",\"Thursdays\",\"Fridays\"])\n",
    "        text = f\"Cargo {name} covers {km} kilometers and departs on {day}.\"\n",
    "        qas = [\n",
    "            (f\"How many kilometers does {name} cover?\", f\"{km} kilometers\"),\n",
    "            (f\"On what day does {name} depart?\", day),\n",
    "            (f\"Which day does {name} depart?\", day),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Compiler\":\n",
    "        name = make_name(\"Compiler\")\n",
    "        vm = _rng.choice([\"Vesta VM\",\"Nova VM\",\"Atlas VM\",\"Redstone VM\"])\n",
    "        ver = f\"{_rng.randint(1,4)}.{_rng.randint(0,9)}\"\n",
    "        text = f\"The {name} targets the {vm}. The latest release is version {ver}.\"\n",
    "        qas = [\n",
    "            (f\"Which VM does the {name} target?\", vm),\n",
    "            (f\"What is the latest release version of the {name}?\", ver),\n",
    "            (f\"What version is the latest release of the {name}?\", ver),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Festival\":\n",
    "        name = make_name(\"Festival\")\n",
    "        days = _rng.choice([2, 3, 4, 5])\n",
    "        month = _rng.choice([\"June\",\"July\",\"August\",\"September\"])\n",
    "        date = _rng.randint(10, 24)\n",
    "        text = f\"The {name} lasts {days} days and begins on {month} {date}.\"\n",
    "        qas = [\n",
    "            (f\"How long does the {name} last?\", f\"{days} days\"),\n",
    "            (f\"On what date does the {name} begin?\", f\"{month} {date}\"),\n",
    "            (f\"When does the {name} begin?\", f\"{month} {date}\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Satellite\":\n",
    "        name = make_name(\"Satellite\")\n",
    "        alt = _rng.choice([520, 620, 710, 840])\n",
    "        freq = _rng.choice([\"7.6 GHz\",\"8.2 GHz\",\"9.1 GHz\",\"10.4 GHz\"])\n",
    "        text = f\"The {name} orbits at {alt} kilometers. Its downlink frequency is {freq}.\"\n",
    "        qas = [\n",
    "            (f\"At what altitude does the {name} orbit?\", f\"{alt} kilometers\"),\n",
    "            (f\"What is the downlink frequency of the {name}?\", freq),\n",
    "            (f\"Which frequency is the downlink of the {name}?\", freq),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Plant\":\n",
    "        name = make_name(\"Plant\")\n",
    "        rate = _rng.choice([45, 55, 65, 75, 85])\n",
    "        mold = _rng.choice([\"cobalt alloy\",\"titanium alloy\",\"steel\",\"ceramic\"])\n",
    "        text = f\"The {name} produces {rate} units per hour. It uses {mold} molds.\"\n",
    "        qas = [\n",
    "            (f\"How many units per hour does the {name} produce?\", f\"{rate} units per hour\"),\n",
    "            (f\"What type of molds does the {name} use?\", mold),\n",
    "            (f\"Which molds does the {name} use?\", mold),\n",
    "        ]\n",
    "\n",
    "    else:  # Library\n",
    "        name = make_name(\"Library\")\n",
    "        py = _rng.choice([\"Python 3.9\",\"Python 3.10\",\"Python 3.11\"])\n",
    "        ser = _rng.choice([\"Nova\",\"Atlas\",\"Aster\",\"Redstone\"]) + \" serializer\"\n",
    "        text = f\"The {name} requires {py}. It introduces the {ser}.\"\n",
    "        qas = [\n",
    "            (f\"Which Python version does the {name} require?\", py),\n",
    "            (f\"What serializer does the {name} introduce?\", ser.split(' ',1)[0] if ' ' in ser else ser),\n",
    "            (f\"What does the {name} introduce?\", ser),\n",
    "        ]\n",
    "\n",
    "    # pick QA_PER_DOC questions from this doc\n",
    "    _rng.shuffle(qas)\n",
    "    qas = qas[:QA_PER_DOC]\n",
    "\n",
    "    return {\"id\": doc_id, \"text\": text}, [\n",
    "        {\"question\": q, \"answer\": a, \"doc_id\": doc_id}\n",
    "        for (q,a) in qas\n",
    "    ]\n",
    "\n",
    "\n",
    "# Build corpus + QAs\n",
    "_docs = []\n",
    "_qa = []\n",
    "for i in range(NUM_DOCS):\n",
    "    d, qas = generate_doc(i)\n",
    "    _docs.append(d)\n",
    "    _qa.extend(qas)\n",
    "\n",
    "docs = _docs\n",
    "qa_pairs = _qa\n",
    "\n",
    "_rng.shuffle(qa_pairs)\n",
    "cut = int(len(qa_pairs) * TRAIN_FRAC)\n",
    "train_pairs = qa_pairs[:cut]\n",
    "dev_pairs = qa_pairs[cut:]\n",
    "\n",
    "trainset = [\n",
    "    dspy.Example(question=p[\"question\"], answer=p[\"answer\"], doc_id=p[\"doc_id\"]).with_inputs(\"question\")\n",
    "    for p in train_pairs\n",
    "]\n",
    "\n",
    "devset = [\n",
    "    dspy.Example(question=p[\"question\"], answer=p[\"answer\"], doc_id=p[\"doc_id\"]).with_inputs(\"question\")\n",
    "    for p in dev_pairs\n",
    "]\n",
    "\n",
    "print(f\"docs={len(docs)} qa_pairs={len(qa_pairs)} train={len(trainset)} dev={len(devset)} TOP_K={TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b452a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings-based retrieval (top-k)\n",
    "\n",
    "def embed_texts(texts, model=EMBED_MODEL, batch_size=64):\n",
    "    # Batch to avoid provider limits when corpus grows.\n",
    "    embs = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        data = sorted(resp.data, key=lambda x: x.index)\n",
    "        embs.extend([d.embedding for d in data])\n",
    "    return np.array(embs)\n",
    "\n",
    "_doc_texts = [d[\"text\"] for d in docs]\n",
    "_doc_embeddings = embed_texts(_doc_texts)\n",
    "_doc_norms = np.linalg.norm(_doc_embeddings, axis=1)\n",
    "\n",
    "_query_cache = {}\n",
    "\n",
    "def embed_query(text):\n",
    "    if text not in _query_cache:\n",
    "        _query_cache[text] = embed_texts([text])[0]\n",
    "    return _query_cache[text]\n",
    "\n",
    "def retrieve(query, k=TOP_K):\n",
    "    q_emb = embed_query(query)\n",
    "    denom = _doc_norms * (np.linalg.norm(q_emb) + 1e-9)\n",
    "    sims = (_doc_embeddings @ q_emb) / denom\n",
    "    topk = np.argsort(sims)[-k:][::-1]\n",
    "    context = \"\\n\\n\".join([f\"[{i}] {docs[i]['text']}\" for i in topk])\n",
    "    return context, topk.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78cf7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy module: query rewrite -> retrieve -> answer\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9 ]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return normalize_text(pred) == normalize_text(gold)\n",
    "\n",
    "class QueryRewrite(dspy.Signature):\n",
    "    # Rewrite a question into a search-friendly query.\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField(desc=\"concise search query with key entities\")\n",
    "\n",
    "class AnswerQuestion(dspy.Signature):\n",
    "    # Answer using the provided context only.\n",
    "    context = dspy.InputField(desc=\"retrieved passages\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"short exact answer copied from context\")\n",
    "\n",
    "class QAWithRewrite(dspy.Module):\n",
    "    def __init__(self, k=TOP_K):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.rewrite = dspy.Predict(QueryRewrite)\n",
    "        self.answer = dspy.Predict(AnswerQuestion)\n",
    "\n",
    "    def forward(self, question):\n",
    "        rewritten = self.rewrite(question=question).query\n",
    "        context, ids = retrieve(rewritten, k=self.k)\n",
    "        pred = self.answer(context=context, question=question)\n",
    "        return dspy.Prediction(\n",
    "            answer=pred.answer,\n",
    "            rewritten_query=rewritten,\n",
    "            context_ids=ids,\n",
    "        )\n",
    "\n",
    "\n",
    "def evaluate(module, dataset, desc=\"Baseline eval\"):\n",
    "    \"\"\"Evaluate with a progress bar (uses tqdm if installed).\"\"\"\n",
    "    try:\n",
    "        from tqdm.auto import tqdm  # type: ignore\n",
    "        iterator = tqdm(dataset, total=len(dataset), desc=desc)\n",
    "    except Exception:\n",
    "        iterator = dataset\n",
    "\n",
    "    correct = 0\n",
    "    retrieval_hits = 0\n",
    "    for ex in iterator:\n",
    "        pred = module(question=ex.question)\n",
    "        if exact_match(pred.answer, ex.answer):\n",
    "            correct += 1\n",
    "        if ex.doc_id in getattr(pred, \"context_ids\", []):\n",
    "            retrieval_hits += 1\n",
    "    total = len(dataset)\n",
    "    return {\n",
    "        \"accuracy\": correct / total,\n",
    "        \"retrieval_hit_rate\": retrieval_hits / total,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "082bc8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luigi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Baseline eval:   0%|          | 0/24 [00:00<?, ?it/s]/home/luigi/.local/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='[[ ## qu...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/luigi/.local/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Baseline eval: 100%|██████████| 24/24 [00:43<00:00,  1.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666, 'retrieval_hit_rate': 1.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline evaluation\n",
    "baseline = QAWithRewrite(k=TOP_K)\n",
    "baseline_metrics = evaluate(baseline, devset)\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cee7deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to bootstrap 3 candidate sets.\n",
      "Average Metric: 72.00 / 96 (75.0%): 100%|██████████| 96/96 [00:19<00:00,  4.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:44:50 INFO dspy.evaluate.evaluate: Average Metric: 72 / 96 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 75.0 for seed -3\n",
      "Scores so far: [75.0]\n",
      "Best score so far: 75.0\n",
      "Average Metric: 76.00 / 96 (79.2%): 100%|██████████| 96/96 [00:11<00:00,  8.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:45:01 INFO dspy.evaluate.evaluate: Average Metric: 76 / 96 (79.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 79.17 for seed -2\n",
      "Scores so far: [75.0, 79.17]\n",
      "Best score so far: 79.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/96 [00:00<00:13,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Average Metric: 76.00 / 96 (79.2%): 100%|██████████| 96/96 [00:21<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:45:24 INFO dspy.evaluate.evaluate: Average Metric: 76 / 96 (79.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [75.0, 79.17, 79.17]\n",
      "Best score so far: 79.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/96 [00:02<01:37,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 72.00 / 96 (75.0%): 100%|██████████| 96/96 [00:19<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:45:45 INFO dspy.evaluate.evaluate: Average Metric: 72 / 96 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [75.0, 79.17, 79.17, 75.0]\n",
      "Best score so far: 79.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [00:00<01:17,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 77.00 / 96 (80.2%): 100%|██████████| 96/96 [00:17<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:46:04 INFO dspy.evaluate.evaluate: Average Metric: 77 / 96 (80.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 80.21 for seed 1\n",
      "Scores so far: [75.0, 79.17, 79.17, 75.0, 80.21]\n",
      "Best score so far: 80.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [00:00<01:17,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 76.00 / 96 (79.2%): 100%|██████████| 96/96 [00:19<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:46:25 INFO dspy.evaluate.evaluate: Average Metric: 76 / 96 (79.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [75.0, 79.17, 79.17, 75.0, 80.21, 79.17]\n",
      "Best score so far: 80.21\n",
      "6 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval: 100%|██████████| 24/24 [00:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.875, 'retrieval_hit_rate': 1.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DSPy optimization (query-rewrite + answer prompt)\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# DSPy passes (example, prediction, trace) to metrics.\n",
    "def combined_metric(example, pred, trace=None):\n",
    "    if not pred or not hasattr(pred, \"answer\"):\n",
    "        return 0\n",
    "    answer_ok = exact_match(pred.answer, example.answer)\n",
    "    context_ok = example.doc_id in getattr(pred, \"context_ids\", [])\n",
    "    return 1 if (answer_ok and context_ok) else 0\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=combined_metric,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=3,\n",
    "    num_candidate_programs=3,\n",
    ")\n",
    "\n",
    "optimized = teleprompter.compile(baseline, trainset=trainset)\n",
    "optimized_metrics = evaluate(optimized, devset)\n",
    "optimized_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "321e33b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics: {'accuracy': 0.6666666666666666, 'retrieval_hit_rate': 1.0}\n",
      "Optimized metrics: {'accuracy': 0.875, 'retrieval_hit_rate': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved cases (wrong→right): 5\n",
      "Regressions (right→wrong): 0\n",
      "\n",
      "================================================================================\n",
      "[dev #1] Q: What is the latest release version of the Northbridge Nimbus Compiler?\n",
      "Gold: 3.4 | gold doc: 35\n",
      "- Baseline\n",
      "  rewritten: latest release version Northbridge Nimbus Compiler\n",
      "  context_ids: [35, 46, 33]\n",
      "  answer: version 3.4\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: latest release version Northbridge Nimbus Compiler\n",
      "  context_ids: [35, 46, 33]\n",
      "  answer: 3.4\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #2] Q: What version is the latest release of the Osprey Aurora Compiler?\n",
      "Gold: 4.5 | gold doc: 3\n",
      "- Baseline\n",
      "  rewritten: latest release version Osprey Aurora Compiler\n",
      "  context_ids: [3, 39, 20]\n",
      "  answer: version 4.5\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: latest version Osprey Aurora Compiler\n",
      "  context_ids: [3, 39, 20]\n",
      "  answer: 4.5\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #3] Q: What outputs does the Redstone Redstone API support?\n",
      "Gold: JSON and CSV | gold doc: 29\n",
      "- Baseline\n",
      "  rewritten: Redstone API supported output fields\n",
      "  context_ids: [29, 40, 9]\n",
      "  answer: JSON and CSV outputs\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: Redstone Redstone API supported outputs\n",
      "  context_ids: [29, 40, 35]\n",
      "  answer: JSON and CSV\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #9] Q: Which outputs does the Nimbus Nimbus API support?\n",
      "Gold: JSON and CSV | gold doc: 27\n",
      "- Baseline\n",
      "  rewritten: Nimbus Nimbus API supported output formats\n",
      "  context_ids: [27, 35, 45]\n",
      "  answer: JSON and CSV outputs\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: Nimbus Nimbus API supported outputs\n",
      "  context_ids: [27, 35, 45]\n",
      "  answer: JSON and CSV\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #17] Q: What version is the latest release of the Raven Redstone Compiler?\n",
      "Gold: 4.6 | gold doc: 9\n",
      "- Baseline\n",
      "  rewritten: latest version Raven Redstone Compiler release\n",
      "  context_ids: [9, 35, 39]\n",
      "  answer: version 4.6\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: latest version Raven Redstone Compiler\n",
      "  context_ids: [9, 35, 39]\n",
      "  answer: 4.6\n",
      "  answer_ok: True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Baseline demos\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  (no demos)\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  (no demos)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Optimized demos\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  demos: 3\n",
      "   - Example({'augmented': True, 'question': 'Which year was the Northbridge Orion Protocol ratified?', 'query': 'Northbridge Orion Protocol ratification year'}) (input_keys=None)\n",
      "   - Example({'question': 'What version is the latest release of the Northbridge Nimbus Compiler?', 'answer': '3.4', 'doc_id': 35}) (input_keys={'question'})\n",
      "   - Example({'question': 'How many kilometers does Aurora Route Kestrel cover?', 'answer': '690 kilometers', 'doc_id': 15}) (input_keys={'question'})\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  demos: 3\n",
      "   - Example({'augmented': True, 'context': \"[34] The Northbridge Orion Protocol encrypts data using a 192-bit key. It was ratified in 2021.\\n\\n[24] Orion Project Orion's lead engineer is Mara Ortiz. The project budget was 8.4 million dollars. The project started in 2020 and is headquartered in Austin.\\n\\n[16] The Forge Forge Protocol encrypts data using a 128-bit key. It was ratified in 2019.\", 'question': 'Which year was the Northbridge Orion Protocol ratified?', 'answer': '2021'}) (input_keys=None)\n",
      "   - Example({'question': 'What version is the latest release of the Northbridge Nimbus Compiler?', 'answer': '3.4', 'doc_id': 35}) (input_keys={'question'})\n",
      "   - Example({'question': 'How many kilometers does Aurora Route Kestrel cover?', 'answer': '690 kilometers', 'doc_id': 15}) (input_keys={'question'})\n"
     ]
    }
   ],
   "source": [
    "# What improved? (baseline vs optimized on dev)\n",
    "\n",
    "print(\"Baseline metrics:\", baseline_metrics)\n",
    "print(\"Optimized metrics:\", optimized_metrics)\n",
    "\n",
    "def show_case(i, ex, base_pred, opt_pred):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[dev #{i}] Q:\", ex.question)\n",
    "    print(\"Gold:\", ex.answer, \"| gold doc:\", ex.doc_id)\n",
    "    print(\"- Baseline\")\n",
    "    print(\"  rewritten:\", getattr(base_pred, \"rewritten_query\", None))\n",
    "    print(\"  context_ids:\", getattr(base_pred, \"context_ids\", None))\n",
    "    print(\"  answer:\", getattr(base_pred, \"answer\", None))\n",
    "    print(\"  answer_ok:\", exact_match(base_pred.answer, ex.answer))\n",
    "    print(\"- Optimized\")\n",
    "    print(\"  rewritten:\", getattr(opt_pred, \"rewritten_query\", None))\n",
    "    print(\"  context_ids:\", getattr(opt_pred, \"context_ids\", None))\n",
    "    print(\"  answer:\", getattr(opt_pred, \"answer\", None))\n",
    "    print(\"  answer_ok:\", exact_match(opt_pred.answer, ex.answer))\n",
    "\n",
    "improved = []\n",
    "regressed = []\n",
    "\n",
    "for i, ex in enumerate(devset):\n",
    "    base_pred = baseline(question=ex.question)\n",
    "    opt_pred  = optimized(question=ex.question)\n",
    "\n",
    "    base_ok = exact_match(base_pred.answer, ex.answer)\n",
    "    opt_ok  = exact_match(opt_pred.answer, ex.answer)\n",
    "\n",
    "    if (not base_ok) and opt_ok:\n",
    "        improved.append((i, ex, base_pred, opt_pred))\n",
    "    elif base_ok and (not opt_ok):\n",
    "        regressed.append((i, ex, base_pred, opt_pred))\n",
    "\n",
    "print(f\"\\nImproved cases (wrong→right): {len(improved)}\")\n",
    "print(f\"Regressions (right→wrong): {len(regressed)}\")\n",
    "\n",
    "# Show a few examples\n",
    "for tup in improved[:5]:\n",
    "    show_case(*tup)\n",
    "\n",
    "if regressed:\n",
    "    print(\"\\nShowing regressions:\")\n",
    "    for tup in regressed[:3]:\n",
    "        show_case(*tup)\n",
    "\n",
    "# Optional: inspect learned few-shot demos (“winning prompts”)\n",
    "def show_demos(label, prog):\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(label)\n",
    "    try:\n",
    "        preds = getattr(prog, \"predictors\", lambda: [])()\n",
    "        for p in preds:\n",
    "            print(\"\\nPREDICTOR:\", type(p).__name__)\n",
    "            demos = getattr(p, \"demos\", None)\n",
    "            if not demos:\n",
    "                print(\"  (no demos)\")\n",
    "                continue\n",
    "            print(f\"  demos: {len(demos)}\")\n",
    "            for d in demos[:3]:\n",
    "                # demos are dspy.Example-like objects\n",
    "                print(\"   -\", d)\n",
    "    except Exception as e:\n",
    "        print(\"Could not inspect demos:\", e)\n",
    "\n",
    "show_demos(\"Baseline demos\", baseline)\n",
    "show_demos(\"Optimized demos\", optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea830c69",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "- **Baseline vs optimized**:\n",
    "  - Baseline dev metrics were accuracy = 0.6667 and retrieval hit rate = 1.0.\n",
    "  - Optimized dev metrics were accuracy = 0.875 and retrieval hit rate = 1.0.\n",
    "\n",
    "- **Which metric improved**:\n",
    "   - Accuracy improved (0.6667 → 0.875).\n",
    "   - Retrieval hit rate did not change (stayed 1.0), the correct doc was already being retrieved.\n",
    "\n",
    "- **Why accuracy improved**:\n",
    "  - Most failures were answer formatting, not missing knowledge.\n",
    "  - Baseline often added extra words like \"version 3.4\" or \"JSON and CSV outputs\".\n",
    "  - Our metric is strict exact match, so those extra words count as wrong.\n",
    "  - The optimized program learned (via DSPy’s compiled demos) to output the short exact span (e.g., \"3.4\", \"JSON and CSV\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247d49",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274dcd0",
   "metadata": {},
   "source": [
    "Coding: Implement a simple version of EvoPrompt. Represent a prompt as a list of tokens or words. Define two evolutionary operators: mutate (randomly replace or insert a word) and crossover (swap a segment between two prompts). Use an LLM (or a heuristic function) to evaluate fitness (e.g. BLEU score or any task-specific score) of prompts. Start with a few initial prompts and run a few generations of evolution. Did the prompts improve? This could be done on a trivial task (like prompt an LLM to output a specific keyword - evolve prompts to maximize the occurrence of that keyword in the response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42147c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07d3c3",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeb964",
   "metadata": {},
   "source": [
    "Compare reinforcement learning vs. evolutionary search for prompt optimization. If our “policy” is the prompt text and the “environment” gives a reward (quality score), RL would tweak the prompt based on gradient of reward (if possible) or black-box optimization. Evolutionary methods like GEPA/EvoPrompt treat it like a search over strings. List pros and cons of each: e.g., RL (with methods like RLPrompt or policy gradients) can directly optimize an objective but may get stuck in local optima or require many samples; evolutionary approaches are more global and can incorporate heuristic knowledge (via LLM reflections in GEPA) but might be slower if search space is huge. In practice, why might GEPA’s ability to incorporate natural language reflections be advantageous in prompt tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c42ade",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8475c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fce57b8",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2da32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
