{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba9c19a",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8bb81",
   "metadata": {},
   "source": [
    "Use DSPy (or a simplified version if DSPy isn’t accessible) to optimize a multi-step QA pipeline. For example, pipeline: (1) retrieve relevant text from a small corpus, (2) ask LLM to answer question given retrieved text. Define the metric as accuracy of answer. Let the system tune the retrieval prompt and answer prompt. Observe what changes it makes (e.g. does it add “Let’s think step by step” automatically?). Report the before vs after performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47be01",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b8fb35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from: /home/luigi/Programming/Onboarding/.env\n",
      "OPENAI_API_KEY set: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from a repo-root `.env` (no extra dependencies)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _find_dotenv(start: Path | None = None, max_up: int = 6) -> Path | None:\n",
    "    p = (start or Path.cwd()).resolve()\n",
    "    for _ in range(max_up + 1):\n",
    "        candidate = p / \".env\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_env_file(dotenv_path: Path) -> None:\n",
    "    for raw in dotenv_path.read_text().splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.startswith(\"export \"):\n",
    "            line = line[len(\"export \") :]\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        key, value = line.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        # strip simple quotes\n",
    "        if (value.startswith('\"') and value.endswith('\"')) or (value.startswith(\"'\") and value.endswith(\"'\")):\n",
    "            value = value[1:-1]\n",
    "        os.environ.setdefault(key, value)\n",
    "\n",
    "\n",
    "dotenv_path = _find_dotenv()\n",
    "if dotenv_path:\n",
    "    _load_env_file(dotenv_path)\n",
    "    print(f\"Loaded .env from: {dotenv_path}\")\n",
    "else:\n",
    "    print(\"No .env found while searching upward from the current working directory.\")\n",
    "\n",
    "print(\"OPENAI_API_KEY set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c33aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import dspy\n",
    "from openai import OpenAI\n",
    "\n",
    "# Cheaper embeddings are fine for synthetic corpora; switch to -large if you want max recall.\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "# DSPy v3.x uses LiteLLM-style names: \"provider/model\"\n",
    "LM_MODEL = \"openai/gpt-4.1-mini\"\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"Missing OPENAI_API_KEY. In a notebook, set it with `%env OPENAI_API_KEY=...` \"\n",
    "        \"or export it in your shell before starting Jupyter.\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# DSPy v3.x\n",
    "lm = dspy.LM(model=LM_MODEL, max_tokens=128)\n",
    "\n",
    "# Configure DSPy to use the LLM\n",
    "if hasattr(dspy, \"settings\"):\n",
    "    dspy.settings.configure(lm=lm)\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5fcd214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs=60 qa_pairs=120 train=96 dev=24 TOP_K=3\n"
     ]
    }
   ],
   "source": [
    "TOP_K = 3\n",
    "NUM_DOCS = 60\n",
    "QA_PER_DOC = 2\n",
    "TRAIN_FRAC = 0.8\n",
    "\n",
    "_rng = random.Random(42)\n",
    "\n",
    "_adjs = [\"Aurora\",\"Nimbus\",\"Orion\",\"Kestrel\",\"Zephyr\",\"Raven\",\"Maple\",\"Helix\",\"Osprey\",\"Forge\",\"Slate\",\"Cedar\",\"Lumen\",\"Delta\",\"Vesta\",\"Atlas\",\"Nova\",\"Redstone\",\"Aster\",\"Northbridge\"]\n",
    "_nouns = [\"Project\",\"Protocol\",\"Battery\",\"Clinic\",\"Drone\",\"Study\",\"Sensor\",\"API\",\"Route\",\"Compiler\",\"Festival\",\"Satellite\",\"Plant\",\"Library\"]\n",
    "_people = [\"Mara Ortiz\",\"Jun Park\",\"Amina Khan\",\"Elena Petrov\",\"Sam Rivera\",\"Noah Chen\",\"Ivy Patel\",\"Luca Rossi\",\"Fatima Ali\",\"Owen Brooks\",\"Hana Suzuki\",\"Diego Silva\"]\n",
    "_cities = [\"Portland\",\"Austin\",\"Berlin\",\"Toronto\",\"Lisbon\",\"Oslo\",\"Seoul\",\"Kyoto\",\"Nairobi\",\"Lima\",\"Dublin\",\"Prague\"]\n",
    "\n",
    "\n",
    "def make_name(kind):\n",
    "    a=_rng.choice(_adjs)\n",
    "    b=_rng.choice(_adjs)\n",
    "    # encourage shared tokens to make lexical matching harder\n",
    "    if _rng.random() < 0.35:\n",
    "        b = a\n",
    "    return f\"{a} {kind} {b}\" if kind in {\"Project\",\"Study\",\"Route\"} else f\"{a} {b} {kind}\"\n",
    "\n",
    "\n",
    "def generate_doc(doc_id):\n",
    "    kind = _rng.choice(_nouns)\n",
    "\n",
    "    if kind == \"Project\":\n",
    "        name = make_name(\"Project\")\n",
    "        lead = _rng.choice(_people)\n",
    "        budget = round(_rng.uniform(1.5, 9.5), 1)\n",
    "        hq = _rng.choice(_cities)\n",
    "        year = _rng.randint(2017, 2024)\n",
    "        text = (\n",
    "            f\"{name}'s lead engineer is {lead}. \"\n",
    "            f\"The project budget was {budget} million dollars. \"\n",
    "            f\"The project started in {year} and is headquartered in {hq}.\"\n",
    "        )\n",
    "        qas = [\n",
    "            (f\"Who is the lead engineer for {name}?\", lead),\n",
    "            (f\"What was the budget for {name}?\", f\"{budget} million dollars\"),\n",
    "            (f\"Where is {name} headquartered?\", hq),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Battery\":\n",
    "        name = make_name(\"Battery\")\n",
    "        cap = round(_rng.uniform(2.0, 9.0), 1)\n",
    "        mins = _rng.randint(12, 38)\n",
    "        pct = _rng.choice([70, 75, 80, 85])\n",
    "        text = (\n",
    "            f\"The {name} has a capacity of {cap} kWh and charges to {pct} percent in {mins} minutes.\"\n",
    "        )\n",
    "        qas = [\n",
    "            (f\"What is the capacity of the {name}?\", f\"{cap} kWh\"),\n",
    "            (f\"How long does the {name} take to reach {pct} percent?\", f\"{mins} minutes\"),\n",
    "            (f\"To what percent does the {name} charge in {mins} minutes?\", f\"{pct} percent\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Clinic\":\n",
    "        name = make_name(\"Clinic\")\n",
    "        system = _rng.choice([\"Atlas\",\"Aster\",\"Nova\",\"Vesta\",\"Redstone\"])\n",
    "        city = _rng.choice(_cities)\n",
    "        year = _rng.randint(2016, 2024)\n",
    "        text = f\"The {name} runs on the {system} scheduling system. The clinic opened in {city} in {year}.\"\n",
    "        qas = [\n",
    "            (f\"Which scheduling system does the {name} use?\", system),\n",
    "            (f\"In which city did the {name} open?\", city),\n",
    "            (f\"What year did the {name} open?\", str(year)),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Drone\":\n",
    "        name = make_name(\"Drone\")\n",
    "        speed = _rng.choice([120, 130, 140, 150, 160])\n",
    "        end = _rng.choice([42, 50, 55, 60, 68])\n",
    "        text = f\"The {name}'s top speed is {speed} kilometers per hour. Its endurance is {end} minutes.\"\n",
    "        qas = [\n",
    "            (f\"What is the top speed of the {name}?\", f\"{speed} kilometers per hour\"),\n",
    "            (f\"What is the endurance of the {name}?\", f\"{end} minutes\"),\n",
    "            (f\"How long is the endurance of the {name}?\", f\"{end} minutes\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Protocol\":\n",
    "        name = make_name(\"Protocol\")\n",
    "        key = _rng.choice([128, 192, 256, 384])\n",
    "        year = _rng.randint(2015, 2023)\n",
    "        text = f\"The {name} encrypts data using a {key}-bit key. It was ratified in {year}.\"\n",
    "        qas = [\n",
    "            (f\"What key size does the {name} use for encryption?\", f\"{key}-bit\"),\n",
    "            (f\"In what year was the {name} ratified?\", str(year)),\n",
    "            (f\"Which year was the {name} ratified?\", str(year)),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Study\":\n",
    "        name = make_name(\"Study\")\n",
    "        sessions = _rng.choice([8, 10, 12, 14, 16])\n",
    "        improve = _rng.choice([12, 15, 18, 21, 24])\n",
    "        text = (\n",
    "            f\"In the {name}, participants completed {sessions} sessions. \"\n",
    "            f\"The primary outcome improved by {improve} percent.\"\n",
    "        )\n",
    "        qas = [\n",
    "            (f\"How many sessions were completed in the {name}?\", str(sessions)),\n",
    "            (f\"By what percent did the primary outcome improve in the {name}?\", f\"{improve} percent\"),\n",
    "            (f\"What was the percent improvement in the {name}?\", f\"{improve} percent\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Sensor\":\n",
    "        name = make_name(\"Sensor\")\n",
    "        material = _rng.choice([\"sapphire\",\"quartz\",\"ceramic\",\"glass\"])\n",
    "        diam = _rng.choice([7, 8, 9, 10, 11])\n",
    "        text = f\"The {name} uses a {material} lens. The lens diameter is {diam} millimeters.\"\n",
    "        qas = [\n",
    "            (f\"What material is the {name}'s lens made of?\", material),\n",
    "            (f\"What is the diameter of the {name}'s lens?\", f\"{diam} millimeters\"),\n",
    "            (f\"How wide is the {name}'s lens diameter?\", f\"{diam} millimeters\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"API\":\n",
    "        name = make_name(\"API\")\n",
    "        limit = _rng.choice([60, 90, 120, 150, 200])\n",
    "        outs = _rng.choice([\n",
    "            \"JSON and CSV\",\n",
    "            \"JSON and XML\",\n",
    "            \"CSV and Parquet\",\n",
    "            \"JSON and YAML\",\n",
    "        ])\n",
    "        text = f\"The {name} has a default rate limit of {limit} requests per minute. It supports {outs} outputs.\"\n",
    "        qas = [\n",
    "            (f\"What is the default rate limit of the {name}?\", f\"{limit} requests per minute\"),\n",
    "            (f\"Which outputs does the {name} support?\", outs),\n",
    "            (f\"What outputs does the {name} support?\", outs),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Route\":\n",
    "        name = make_name(\"Route\")\n",
    "        km = _rng.choice([420, 480, 540, 610, 690])\n",
    "        day = _rng.choice([\"Mondays\",\"Tuesdays\",\"Wednesdays\",\"Thursdays\",\"Fridays\"])\n",
    "        text = f\"Cargo {name} covers {km} kilometers and departs on {day}.\"\n",
    "        qas = [\n",
    "            (f\"How many kilometers does {name} cover?\", f\"{km} kilometers\"),\n",
    "            (f\"On what day does {name} depart?\", day),\n",
    "            (f\"Which day does {name} depart?\", day),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Compiler\":\n",
    "        name = make_name(\"Compiler\")\n",
    "        vm = _rng.choice([\"Vesta VM\",\"Nova VM\",\"Atlas VM\",\"Redstone VM\"])\n",
    "        ver = f\"{_rng.randint(1,4)}.{_rng.randint(0,9)}\"\n",
    "        text = f\"The {name} targets the {vm}. The latest release is version {ver}.\"\n",
    "        qas = [\n",
    "            (f\"Which VM does the {name} target?\", vm),\n",
    "            (f\"What is the latest release version of the {name}?\", ver),\n",
    "            (f\"What version is the latest release of the {name}?\", ver),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Festival\":\n",
    "        name = make_name(\"Festival\")\n",
    "        days = _rng.choice([2, 3, 4, 5])\n",
    "        month = _rng.choice([\"June\",\"July\",\"August\",\"September\"])\n",
    "        date = _rng.randint(10, 24)\n",
    "        text = f\"The {name} lasts {days} days and begins on {month} {date}.\"\n",
    "        qas = [\n",
    "            (f\"How long does the {name} last?\", f\"{days} days\"),\n",
    "            (f\"On what date does the {name} begin?\", f\"{month} {date}\"),\n",
    "            (f\"When does the {name} begin?\", f\"{month} {date}\"),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Satellite\":\n",
    "        name = make_name(\"Satellite\")\n",
    "        alt = _rng.choice([520, 620, 710, 840])\n",
    "        freq = _rng.choice([\"7.6 GHz\",\"8.2 GHz\",\"9.1 GHz\",\"10.4 GHz\"])\n",
    "        text = f\"The {name} orbits at {alt} kilometers. Its downlink frequency is {freq}.\"\n",
    "        qas = [\n",
    "            (f\"At what altitude does the {name} orbit?\", f\"{alt} kilometers\"),\n",
    "            (f\"What is the downlink frequency of the {name}?\", freq),\n",
    "            (f\"Which frequency is the downlink of the {name}?\", freq),\n",
    "        ]\n",
    "\n",
    "    elif kind == \"Plant\":\n",
    "        name = make_name(\"Plant\")\n",
    "        rate = _rng.choice([45, 55, 65, 75, 85])\n",
    "        mold = _rng.choice([\"cobalt alloy\",\"titanium alloy\",\"steel\",\"ceramic\"])\n",
    "        text = f\"The {name} produces {rate} units per hour. It uses {mold} molds.\"\n",
    "        qas = [\n",
    "            (f\"How many units per hour does the {name} produce?\", f\"{rate} units per hour\"),\n",
    "            (f\"What type of molds does the {name} use?\", mold),\n",
    "            (f\"Which molds does the {name} use?\", mold),\n",
    "        ]\n",
    "\n",
    "    else:  # Library\n",
    "        name = make_name(\"Library\")\n",
    "        py = _rng.choice([\"Python 3.9\",\"Python 3.10\",\"Python 3.11\"])\n",
    "        ser = _rng.choice([\"Nova\",\"Atlas\",\"Aster\",\"Redstone\"]) + \" serializer\"\n",
    "        text = f\"The {name} requires {py}. It introduces the {ser}.\"\n",
    "        qas = [\n",
    "            (f\"Which Python version does the {name} require?\", py),\n",
    "            (f\"What serializer does the {name} introduce?\", ser.split(' ',1)[0] if ' ' in ser else ser),\n",
    "            (f\"What does the {name} introduce?\", ser),\n",
    "        ]\n",
    "\n",
    "    # pick QA_PER_DOC questions from this doc\n",
    "    _rng.shuffle(qas)\n",
    "    qas = qas[:QA_PER_DOC]\n",
    "\n",
    "    return {\"id\": doc_id, \"text\": text}, [\n",
    "        {\"question\": q, \"answer\": a, \"doc_id\": doc_id}\n",
    "        for (q,a) in qas\n",
    "    ]\n",
    "\n",
    "\n",
    "# Build corpus + QAs\n",
    "_docs = []\n",
    "_qa = []\n",
    "for i in range(NUM_DOCS):\n",
    "    d, qas = generate_doc(i)\n",
    "    _docs.append(d)\n",
    "    _qa.extend(qas)\n",
    "\n",
    "docs = _docs\n",
    "qa_pairs = _qa\n",
    "\n",
    "_rng.shuffle(qa_pairs)\n",
    "cut = int(len(qa_pairs) * TRAIN_FRAC)\n",
    "train_pairs = qa_pairs[:cut]\n",
    "dev_pairs = qa_pairs[cut:]\n",
    "\n",
    "trainset = [\n",
    "    dspy.Example(question=p[\"question\"], answer=p[\"answer\"], doc_id=p[\"doc_id\"]).with_inputs(\"question\")\n",
    "    for p in train_pairs\n",
    "]\n",
    "\n",
    "devset = [\n",
    "    dspy.Example(question=p[\"question\"], answer=p[\"answer\"], doc_id=p[\"doc_id\"]).with_inputs(\"question\")\n",
    "    for p in dev_pairs\n",
    "]\n",
    "\n",
    "print(f\"docs={len(docs)} qa_pairs={len(qa_pairs)} train={len(trainset)} dev={len(devset)} TOP_K={TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b452a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings-based retrieval (top-k)\n",
    "\n",
    "def embed_texts(texts, model=EMBED_MODEL, batch_size=64):\n",
    "    # Batch to avoid provider limits when corpus grows.\n",
    "    embs = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        data = sorted(resp.data, key=lambda x: x.index)\n",
    "        embs.extend([d.embedding for d in data])\n",
    "    return np.array(embs)\n",
    "\n",
    "_doc_texts = [d[\"text\"] for d in docs]\n",
    "_doc_embeddings = embed_texts(_doc_texts)\n",
    "_doc_norms = np.linalg.norm(_doc_embeddings, axis=1)\n",
    "\n",
    "_query_cache = {}\n",
    "\n",
    "def embed_query(text):\n",
    "    if text not in _query_cache:\n",
    "        _query_cache[text] = embed_texts([text])[0]\n",
    "    return _query_cache[text]\n",
    "\n",
    "def retrieve(query, k=TOP_K):\n",
    "    q_emb = embed_query(query)\n",
    "    denom = _doc_norms * (np.linalg.norm(q_emb) + 1e-9)\n",
    "    sims = (_doc_embeddings @ q_emb) / denom\n",
    "    topk = np.argsort(sims)[-k:][::-1]\n",
    "    context = \"\\n\\n\".join([f\"[{i}] {docs[i]['text']}\" for i in topk])\n",
    "    return context, topk.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78cf7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy module: query rewrite -> retrieve -> answer\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9 ]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return normalize_text(pred) == normalize_text(gold)\n",
    "\n",
    "class QueryRewrite(dspy.Signature):\n",
    "    # Rewrite a question into a search-friendly query.\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField(desc=\"concise search query with key entities\")\n",
    "\n",
    "class AnswerQuestion(dspy.Signature):\n",
    "    # Answer using the provided context only.\n",
    "    context = dspy.InputField(desc=\"retrieved passages\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"short exact answer copied from context\")\n",
    "\n",
    "class QAWithRewrite(dspy.Module):\n",
    "    def __init__(self, k=TOP_K):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.rewrite = dspy.Predict(QueryRewrite)\n",
    "        self.answer = dspy.Predict(AnswerQuestion)\n",
    "\n",
    "    def forward(self, question):\n",
    "        rewritten = self.rewrite(question=question).query\n",
    "        context, ids = retrieve(rewritten, k=self.k)\n",
    "        pred = self.answer(context=context, question=question)\n",
    "        return dspy.Prediction(\n",
    "            answer=pred.answer,\n",
    "            rewritten_query=rewritten,\n",
    "            context_ids=ids,\n",
    "        )\n",
    "\n",
    "\n",
    "def evaluate(module, dataset, desc=\"Baseline eval\"):\n",
    "    \"\"\"Evaluate with a progress bar (uses tqdm if installed).\"\"\"\n",
    "    try:\n",
    "        from tqdm.auto import tqdm  # type: ignore\n",
    "        iterator = tqdm(dataset, total=len(dataset), desc=desc)\n",
    "    except Exception:\n",
    "        iterator = dataset\n",
    "\n",
    "    correct = 0\n",
    "    retrieval_hits = 0\n",
    "    for ex in iterator:\n",
    "        pred = module(question=ex.question)\n",
    "        if exact_match(pred.answer, ex.answer):\n",
    "            correct += 1\n",
    "        if ex.doc_id in getattr(pred, \"context_ids\", []):\n",
    "            retrieval_hits += 1\n",
    "    total = len(dataset)\n",
    "    return {\n",
    "        \"accuracy\": correct / total,\n",
    "        \"retrieval_hit_rate\": retrieval_hits / total,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "082bc8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luigi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Baseline eval:   0%|          | 0/24 [00:00<?, ?it/s]/home/luigi/.local/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='[[ ## qu...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/luigi/.local/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Baseline eval: 100%|██████████| 24/24 [00:43<00:00,  1.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666, 'retrieval_hit_rate': 1.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline evaluation\n",
    "baseline = QAWithRewrite(k=TOP_K)\n",
    "baseline_metrics = evaluate(baseline, devset)\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cee7deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to bootstrap 3 candidate sets.\n",
      "Average Metric: 72.00 / 96 (75.0%): 100%|██████████| 96/96 [00:19<00:00,  4.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:44:50 INFO dspy.evaluate.evaluate: Average Metric: 72 / 96 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 75.0 for seed -3\n",
      "Scores so far: [75.0]\n",
      "Best score so far: 75.0\n",
      "Average Metric: 76.00 / 96 (79.2%): 100%|██████████| 96/96 [00:11<00:00,  8.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:45:01 INFO dspy.evaluate.evaluate: Average Metric: 76 / 96 (79.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 79.17 for seed -2\n",
      "Scores so far: [75.0, 79.17]\n",
      "Best score so far: 79.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/96 [00:00<00:13,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Average Metric: 76.00 / 96 (79.2%): 100%|██████████| 96/96 [00:21<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:45:24 INFO dspy.evaluate.evaluate: Average Metric: 76 / 96 (79.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [75.0, 79.17, 79.17]\n",
      "Best score so far: 79.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/96 [00:02<01:37,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 72.00 / 96 (75.0%): 100%|██████████| 96/96 [00:19<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:45:45 INFO dspy.evaluate.evaluate: Average Metric: 72 / 96 (75.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [75.0, 79.17, 79.17, 75.0]\n",
      "Best score so far: 79.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [00:00<01:17,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 77.00 / 96 (80.2%): 100%|██████████| 96/96 [00:17<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:46:04 INFO dspy.evaluate.evaluate: Average Metric: 77 / 96 (80.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 80.21 for seed 1\n",
      "Scores so far: [75.0, 79.17, 79.17, 75.0, 80.21]\n",
      "Best score so far: 80.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [00:00<01:17,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 76.00 / 96 (79.2%): 100%|██████████| 96/96 [00:19<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 11:46:25 INFO dspy.evaluate.evaluate: Average Metric: 76 / 96 (79.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [75.0, 79.17, 79.17, 75.0, 80.21, 79.17]\n",
      "Best score so far: 80.21\n",
      "6 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval: 100%|██████████| 24/24 [00:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.875, 'retrieval_hit_rate': 1.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DSPy optimization (query-rewrite + answer prompt)\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# DSPy passes (example, prediction, trace) to metrics.\n",
    "def combined_metric(example, pred, trace=None):\n",
    "    if not pred or not hasattr(pred, \"answer\"):\n",
    "        return 0\n",
    "    answer_ok = exact_match(pred.answer, example.answer)\n",
    "    context_ok = example.doc_id in getattr(pred, \"context_ids\", [])\n",
    "    return 1 if (answer_ok and context_ok) else 0\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=combined_metric,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=3,\n",
    "    num_candidate_programs=3,\n",
    ")\n",
    "\n",
    "optimized = teleprompter.compile(baseline, trainset=trainset)\n",
    "optimized_metrics = evaluate(optimized, devset)\n",
    "optimized_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "321e33b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metrics: {'accuracy': 0.6666666666666666, 'retrieval_hit_rate': 1.0}\n",
      "Optimized metrics: {'accuracy': 0.875, 'retrieval_hit_rate': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved cases (wrong→right): 5\n",
      "Regressions (right→wrong): 0\n",
      "\n",
      "================================================================================\n",
      "[dev #1] Q: What is the latest release version of the Northbridge Nimbus Compiler?\n",
      "Gold: 3.4 | gold doc: 35\n",
      "- Baseline\n",
      "  rewritten: latest release version Northbridge Nimbus Compiler\n",
      "  context_ids: [35, 46, 33]\n",
      "  answer: version 3.4\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: latest release version Northbridge Nimbus Compiler\n",
      "  context_ids: [35, 46, 33]\n",
      "  answer: 3.4\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #2] Q: What version is the latest release of the Osprey Aurora Compiler?\n",
      "Gold: 4.5 | gold doc: 3\n",
      "- Baseline\n",
      "  rewritten: latest release version Osprey Aurora Compiler\n",
      "  context_ids: [3, 39, 20]\n",
      "  answer: version 4.5\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: latest version Osprey Aurora Compiler\n",
      "  context_ids: [3, 39, 20]\n",
      "  answer: 4.5\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #3] Q: What outputs does the Redstone Redstone API support?\n",
      "Gold: JSON and CSV | gold doc: 29\n",
      "- Baseline\n",
      "  rewritten: Redstone API supported output fields\n",
      "  context_ids: [29, 40, 9]\n",
      "  answer: JSON and CSV outputs\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: Redstone Redstone API supported outputs\n",
      "  context_ids: [29, 40, 35]\n",
      "  answer: JSON and CSV\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #9] Q: Which outputs does the Nimbus Nimbus API support?\n",
      "Gold: JSON and CSV | gold doc: 27\n",
      "- Baseline\n",
      "  rewritten: Nimbus Nimbus API supported output formats\n",
      "  context_ids: [27, 35, 45]\n",
      "  answer: JSON and CSV outputs\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: Nimbus Nimbus API supported outputs\n",
      "  context_ids: [27, 35, 45]\n",
      "  answer: JSON and CSV\n",
      "  answer_ok: True\n",
      "\n",
      "================================================================================\n",
      "[dev #17] Q: What version is the latest release of the Raven Redstone Compiler?\n",
      "Gold: 4.6 | gold doc: 9\n",
      "- Baseline\n",
      "  rewritten: latest version Raven Redstone Compiler release\n",
      "  context_ids: [9, 35, 39]\n",
      "  answer: version 4.6\n",
      "  answer_ok: False\n",
      "- Optimized\n",
      "  rewritten: latest version Raven Redstone Compiler\n",
      "  context_ids: [9, 35, 39]\n",
      "  answer: 4.6\n",
      "  answer_ok: True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Baseline demos\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  (no demos)\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  (no demos)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Optimized demos\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  demos: 3\n",
      "   - Example({'augmented': True, 'question': 'Which year was the Northbridge Orion Protocol ratified?', 'query': 'Northbridge Orion Protocol ratification year'}) (input_keys=None)\n",
      "   - Example({'question': 'What version is the latest release of the Northbridge Nimbus Compiler?', 'answer': '3.4', 'doc_id': 35}) (input_keys={'question'})\n",
      "   - Example({'question': 'How many kilometers does Aurora Route Kestrel cover?', 'answer': '690 kilometers', 'doc_id': 15}) (input_keys={'question'})\n",
      "\n",
      "PREDICTOR: Predict\n",
      "  demos: 3\n",
      "   - Example({'augmented': True, 'context': \"[34] The Northbridge Orion Protocol encrypts data using a 192-bit key. It was ratified in 2021.\\n\\n[24] Orion Project Orion's lead engineer is Mara Ortiz. The project budget was 8.4 million dollars. The project started in 2020 and is headquartered in Austin.\\n\\n[16] The Forge Forge Protocol encrypts data using a 128-bit key. It was ratified in 2019.\", 'question': 'Which year was the Northbridge Orion Protocol ratified?', 'answer': '2021'}) (input_keys=None)\n",
      "   - Example({'question': 'What version is the latest release of the Northbridge Nimbus Compiler?', 'answer': '3.4', 'doc_id': 35}) (input_keys={'question'})\n",
      "   - Example({'question': 'How many kilometers does Aurora Route Kestrel cover?', 'answer': '690 kilometers', 'doc_id': 15}) (input_keys={'question'})\n"
     ]
    }
   ],
   "source": [
    "# What improved? (baseline vs optimized on dev)\n",
    "\n",
    "print(\"Baseline metrics:\", baseline_metrics)\n",
    "print(\"Optimized metrics:\", optimized_metrics)\n",
    "\n",
    "def show_case(i, ex, base_pred, opt_pred):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"[dev #{i}] Q:\", ex.question)\n",
    "    print(\"Gold:\", ex.answer, \"| gold doc:\", ex.doc_id)\n",
    "    print(\"- Baseline\")\n",
    "    print(\"  rewritten:\", getattr(base_pred, \"rewritten_query\", None))\n",
    "    print(\"  context_ids:\", getattr(base_pred, \"context_ids\", None))\n",
    "    print(\"  answer:\", getattr(base_pred, \"answer\", None))\n",
    "    print(\"  answer_ok:\", exact_match(base_pred.answer, ex.answer))\n",
    "    print(\"- Optimized\")\n",
    "    print(\"  rewritten:\", getattr(opt_pred, \"rewritten_query\", None))\n",
    "    print(\"  context_ids:\", getattr(opt_pred, \"context_ids\", None))\n",
    "    print(\"  answer:\", getattr(opt_pred, \"answer\", None))\n",
    "    print(\"  answer_ok:\", exact_match(opt_pred.answer, ex.answer))\n",
    "\n",
    "improved = []\n",
    "regressed = []\n",
    "\n",
    "for i, ex in enumerate(devset):\n",
    "    base_pred = baseline(question=ex.question)\n",
    "    opt_pred  = optimized(question=ex.question)\n",
    "\n",
    "    base_ok = exact_match(base_pred.answer, ex.answer)\n",
    "    opt_ok  = exact_match(opt_pred.answer, ex.answer)\n",
    "\n",
    "    if (not base_ok) and opt_ok:\n",
    "        improved.append((i, ex, base_pred, opt_pred))\n",
    "    elif base_ok and (not opt_ok):\n",
    "        regressed.append((i, ex, base_pred, opt_pred))\n",
    "\n",
    "print(f\"\\nImproved cases (wrong→right): {len(improved)}\")\n",
    "print(f\"Regressions (right→wrong): {len(regressed)}\")\n",
    "\n",
    "# Show a few examples\n",
    "for tup in improved[:5]:\n",
    "    show_case(*tup)\n",
    "\n",
    "if regressed:\n",
    "    print(\"\\nShowing regressions:\")\n",
    "    for tup in regressed[:3]:\n",
    "        show_case(*tup)\n",
    "\n",
    "# Optional: inspect learned few-shot demos (“winning prompts”)\n",
    "def show_demos(label, prog):\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(label)\n",
    "    try:\n",
    "        preds = getattr(prog, \"predictors\", lambda: [])()\n",
    "        for p in preds:\n",
    "            print(\"\\nPREDICTOR:\", type(p).__name__)\n",
    "            demos = getattr(p, \"demos\", None)\n",
    "            if not demos:\n",
    "                print(\"  (no demos)\")\n",
    "                continue\n",
    "            print(f\"  demos: {len(demos)}\")\n",
    "            for d in demos[:3]:\n",
    "                # demos are dspy.Example-like objects\n",
    "                print(\"   -\", d)\n",
    "    except Exception as e:\n",
    "        print(\"Could not inspect demos:\", e)\n",
    "\n",
    "show_demos(\"Baseline demos\", baseline)\n",
    "show_demos(\"Optimized demos\", optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea830c69",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "\n",
    "- **Baseline vs optimized**:\n",
    "  - Baseline dev metrics were accuracy = 0.6667 and retrieval hit rate = 1.0.\n",
    "  - Optimized dev metrics were accuracy = 0.875 and retrieval hit rate = 1.0.\n",
    "\n",
    "- **Which metric improved**:\n",
    "   - Accuracy improved (0.6667 → 0.875).\n",
    "   - Retrieval hit rate did not change (stayed 1.0), the correct doc was already being retrieved.\n",
    "\n",
    "- **Why accuracy improved**:\n",
    "  - Most failures were answer formatting, not missing knowledge.\n",
    "  - Baseline often added extra words like \"version 3.4\" or \"JSON and CSV outputs\".\n",
    "  - Our metric is strict exact match, so those extra words count as wrong.\n",
    "  - The optimized program learned (via DSPy’s compiled demos) to output the short exact span (e.g., \"3.4\", \"JSON and CSV\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247d49",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274dcd0",
   "metadata": {},
   "source": [
    "Coding: Implement a simple version of EvoPrompt. Represent a prompt as a list of tokens or words. Define two evolutionary operators: mutate (randomly replace or insert a word) and crossover (swap a segment between two prompts). Use an LLM (or a heuristic function) to evaluate fitness (e.g. BLEU score or any task-specific score) of prompts. Start with a few initial prompts and run a few generations of evolution. Did the prompts improve? This could be done on a trivial task (like prompt an LLM to output a specific keyword - evolve prompts to maximize the occurrence of that keyword in the response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac5a66a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 2 configured: openai/gpt-4.1-mini dev_examples= 18\n"
     ]
    }
   ],
   "source": [
    "# EvoPrompt configuration (Exercise 2)\n",
    "# Multi-example dev set: prompts are scored by exact-match accuracy across examples.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "EVO_LM_MODEL = \"openai/gpt-4.1-mini\"\n",
    "\n",
    "POP_SIZE = 6\n",
    "GENERATIONS = 5\n",
    "ELITE_K = 2\n",
    "P_MUTATE = 0.7\n",
    "P_CROSSOVER = 0.6\n",
    "\n",
    "# Dev set for prompt fitness (varied targets, so you can't hardcode one answer)\n",
    "DEV_EXAMPLES = [\n",
    "    {\n",
    "        \"question\": \"What is the latest release version?\",\n",
    "        \"input\": \"The Northbridge Nimbus Compiler targets the Vesta VM. The latest release is version 3.4. The previous release was version 3.3.\",\n",
    "        \"target\": \"3.4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the latest release version?\",\n",
    "        \"input\": \"The Osprey Aurora Compiler targets the Redstone VM. The latest release is version 4.5. A beta build is labeled 4.6.\",\n",
    "        \"target\": \"4.5\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the latest release version?\",\n",
    "        \"input\": \"The Raven Redstone Compiler targets the Nova VM. The latest release is version 4.6. The previous release was 4.5.\",\n",
    "        \"target\": \"4.6\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many kilometers does the route cover?\",\n",
    "        \"input\": \"Cargo Aurora Route Kestrel covers 690 kilometers and departs on Thursdays. A backup route covers 710 kilometers.\",\n",
    "        \"target\": \"690 kilometers\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"On what day does the route depart?\",\n",
    "        \"input\": \"Cargo Delta Route Orion covers 540 kilometers and departs on Mondays. A test run departed on Fridays.\",\n",
    "        \"target\": \"Mondays\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What key size is used for encryption?\",\n",
    "        \"input\": \"The Zephyr Orion Protocol encrypts data using a 256-bit key. An older spec used 128-bit.\",\n",
    "        \"target\": \"256-bit\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In what year was it ratified?\",\n",
    "        \"input\": \"The Northbridge Orion Protocol encrypts data using a 192-bit key. It was ratified in 2021. A draft appeared in 2019.\",\n",
    "        \"target\": \"2021\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What material is the lens made of?\",\n",
    "        \"input\": \"The Slate Osprey Sensor uses a ceramic lens. The housing is aluminum.\",\n",
    "        \"target\": \"ceramic\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the lens diameter?\",\n",
    "        \"input\": \"The Raven Nimbus Sensor uses a sapphire lens. The lens diameter is 9 millimeters, and the casing diameter is 11 millimeters.\",\n",
    "        \"target\": \"9 millimeters\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which outputs does the API support?\",\n",
    "        \"input\": \"The Maple Redstone API has a default rate limit of 120 requests per minute. It supports JSON and CSV outputs.\",\n",
    "        \"target\": \"JSON and CSV\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In which city did the clinic open?\",\n",
    "        \"input\": \"The Atlas Kestrel Clinic runs on the Nova scheduling system. The clinic opened in Portland in 2022. Planning started in Austin.\",\n",
    "        \"target\": \"Portland\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long does it take to reach 80 percent?\",\n",
    "        \"input\": \"The Lumen Aurora Battery has a capacity of 4.2 kWh and charges to 80 percent in 22 minutes. A full charge takes 60 minutes.\",\n",
    "        \"target\": \"22 minutes\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many participants were enrolled?\",\n",
    "        \"input\": \"The Zephyr Atlas Study enrolled 240 participants in 2022 and reported 12 dropouts.\",\n",
    "        \"target\": \"240\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many floors does the library have?\",\n",
    "        \"input\": \"The Lumen Aster Library has 3 floors and 120 desks.\",\n",
    "        \"target\": \"3\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the project budget?\",\n",
    "        \"input\": \"The Orion Nova Project budget is 1.2 million dollars; current spending is 0.8 million dollars.\",\n",
    "        \"target\": \"1.2 million dollars\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In what year was the satellite launched?\",\n",
    "        \"input\": \"The Kestrel Aster Satellite launched in 2018 and was decommissioned in 2023.\",\n",
    "        \"target\": \"2018\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many days does the festival run?\",\n",
    "        \"input\": \"The Helix Northbridge Festival runs for 4 days and opens on June 3.\",\n",
    "        \"target\": \"4 days\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the capacity?\",\n",
    "        \"input\": \"The Vesta Cedar Battery has a capacity of 4.2 kWh and a peak output of 2.1 kW.\",\n",
    "        \"target\": \"4.2 kWh\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Clear cache/state when re-running\n",
    "fitness_cache = {}\n",
    "\n",
    "random.seed(42)\n",
    "print(\"Exercise 2 configured:\", EVO_LM_MODEL, \"dev_examples=\", len(DEV_EXAMPLES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d080dd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial population:\n",
      "0 Answer the question.\n",
      "1 Respond in a helpful way.\n",
      "2 Summarize the input and answer.\n",
      "3 Explain your reasoning and give the answer.\n",
      "4 Give a complete response to the question.\n",
      "5 Provide the best possible answer.\n"
     ]
    }
   ],
   "source": [
    "# Prompt representation + LLM-guided evolutionary operators\n",
    "\n",
    "def to_text(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens).strip()\n",
    "\n",
    "\n",
    "def from_text(text: str) -> List[str]:\n",
    "    # simple tokenization; keeps exercise requirement (list of words)\n",
    "    return [t for t in text.strip().split() if t]\n",
    "\n",
    "\n",
    "def _call_evo_lm(user_prompt: str) -> str:\n",
    "    \"\"\"Call the Exercise 2 LLM without changing Exercise 1 global settings.\"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=EVO_LM_MODEL.replace(\"openai/\", \"\"),\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful prompt engineer. Return only the prompt text.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def mutate(tokens: List[str]) -> List[str]:\n",
    "    prompt = to_text(tokens)\n",
    "    # Provide a few examples so the operator can steer toward exact-match extraction behavior.\n",
    "    demo = \"\\n\".join([\n",
    "        f\"- QUESTION: {DEV_EXAMPLES[i]['question']}\\n  INPUT: {DEV_EXAMPLES[i]['input']}\\n  TARGET: {DEV_EXAMPLES[i]['target']}\"\n",
    "        for i in range(min(3, len(DEV_EXAMPLES)))\n",
    "    ])\n",
    "\n",
    "    instr = (\n",
    "        \"You will receive a prompt that will be evaluated on a dev set. \"\n",
    "        \"For each dev example, the evaluation model will see:\\n\"\n",
    "        \"PROMPT\\n\\nQUESTION: <question>\\n\\nINPUT: <text>\\n\\nFINAL ANSWER:\\n\"\n",
    "        \"and must output exactly the TARGET string (no extra words).\\n\\n\"\n",
    "        \"Improve the prompt to maximize exact-match accuracy across the dev set. \"\n",
    "        \"Keep the prompt short and clear. Return ONLY the new prompt text.\\n\\n\"\n",
    "        \"DEV EXAMPLES (format):\\n\"\n",
    "        f\"{demo}\\n\\n\"\n",
    "        f\"CURRENT_PROMPT: {prompt}\\n\\n\"\n",
    "        \"NEW_PROMPT:\"\n",
    "    )\n",
    "    out = _call_evo_lm(instr)\n",
    "    return from_text(out)\n",
    "\n",
    "\n",
    "def crossover(a_tokens: List[str], b_tokens: List[str]) -> List[str]:\n",
    "    a = to_text(a_tokens)\n",
    "    b = to_text(b_tokens)\n",
    "\n",
    "    demo = \"\\n\".join([\n",
    "        f\"- QUESTION: {DEV_EXAMPLES[i]['question']}\\n  INPUT: {DEV_EXAMPLES[i]['input']}\\n  TARGET: {DEV_EXAMPLES[i]['target']}\"\n",
    "        for i in range(min(2, len(DEV_EXAMPLES)))\n",
    "    ])\n",
    "\n",
    "    instr = (\n",
    "        \"Combine the best parts of two prompts into one better prompt. \"\n",
    "        \"The prompt is evaluated across multiple dev examples. \"\n",
    "        \"For each example, the evaluation model sees QUESTION+INPUT and must output exactly TARGET (no extra words). \"\n",
    "        \"Keep the new prompt short and unambiguous. Return ONLY the new prompt text.\\n\\n\"\n",
    "        \"DEV EXAMPLES (format):\\n\"\n",
    "        f\"{demo}\\n\\n\"\n",
    "        f\"PROMPT_A: {a}\\n\"\n",
    "        f\"PROMPT_B: {b}\\n\\n\"\n",
    "        \"NEW_PROMPT:\"\n",
    "    )\n",
    "    out = _call_evo_lm(instr)\n",
    "    return from_text(out)\n",
    "\n",
    "\n",
    "# Seed prompts (intentionally imperfect)\n",
    "# These should try to extract the correct short answer from the INPUT text.\n",
    "# Seed prompts (intentionally weak/vague)\n",
    "# These start off bad so evolution has room to improve.\n",
    "seed_prompts: List[List[str]] = [\n",
    "    from_text(\"Answer the question.\"),\n",
    "    from_text(\"Respond in a helpful way.\"),\n",
    "    from_text(\"Summarize the input and answer.\"),\n",
    "    from_text(\"Explain your reasoning and give the answer.\"),\n",
    "    from_text(\"Give a complete response to the question.\"),\n",
    "    from_text(\"Provide the best possible answer.\"),\n",
    "]\n",
    "\n",
    "population: List[List[str]] = seed_prompts[:POP_SIZE]\n",
    "print(\"Initial population:\")\n",
    "for i,p in enumerate(population):\n",
    "    print(i, to_text(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf7c0b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EvoPrompt generations:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "EvoPrompt generations:  20%|██        | 1/5 [01:47<07:09, 107.39s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "EvoPrompt generations:  20%|██        | 1/5 [02:27<09:48, 147.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Generations run: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fitness is dev-set accuracy (exact match), not a single target phrase.\n",
    "FitnessCacheVal = Tuple[float, List[str]]  # (accuracy, outputs per example)\n",
    "\n",
    "# cache by (prompt_text, example_index)\n",
    "_run_cache: Dict[Tuple[str, int], str] = {}\n",
    "\n",
    "\n",
    "def run_candidate(prompt_tokens: List[str], ex_question: str, ex_input: str) -> str:\n",
    "    \"\"\"Run the candidate prompt on a single (question, input) pair and return the model output.\"\"\"\n",
    "    prompt_text = to_text(prompt_tokens)\n",
    "\n",
    "    user_text = f\"{prompt_text}\\n\\nQUESTION:\\n{ex_question}\\n\\nINPUT:\\n{ex_input}\\n\\nFINAL ANSWER:\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=EVO_LM_MODEL.replace(\"openai/\", \"\"),\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=64,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def fitness(prompt_tokens: List[str]) -> FitnessCacheVal:\n",
    "    prompt_text = to_text(prompt_tokens)\n",
    "\n",
    "    outputs: List[str] = []\n",
    "    correct = 0\n",
    "\n",
    "    ex_iter = range(len(DEV_EXAMPLES))\n",
    "    if tqdm is not None:\n",
    "        ex_iter = tqdm(ex_iter, desc=\"Dev examples\", total=len(DEV_EXAMPLES), leave=False)\n",
    "\n",
    "    for j in ex_iter:\n",
    "        key = (prompt_text, j)\n",
    "        if key in _run_cache:\n",
    "            out = _run_cache[key]\n",
    "        else:\n",
    "            out = run_candidate(prompt_tokens, DEV_EXAMPLES[j][\"question\"], DEV_EXAMPLES[j][\"input\"])\n",
    "            _run_cache[key] = out\n",
    "\n",
    "        outputs.append(out)\n",
    "        if out.strip() == DEV_EXAMPLES[j][\"target\"].strip():\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / max(1, len(DEV_EXAMPLES))\n",
    "    return acc, outputs\n",
    "\n",
    "\n",
    "def tournament_select(pop: List[List[str]], scores: List[float], k: int = 3) -> List[str]:\n",
    "    # Pick k random individuals and return the best\n",
    "    idxs = [random.randrange(len(pop)) for _ in range(k)]\n",
    "    best = max(idxs, key=lambda i: scores[i])\n",
    "    return pop[best]\n",
    "\n",
    "\n",
    "def evolve(population: List[List[str]]) -> Tuple[List[List[str]], List[dict]]:\n",
    "    history = []\n",
    "\n",
    "    gen_iter = range(GENERATIONS)\n",
    "    if tqdm is not None:\n",
    "        gen_iter = tqdm(gen_iter, desc=\"EvoPrompt generations\", total=GENERATIONS)\n",
    "\n",
    "    for gen in gen_iter:\n",
    "        # Evaluate population fitness\n",
    "        pop_iter = range(len(population))\n",
    "        if tqdm is not None:\n",
    "            pop_iter = tqdm(pop_iter, desc=f\"Fitness eval (gen {gen})\", total=len(population), leave=False)\n",
    "\n",
    "        fits: List[float] = []\n",
    "        outs: List[List[str]] = []\n",
    "        for i in pop_iter:\n",
    "            f, o_list = fitness(population[i])\n",
    "            fits.append(f)\n",
    "            outs.append(o_list)\n",
    "\n",
    "        # Rank\n",
    "        ranked = sorted(range(len(population)), key=lambda i: fits[i], reverse=True)\n",
    "        best_i = ranked[0]\n",
    "        best = population[best_i]\n",
    "        history.append(\n",
    "            {\n",
    "                \"gen\": gen,\n",
    "                \"best_accuracy\": fits[best_i],\n",
    "                \"best_prompt\": to_text(best),\n",
    "                \"best_outputs\": outs[best_i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Early stop if perfect accuracy\n",
    "        if fits[best_i] >= 1.0:\n",
    "            break\n",
    "\n",
    "        # Elitism\n",
    "        new_pop: List[List[str]] = [population[i] for i in ranked[:ELITE_K]]\n",
    "\n",
    "        # Refill\n",
    "        while len(new_pop) < POP_SIZE:\n",
    "            child: List[str]\n",
    "            if random.random() < P_CROSSOVER:\n",
    "                pa = tournament_select(population, fits, k=3)\n",
    "                pb = tournament_select(population, fits, k=3)\n",
    "                child = crossover(pa, pb)\n",
    "            else:\n",
    "                pa = tournament_select(population, fits, k=3)\n",
    "                child = pa[:]\n",
    "\n",
    "            if random.random() < P_MUTATE:\n",
    "                child = mutate(child)\n",
    "\n",
    "            # Basic guardrail\n",
    "            if not child:\n",
    "                child = from_text(\"Output exactly\")\n",
    "\n",
    "            new_pop.append(child)\n",
    "\n",
    "        population = new_pop\n",
    "\n",
    "    return population, history\n",
    "\n",
    "\n",
    "final_population, evo_history = evolve(population)\n",
    "print(\"Done. Generations run:\", len(evo_history))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58a64091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EvoPrompt results (multi-example accuracy) ===\n",
      "Dev examples: 18\n",
      "\n",
      "Best per generation:\n",
      "gen=0  best_accuracy=0.500 | prompt=Answer the question.\n",
      "gen=1  best_accuracy=1.000 | prompt=Extract and return exactly the answer to the question based on the input text. Do not add any extra words or explanation.\n",
      "\n",
      "Initial best:\n",
      "  prompt: Answer the question.\n",
      "  accuracy: 0.5\n",
      "\n",
      "Final best:\n",
      "  prompt: Extract and return exactly the answer to the question based on the input text. Do not add any extra words or explanation.\n",
      "  accuracy: 1.0\n",
      "\n",
      "Did it improve? True\n",
      "\n",
      "Sample dev examples (before vs after):\n",
      "\n",
      "---\n",
      "INPUT: The Northbridge Nimbus Compiler targets the Vesta VM. The latest release is version 3.4. The previous release was version 3.3.\n",
      "TARGET: 3.4\n",
      "BASE_OUT: 3.4\n",
      "FINAL_OUT: 3.4\n",
      "\n",
      "---\n",
      "INPUT: The Osprey Aurora Compiler targets the Redstone VM. The latest release is version 4.5. A beta build is labeled 4.6.\n",
      "TARGET: 4.5\n",
      "BASE_OUT: 4.5\n",
      "FINAL_OUT: 4.5\n",
      "\n",
      "---\n",
      "INPUT: The Raven Redstone Compiler targets the Nova VM. The latest release is version 4.6. The previous release was 4.5.\n",
      "TARGET: 4.6\n",
      "BASE_OUT: 4.6\n",
      "FINAL_OUT: 4.6\n",
      "\n",
      "---\n",
      "INPUT: Cargo Aurora Route Kestrel covers 690 kilometers and departs on Thursdays. A backup route covers 710 kilometers.\n",
      "TARGET: 690 kilometers\n",
      "BASE_OUT: The route covers 690 kilometers.\n",
      "\n",
      "FINAL ANSWER: 690 kilometers\n",
      "FINAL_OUT: 690 kilometers\n",
      "\n",
      "---\n",
      "INPUT: Cargo Delta Route Orion covers 540 kilometers and departs on Mondays. A test run departed on Fridays.\n",
      "TARGET: Mondays\n",
      "BASE_OUT: The route normally departs on Mondays. The test run, however, departed on a Friday.\n",
      "\n",
      "FINAL ANSWER: The route departs on Mondays.\n",
      "FINAL_OUT: Mondays\n",
      "\n",
      "Winning prompt tokens: ['Extract', 'and', 'return', 'exactly', 'the', 'answer', 'to', 'the', 'question', 'based', 'on', 'the', 'input', 'text.', 'Do', 'not', 'add', 'any', 'extra', 'words', 'or', 'explanation.']\n"
     ]
    }
   ],
   "source": [
    "# Reporting: did prompts improve?\n",
    "\n",
    "if not evo_history:\n",
    "    raise RuntimeError(\"No evolution history; did you run the evolution cell?\")\n",
    "\n",
    "print(\"\\n=== EvoPrompt results (multi-example accuracy) ===\")\n",
    "print(\"Dev examples:\", len(DEV_EXAMPLES))\n",
    "\n",
    "# Show best per generation\n",
    "print(\"\\nBest per generation:\")\n",
    "for row in evo_history:\n",
    "    print(f\"gen={row['gen']:<2} best_accuracy={row['best_accuracy']:.3f} | prompt={row['best_prompt']}\")\n",
    "\n",
    "# Compare initial best vs final best\n",
    "initial_best = evo_history[0]\n",
    "final_best = evo_history[-1]\n",
    "\n",
    "print(\"\\nInitial best:\")\n",
    "print(\"  prompt:\", initial_best[\"best_prompt\"])\n",
    "print(\"  accuracy:\", initial_best[\"best_accuracy\"])\n",
    "\n",
    "print(\"\\nFinal best:\")\n",
    "print(\"  prompt:\", final_best[\"best_prompt\"])\n",
    "print(\"  accuracy:\", final_best[\"best_accuracy\"])\n",
    "\n",
    "improved = final_best[\"best_accuracy\"] > initial_best[\"best_accuracy\"]\n",
    "print(\"\\nDid it improve?\", improved)\n",
    "\n",
    "# Show a few per-example before/after outputs\n",
    "print(\"\\nSample dev examples (before vs after):\")\n",
    "for i in range(min(5, len(DEV_EXAMPLES))):\n",
    "    ex = DEV_EXAMPLES[i]\n",
    "    base_out = initial_best[\"best_outputs\"][i]\n",
    "    final_out = final_best[\"best_outputs\"][i]\n",
    "    print(\"\\n---\")\n",
    "    print(\"INPUT:\", ex[\"input\"])\n",
    "    print(\"TARGET:\", ex[\"target\"])\n",
    "    print(\"BASE_OUT:\", base_out)\n",
    "    print(\"FINAL_OUT:\", final_out)\n",
    "\n",
    "# Show the winning prompt tokens\n",
    "winning_tokens = from_text(final_best[\"best_prompt\"])\n",
    "print(\"\\nWinning prompt tokens:\", winning_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07d3c3",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeb964",
   "metadata": {},
   "source": [
    "Compare reinforcement learning vs. evolutionary search for prompt optimization. If our “policy” is the prompt text and the “environment” gives a reward (quality score), RL would tweak the prompt based on gradient of reward (if possible) or black-box optimization. Evolutionary methods like GEPA/EvoPrompt treat it like a search over strings. List pros and cons of each: e.g., RL (with methods like RLPrompt or policy gradients) can directly optimize an objective but may get stuck in local optima or require many samples; evolutionary approaches are more global and can incorporate heuristic knowledge (via LLM reflections in GEPA) but might be slower if search space is huge. In practice, why might GEPA’s ability to incorporate natural language reflections be advantageous in prompt tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c42ade",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8475c",
   "metadata": {},
   "source": [
    "RL-style prompt optimization treats the prompt as a policy over tokens and improves it from a scalar reward signal. The upside is that you can directly optimize whatever reward you define and it can discover “weird but effective” prompts. The downsides are: high sample cost ( low sample efficiency and many rollouts needed), sensitivity to reward design/hyperparameters, can be unstable, the learned prompts can be brittle and uninterpretable.\n",
    "\n",
    "Evolutionary search treats prompts (often per-module prompts in a pipeline) as candidates in a population and uses mutation/crossover + selection (often Pareto/multi-objective) to explore more globally. Upsides: better at avoiding greedy local minima, naturally supports multi-objective tradeoffs (accuracy vs cost/latency), works well for compound systems (retriever → reasoner → formatter), and can be more sample-efficient. \n",
    "\n",
    "GEPA’s natural-language reflection upgrades the learning signal from “reward = 0.63” to actionable, structured credit assignment. By reading execution traces (retrieval results, tool errors, reasoning steps) the reflector can say what failed and why and propose a targeted prompt patch (e.g., “don’t paraphrase the original query; retrieve missing hop entities”). That leverages 2025-era reasoning models’ strengths—turning a few rollouts into meaningful edits—whereas RL often needs many rollouts to infer the same lesson from scalar rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce57b8",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2da32",
   "metadata": {},
   "source": [
    "*Coding (ACE):* Implement a simplified **ACE-style context evolution** system. Create a playbook with sections: STRATEGIES, CODE_SNIPPETS, PITFALLS. Each entry has an ID, helpful/harmful counters, and content. For a simple task (e.g., math problems), implement: (1) **Generator** produces a solution noting which playbook entries helped/hurt, (2) **Reflector** extracts a lesson from success/failure, (3) **Curator** converts the lesson to a structured bullet, checks for duplicates, and merges with counters. Run 20-30 iterations. Compare against a baseline that concatenates all lessons without structure. Does the structured approach prevent context collapse and preserve early lessons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17756727",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a13f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 (ACE) - setup\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "ACE_MODEL = \"openai/gpt-4.1\"\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3f65b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playbook structures + helpers\n",
    "SECTION_PREFIX = {\n",
    "    \"STRATEGIES\": \"str\",\n",
    "    \"CODE_SNIPPETS\": \"code\",\n",
    "    \"PITFALLS\": \"pit\",\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class PlaybookEntry:\n",
    "    section: str\n",
    "    entry_id: str\n",
    "    helpful: int\n",
    "    harmful: int\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Playbook:\n",
    "    sections: Dict[str, List[PlaybookEntry]] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"STRATEGIES\": [],\n",
    "            \"CODE_SNIPPETS\": [],\n",
    "            \"PITFALLS\": [],\n",
    "        }\n",
    "    )\n",
    "    counters: Dict[str, int] = field(\n",
    "        default_factory=lambda: {\"STRATEGIES\": 0, \"CODE_SNIPPETS\": 0, \"PITFALLS\": 0}\n",
    "    )\n",
    "\n",
    "\n",
    "def new_entry_id(playbook: Playbook, section: str) -> str:\n",
    "    playbook.counters[section] += 1\n",
    "    prefix = SECTION_PREFIX[section]\n",
    "    return f\"{prefix}-{playbook.counters[section]:05d}\"\n",
    "\n",
    "\n",
    "def render_playbook(playbook: Playbook) -> str:\n",
    "    lines: List[str] = []\n",
    "    for section in [\"STRATEGIES\", \"CODE_SNIPPETS\", \"PITFALLS\"]:\n",
    "        lines.append(f\"## {section}\")\n",
    "        entries = playbook.sections[section]\n",
    "        if not entries:\n",
    "            lines.append(\"(empty)\")\n",
    "        else:\n",
    "            for e in entries:\n",
    "                lines.append(f\"[{e.entry_id}] helpful={e.helpful} harmful={e.harmful} :: {e.content}\")\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9 ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def jaccard_sim(a: str, b: str) -> float:\n",
    "    a_set = set(normalize_text(a).split())\n",
    "    b_set = set(normalize_text(b).split())\n",
    "    if not a_set or not b_set:\n",
    "        return 0.0\n",
    "    return len(a_set & b_set) / len(a_set | b_set)\n",
    "\n",
    "\n",
    "def find_duplicate(playbook: Playbook, section: str, content: str, threshold: float = 0.7):\n",
    "    for entry in playbook.sections[section]:\n",
    "        if jaccard_sim(entry.content, content) >= threshold:\n",
    "            return entry\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_id_map(playbook: Playbook) -> Dict[str, PlaybookEntry]:\n",
    "    id_map: Dict[str, PlaybookEntry] = {}\n",
    "    for section_entries in playbook.sections.values():\n",
    "        for entry in section_entries:\n",
    "            id_map[entry.entry_id] = entry\n",
    "    return id_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bfd02f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task generator: rule-heavy extraction domain (synthetic finance)\n",
    "\n",
    "RULE_HINTS = [\n",
    "    \"Core Value = Revenue - COGS - OPEX + Rebates\",\n",
    "    \"Stability Index = Core Value - 2*Penalties\",\n",
    "    \"Unit Score = Stability Index / Units\",\n",
    "    \"Use FY2024 values only (ignore FY2023 and FY2025 forecast).\",\n",
    "]\n",
    "\n",
    "\n",
    "def _format_report(values_2024, values_2023, values_2025):\n",
    "    return (\n",
    "        \"FY2024 (USD millions): \"\n",
    "        f\"Revenue={values_2024['revenue']} COGS={values_2024['cogs']} \"\n",
    "        f\"OPEX={values_2024['opex']} Rebates={values_2024['rebates']} \"\n",
    "        f\"Penalties={values_2024['penalties']} Units={values_2024['units']}\\n\"\n",
    "        \"FY2023 (USD millions): \"\n",
    "        f\"Revenue={values_2023['revenue']} COGS={values_2023['cogs']} \"\n",
    "        f\"OPEX={values_2023['opex']} Rebates={values_2023['rebates']} \"\n",
    "        f\"Penalties={values_2023['penalties']} Units={values_2023['units']}\\n\"\n",
    "        \"FY2025 Forecast (USD millions): \"\n",
    "        f\"Revenue={values_2025['revenue']} COGS={values_2025['cogs']} \"\n",
    "        f\"OPEX={values_2025['opex']} Rebates={values_2025['rebates']} \"\n",
    "        f\"Penalties={values_2025['penalties']} Units={values_2025['units']}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _generate_values(rng: random.Random):\n",
    "    while True:\n",
    "        revenue = rng.randint(900, 2200)\n",
    "        cogs = rng.randint(250, 900)\n",
    "        opex = rng.randint(180, 800)\n",
    "        rebates = rng.randint(10, 150)\n",
    "        penalties = rng.randint(5, 80)\n",
    "        core = revenue - cogs - opex + rebates\n",
    "        stability = core - 2 * penalties\n",
    "        if stability <= 50:\n",
    "            continue\n",
    "        divisors = [d for d in range(2, 21) if stability % d == 0]\n",
    "        if not divisors:\n",
    "            continue\n",
    "        units = rng.choice(divisors)\n",
    "        values_2024 = {\n",
    "            \"revenue\": revenue,\n",
    "            \"cogs\": cogs,\n",
    "            \"opex\": opex,\n",
    "            \"rebates\": rebates,\n",
    "            \"penalties\": penalties,\n",
    "            \"units\": units,\n",
    "        }\n",
    "        return values_2024\n",
    "\n",
    "\n",
    "def _jitter_values(base, rng: random.Random, scale: int):\n",
    "    return {\n",
    "        \"revenue\": max(100, base[\"revenue\"] + rng.randint(-scale, scale)),\n",
    "        \"cogs\": max(50, base[\"cogs\"] + rng.randint(-scale // 2, scale // 2)),\n",
    "        \"opex\": max(50, base[\"opex\"] + rng.randint(-scale // 2, scale // 2)),\n",
    "        \"rebates\": max(0, base[\"rebates\"] + rng.randint(-20, 20)),\n",
    "        \"penalties\": max(0, base[\"penalties\"] + rng.randint(-15, 15)),\n",
    "        \"units\": max(2, base[\"units\"] + rng.choice([-2, -1, 0, 1, 2])),\n",
    "    }\n",
    "\n",
    "\n",
    "def _compute_metrics(values):\n",
    "    core = values[\"revenue\"] - values[\"cogs\"] - values[\"opex\"] + values[\"rebates\"]\n",
    "    stability = core - 2 * values[\"penalties\"]\n",
    "    unit_score = stability // values[\"units\"]\n",
    "    return {\n",
    "        \"core_value\": core,\n",
    "        \"stability_index\": stability,\n",
    "        \"unit_score\": unit_score,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_tasks(n: int, rng: random.Random) -> List[Dict[str, str]]:\n",
    "    tasks: List[Dict[str, str]] = []\n",
    "    q_types = [\n",
    "        \"core_value\",\n",
    "        \"stability_index\",\n",
    "        \"unit_score\",\n",
    "        \"revenue\",\n",
    "        \"penalties\",\n",
    "    ]\n",
    "\n",
    "    for _ in range(n):\n",
    "        values_2024 = _generate_values(rng)\n",
    "        values_2023 = _jitter_values(values_2024, rng, 250)\n",
    "        values_2025 = _jitter_values(values_2024, rng, 300)\n",
    "        report = _format_report(values_2024, values_2023, values_2025)\n",
    "\n",
    "        q_type = rng.choice(q_types)\n",
    "        metrics_2024 = _compute_metrics(values_2024)\n",
    "        if q_type == \"core_value\":\n",
    "            answer = metrics_2024[\"core_value\"]\n",
    "            q_text = \"What is the FY2024 Core Value? Answer with a number only.\"\n",
    "        elif q_type == \"stability_index\":\n",
    "            answer = metrics_2024[\"stability_index\"]\n",
    "            q_text = \"What is the FY2024 Stability Index? Answer with a number only.\"\n",
    "        elif q_type == \"unit_score\":\n",
    "            answer = metrics_2024[\"unit_score\"]\n",
    "            q_text = \"What is the FY2024 Unit Score? Answer with a number only.\"\n",
    "        elif q_type == \"revenue\":\n",
    "            answer = values_2024[\"revenue\"]\n",
    "            q_text = \"What is FY2024 Revenue? Answer with a number only.\"\n",
    "        else:\n",
    "            answer = values_2024[\"penalties\"]\n",
    "            q_text = \"What are FY2024 Penalties? Answer with a number only.\"\n",
    "\n",
    "        tasks.append(\n",
    "            {\n",
    "                \"report\": report,\n",
    "                \"question\": q_text,\n",
    "                \"q_type\": q_type,\n",
    "                \"answer\": str(answer),\n",
    "                \"values_2024\": values_2024,\n",
    "                \"values_2023\": values_2023,\n",
    "                \"values_2025\": values_2025,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return tasks\n",
    "\n",
    "\n",
    "TASKS = make_tasks(80, random.Random(123))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b1ac0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM helpers: generator + reflector\n",
    "\n",
    "def call_ace_lm(system: str, user: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=ACE_MODEL.replace(\"openai/\", \"\"),\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=250,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def parse_json(text: str) -> Dict:\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.S)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "def normalize_number(text: str) -> str:\n",
    "    return re.sub(r\"[^0-9-]\", \"\", text or \"\")\n",
    "\n",
    "\n",
    "def clean_lesson(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text[:180].rstrip()\n",
    "\n",
    "\n",
    "def pick_section(raw_section: str, content: str, success: bool) -> str:\n",
    "    if not success:\n",
    "        return \"PITFALLS\"\n",
    "    section = raw_section.upper().strip()\n",
    "    if section not in {\"STRATEGIES\", \"CODE_SNIPPETS\", \"PITFALLS\"}:\n",
    "        section = \"STRATEGIES\"\n",
    "    if \"=\" in content or \"*\" in content or \"/\" in content:\n",
    "        return \"CODE_SNIPPETS\"\n",
    "    if \"avoid\" in content.lower() or \"don't\" in content.lower() or \"do not\" in content.lower():\n",
    "        return \"PITFALLS\"\n",
    "    return section\n",
    "\n",
    "\n",
    "def format_question(task: Dict[str, str], include_hint: bool) -> str:\n",
    "    hint = (\"\\nHINTS:\\n\" + \"\\n\".join(RULE_HINTS)) if include_hint else \"\"\n",
    "    return f\"{task['report']}\\n\\n{task['question']}{hint}\"\n",
    "\n",
    "\n",
    "def generator(playbook_text: str, question: str, available_ids: List[str], pitfall_ids: List[str]) -> Dict:\n",
    "    system = \"You solve word problems. Return ONLY valid JSON.\"\n",
    "    user = (\n",
    "        \"PLAYBOOK:\\n\"\n",
    "        f\"{playbook_text}\\n\\n\"\n",
    "        \"QUESTION:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"Return JSON with keys:\\n\"\n",
    "        \"- answer: string (number only)\\n\"\n",
    "        \"- helped_ids: array of IDs from AVAILABLE_IDS that you used (include PITFALLS you avoided)\\n\"\n",
    "        \"- hurt_ids: array of IDs from AVAILABLE_IDS that misled you\\n\\n\"\n",
    "        \"If AVAILABLE_IDS is non-empty, include at least one ID in helped_ids or hurt_ids.\\n\"\n",
    "        \"If PITFALL_IDS is non-empty, include at least one of those IDs in helped_ids (as an avoided mistake).\\n\"\n",
    "        f\"AVAILABLE_IDS: {available_ids}\\n\"\n",
    "        f\"PITFALL_IDS: {pitfall_ids}\\n\"\n",
    "        \"JSON:\"\n",
    "    )\n",
    "    raw = call_ace_lm(system, user)\n",
    "    data = parse_json(raw)\n",
    "    answer = str(data.get(\"answer\", \"\")).strip()\n",
    "    helped = [x for x in data.get(\"helped_ids\", []) if x in available_ids]\n",
    "    hurt = [x for x in data.get(\"hurt_ids\", []) if x in available_ids]\n",
    "\n",
    "    if available_ids and not helped and not hurt:\n",
    "        helped = [available_ids[0]]\n",
    "\n",
    "    if pitfall_ids and not any(x in pitfall_ids for x in helped):\n",
    "        helped.append(pitfall_ids[0])\n",
    "\n",
    "    return {\"answer\": answer, \"helped_ids\": helped, \"hurt_ids\": hurt}\n",
    "\n",
    "\n",
    "def _alt_answer(task: Dict[str, str], source: str, variant: str):\n",
    "    values = task[source]\n",
    "    core = values[\"revenue\"] - values[\"cogs\"] - values[\"opex\"] + values[\"rebates\"]\n",
    "    if variant == \"no_rebates\":\n",
    "        core = values[\"revenue\"] - values[\"cogs\"] - values[\"opex\"]\n",
    "    if variant == \"no_double_penalty\":\n",
    "        stability = core - values[\"penalties\"]\n",
    "    else:\n",
    "        stability = core - 2 * values[\"penalties\"]\n",
    "    unit_score = stability // values[\"units\"]\n",
    "    if task[\"q_type\"] == \"core_value\":\n",
    "        return core\n",
    "    if task[\"q_type\"] == \"stability_index\":\n",
    "        return stability\n",
    "    if task[\"q_type\"] == \"unit_score\":\n",
    "        return unit_score\n",
    "    if task[\"q_type\"] == \"revenue\":\n",
    "        return values[\"revenue\"]\n",
    "    return values[\"penalties\"]\n",
    "\n",
    "\n",
    "def reflector(question: str, gold: str, pred: str, success: bool, task: Dict[str, str]) -> Dict:\n",
    "    system = \"You are a reflection agent. Return ONLY valid JSON.\"\n",
    "    result = \"correct\" if success else \"incorrect\"\n",
    "    pred_num = normalize_number(pred)\n",
    "    error_hint = \"\"\n",
    "    if not success and pred_num:\n",
    "        pred_val = int(pred_num)\n",
    "        if pred_val == _alt_answer(task, \"values_2023\", \"default\"):\n",
    "            error_hint = \"Used FY2023 values instead of FY2024.\"\n",
    "        elif pred_val == _alt_answer(task, \"values_2025\", \"default\"):\n",
    "            error_hint = \"Used FY2025 forecast values instead of FY2024.\"\n",
    "        elif pred_val == _alt_answer(task, \"values_2024\", \"no_double_penalty\"):\n",
    "            error_hint = \"Forgot to double penalties in Stability Index.\"\n",
    "        elif pred_val == _alt_answer(task, \"values_2024\", \"no_rebates\"):\n",
    "            error_hint = \"Forgot to add rebates in Core Value.\"\n",
    "\n",
    "    user = (\n",
    "        f\"QUESTION: {question}\\n\"\n",
    "        f\"GOLD: {gold}\\n\"\n",
    "        f\"MODEL_ANSWER: {pred}\\n\"\n",
    "        f\"RESULT: {result}\\n\"\n",
    "        f\"ERROR_HINT: {error_hint or 'none'}\\n\\n\"\n",
    "        \"Write one short, specific lesson that improves future answers. \"\n",
    "        \"If incorrect, write it as a PITFALL to avoid next time. \"\n",
    "        \"If correct, write a strategy or a formula. \"\n",
    "        \"Prefer explicit formulas, e.g., 'stability = core - 2*penalties'.\\n\"\n",
    "        'Return JSON: {\"section\": ..., \"content\": ...}'\n",
    "    )\n",
    "    raw = call_ace_lm(system, user)\n",
    "    data = parse_json(raw)\n",
    "    content = clean_lesson(str(data.get(\"content\", \"Convert units before computing.\")))\n",
    "    section = pick_section(str(data.get(\"section\", \"STRATEGIES\")), content, success)\n",
    "    return {\"section\": section, \"content\": content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cbff7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured accuracy: 0.55\n",
      "Baseline accuracy: 0.375\n",
      "Early lesson retention (structured): 0 / 5\n",
      "Early lesson retention (baseline): 0 / 5\n",
      "\n",
      "Final playbook:\n",
      "\n",
      "## STRATEGIES\n",
      "[str-00001] helpful=24 harmful=0 :: To find FY2024 Revenue, directly extract the value labeled 'Revenue' under FY2024 from the data provided.\n",
      "[str-00002] helpful=5 harmful=0 :: To find the value of a specific financial metric for a given fiscal year, directly extract the number associated with that metric from the provided data without additional calculat\n",
      "\n",
      "## CODE_SNIPPETS\n",
      "[code-00001] helpful=17 harmful=0 :: To find FY2024 Penalties, directly use the given FY2024 Penalties value. For related calculations, use formulas: Core Value = Revenue - COGS - OPEX + Rebates; Stability Index = Cor\n",
      "[code-00002] helpful=19 harmful=0 :: Core Value = Revenue - COGS - OPEX + Rebates; apply this formula directly using FY2024 values to calculate Core Value accurately.\n",
      "[code-00003] helpful=9 harmful=0 :: Core Value = Revenue - COGS - OPEX - Rebates - Penalties\n",
      "\n",
      "## PITFALLS\n",
      "(empty)\n"
     ]
    }
   ],
   "source": [
    "# Curator + evaluation loop\n",
    "\n",
    "def update_help_hurt(playbook: Playbook, helped_ids: List[str], hurt_ids: List[str]) -> None:\n",
    "    id_map = build_id_map(playbook)\n",
    "    for entry_id in helped_ids:\n",
    "        if entry_id in id_map:\n",
    "            id_map[entry_id].helpful += 1\n",
    "    for entry_id in hurt_ids:\n",
    "        if entry_id in id_map:\n",
    "            id_map[entry_id].harmful += 1\n",
    "\n",
    "\n",
    "def prune_playbook(playbook: Playbook, max_entries_per_section: int = 18) -> None:\n",
    "    for section in playbook.sections:\n",
    "        entries = playbook.sections[section]\n",
    "        entries = [\n",
    "            e\n",
    "            for e in entries\n",
    "            if not ((e.harmful >= e.helpful and e.harmful >= 2) or (e.helpful == 0 and e.harmful > 0))\n",
    "        ]\n",
    "        if len(entries) > max_entries_per_section:\n",
    "            entries = sorted(\n",
    "                entries,\n",
    "                key=lambda e: (e.helpful - e.harmful, e.helpful),\n",
    "                reverse=True,\n",
    "            )[:max_entries_per_section]\n",
    "        playbook.sections[section] = entries\n",
    "\n",
    "\n",
    "def curate(playbook: Playbook, section: str, content: str, success: bool) -> None:\n",
    "    content = content.strip()\n",
    "    if not content:\n",
    "        return\n",
    "    existing = find_duplicate(playbook, section, content, threshold=0.5)\n",
    "    if existing:\n",
    "        if success:\n",
    "            existing.helpful += 1\n",
    "        else:\n",
    "            existing.harmful += 1\n",
    "        return\n",
    "\n",
    "    entry_id = new_entry_id(playbook, section)\n",
    "    entry = PlaybookEntry(\n",
    "        section=section,\n",
    "        entry_id=entry_id,\n",
    "        helpful=1 if success else 0,\n",
    "        harmful=0 if success else 1,\n",
    "        content=content,\n",
    "    )\n",
    "    playbook.sections[section].append(entry)\n",
    "\n",
    "\n",
    "def run_ace(iterations: int = 40, baseline_budget: int = 900, hint_steps: int = 8):\n",
    "    playbook = Playbook()\n",
    "    baseline_lessons: List[str] = []\n",
    "    history: List[Dict] = []\n",
    "    early_structured: List[str] = []\n",
    "    early_baseline: List[str] = []\n",
    "    rng = random.Random(999)\n",
    "\n",
    "    for step in range(iterations):\n",
    "        task = rng.choice(TASKS)\n",
    "        question = format_question(task, include_hint=step < hint_steps)\n",
    "        gold = task[\"answer\"]\n",
    "\n",
    "        # Structured playbook run\n",
    "        playbook_text = render_playbook(playbook)\n",
    "        id_map = build_id_map(playbook)\n",
    "        available_ids = list(id_map.keys())\n",
    "        pitfall_ids = [e.entry_id for e in playbook.sections[\"PITFALLS\"]]\n",
    "        gen = generator(playbook_text, question, available_ids, pitfall_ids)\n",
    "        pred = gen[\"answer\"]\n",
    "        success = normalize_number(pred) == normalize_number(gold)\n",
    "        update_help_hurt(playbook, gen[\"helped_ids\"], gen[\"hurt_ids\"])\n",
    "        lesson = reflector(question, gold, pred, success, task)\n",
    "        curate(playbook, lesson[\"section\"], lesson[\"content\"], success)\n",
    "        prune_playbook(playbook)\n",
    "\n",
    "        if step < 5:\n",
    "            early_structured.append(lesson[\"content\"])\n",
    "\n",
    "        # Baseline: concat lessons, no structure or dedupe\n",
    "        baseline_context = \"\\n\".join(baseline_lessons)\n",
    "        if len(baseline_context) > baseline_budget:\n",
    "            baseline_context = baseline_context[-baseline_budget:]\n",
    "        baseline_notes = f\"NOTES:\\n{baseline_context}\" if baseline_context else \"(no lessons)\"\n",
    "        base_gen = generator(baseline_notes, question, [], [])\n",
    "        base_pred = base_gen[\"answer\"]\n",
    "        base_success = normalize_number(base_pred) == normalize_number(gold)\n",
    "        base_lesson = reflector(question, gold, base_pred, base_success, task)\n",
    "        baseline_lessons.append(base_lesson[\"content\"])\n",
    "        if step < 5:\n",
    "            early_baseline.append(base_lesson[\"content\"])\n",
    "\n",
    "        history.append(\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"structured_ok\": success,\n",
    "                \"baseline_ok\": base_success,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    structured_acc = sum(1 for h in history if h[\"structured_ok\"]) / len(history)\n",
    "    baseline_acc = sum(1 for h in history if h[\"baseline_ok\"]) / len(history)\n",
    "\n",
    "    final_playbook_text = render_playbook(playbook)\n",
    "    baseline_context = \"\\n\".join(baseline_lessons)\n",
    "    if len(baseline_context) > baseline_budget:\n",
    "        baseline_context = baseline_context[-baseline_budget:]\n",
    "\n",
    "    structured_retained = sum(1 for l in early_structured if l in final_playbook_text)\n",
    "    baseline_retained = sum(1 for l in early_baseline if l in baseline_context)\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"structured_acc\": structured_acc,\n",
    "        \"baseline_acc\": baseline_acc,\n",
    "        \"structured_retained\": structured_retained,\n",
    "        \"baseline_retained\": baseline_retained,\n",
    "        \"playbook\": playbook,\n",
    "        \"baseline_context\": baseline_context,\n",
    "    }\n",
    "\n",
    "results = run_ace(iterations=40, baseline_budget=900, hint_steps=8)\n",
    "print(\"Structured accuracy:\", round(results[\"structured_acc\"], 3))\n",
    "print(\"Baseline accuracy:\", round(results[\"baseline_acc\"], 3))\n",
    "print(\"Early lesson retention (structured):\", results[\"structured_retained\"], \"/ 5\")\n",
    "print(\"Early lesson retention (baseline):\", results[\"baseline_retained\"], \"/ 5\")\n",
    "print(\"\\nFinal playbook:\\n\")\n",
    "print(render_playbook(results[\"playbook\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
