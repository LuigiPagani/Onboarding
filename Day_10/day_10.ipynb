{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba9c19a",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8bb81",
   "metadata": {},
   "source": [
    "Use DSPy (or a simplified version if DSPy isn’t accessible) to optimize a multi-step QA pipeline. For example, pipeline: (1) retrieve relevant text from a small corpus, (2) ask LLM to answer question given retrieved text. Define the metric as accuracy of answer. Let the system tune the retrieval prompt and answer prompt. Observe what changes it makes (e.g. does it add “Let’s think step by step” automatically?). Report the before vs after performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47be01",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d4197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4247d49",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274dcd0",
   "metadata": {},
   "source": [
    "Coding: Implement a simple version of EvoPrompt. Represent a prompt as a list of tokens or words. Define two evolutionary operators: mutate (randomly replace or insert a word) and crossover (swap a segment between two prompts). Use an LLM (or a heuristic function) to evaluate fitness (e.g. BLEU score or any task-specific score) of prompts. Start with a few initial prompts and run a few generations of evolution. Did the prompts improve? This could be done on a trivial task (like prompt an LLM to output a specific keyword - evolve prompts to maximize the occurrence of that keyword in the response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42147c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07d3c3",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeb964",
   "metadata": {},
   "source": [
    "Compare reinforcement learning vs. evolutionary search for prompt optimization. If our “policy” is the prompt text and the “environment” gives a reward (quality score), RL would tweak the prompt based on gradient of reward (if possible) or black-box optimization. Evolutionary methods like GEPA/EvoPrompt treat it like a search over strings. List pros and cons of each: e.g., RL (with methods like RLPrompt or policy gradients) can directly optimize an objective but may get stuck in local optima or require many samples; evolutionary approaches are more global and can incorporate heuristic knowledge (via LLM reflections in GEPA) but might be slower if search space is huge. In practice, why might GEPA’s ability to incorporate natural language reflections be advantageous in prompt tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c42ade",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8475c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fce57b8",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2da32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
