{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412d336c",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945db9ea",
   "metadata": {},
   "source": [
    "Recreate the RLHF loop in your own words: first supervised fine-tune (SFT) a model, then train a reward model on comparisons, then run PPO. Why is the KL divergence term (keeping $\\pi_\\theta$ close to a reference model $\\pi_{\\text{ref}}$) crucial? What happens if $\\beta$ (the KL penalty coefficient) is set too low or zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793827ed",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ed448",
   "metadata": {},
   "source": [
    "**RLHF Loop**\n",
    "\n",
    "1. **SFT:** Start with a model that already follows instructions and has a decent style.\n",
    "\n",
    "2. **Reward Modeling:** Train a Reward Model (RM) on human preference comparisons between candidate answers. Freeze the RM after this step, and use it to score rewards on the rollouts.\n",
    "\n",
    "3. **PPO:** Run PPO to push the policy toward outputs that the RM scores higher. PPO trains two networks: an **actor** (the policy) and a **critic** (the **value function**). The critic is not another reward model; it learns to predict the expected shaped return (reward minus KL) and is trained continuously (not frozen), alongside the actor.\n",
    "\n",
    "The **KL term** relative to the reference model transforms PPO into a constrained optimization problem: *“maximize reward while staying close to the original model.”*\n",
    "\n",
    "### Consequences of a Low (\\beta)\n",
    "\n",
    "If $beta$ is too low or zero, the policy will drift aggressively toward patterns that “spike” the reward model, even if they decrease real-world usefulness. This results in reward hacking, characterized by:\n",
    "\n",
    "* **Repeated Structures:** Exploiting structural patterns the RM likes.\n",
    "\n",
    "* **Confident Nonsense:** Hallucinating facts that sound convincing.\n",
    "\n",
    "* **Evasive Disclaimers:** Over-relying on safe but unhelpful responses.\n",
    "\n",
    "This drift can create a feedback loop: the RM score increases while human preference decreases, leading to instability, mode collapse, and degraded general language quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ee61c",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5c607",
   "metadata": {},
   "source": [
    "Coding: Implement the Generalized Advantage Estimator (GAE) for PPO. Given a sequence of rewards and value estimates, code the calculation of advantages with a decay parameter $\\lambda$. Verify your implementation on a small synthetic sequence by comparing to the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a10e7",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46626875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages: [ 0.99829195  0.2119     -0.2         2.196     ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_gae(rewards, values, dones=None, gamma=0.99, lam=0.95):\n",
    "    \"\"\"Generalized Advantage Estimation (GAE-λ) — backward recursion.\n",
    "\n",
    "    Formula (recursive form):\n",
    "        δ_t = r_t + γ · V(s_{t+1}) · (1 - d_t) − V(s_t)\n",
    "        A_t = δ_t + γ · λ · (1 - d_t) · A_{t+1}\n",
    "\n",
    "    Complexity: O(T) time, O(T) space.\n",
    "\n",
    "    Args:\n",
    "        rewards: (T,) array of rewards at each timestep.\n",
    "        values:  (T+1,) array of value estimates; includes bootstrap V(s_T).\n",
    "        dones:   (T,) array, 1 if episode ended after step t (optional).\n",
    "        gamma:   Discount factor (default 0.99).\n",
    "        lam:     GAE λ for bias-variance tradeoff (default 0.95).\n",
    "\n",
    "    Returns:\n",
    "        adv: (T,) array of advantage estimates.\n",
    "    \"\"\"\n",
    "    rewards = np.asarray(rewards, dtype=np.float64)\n",
    "    values = np.asarray(values, dtype=np.float64)\n",
    "    T = rewards.shape[0]\n",
    "\n",
    "    if dones is None:\n",
    "        dones = np.zeros(T, dtype=np.float64)\n",
    "    else:\n",
    "        dones = np.asarray(dones, dtype=np.float64)\n",
    "\n",
    "    adv = np.zeros(T, dtype=np.float64)\n",
    "    gae = 0.0\n",
    "\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        nonterminal = 1.0 - dones[t]\n",
    "        delta = rewards[t] + gamma * values[t + 1] * nonterminal - values[t]\n",
    "        gae = delta + gamma * lam * nonterminal * gae\n",
    "        adv[t] = gae\n",
    "\n",
    "    return adv \n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "rewards = np.array([1.0, 0.5, -0.2, 2.0])       # T=4\n",
    "values = np.array([0.3, 0.1, 0.0, 0.2, 0.4])    # T+1=5 (includes bootstrap)\n",
    "dones = np.array([0, 0, 1, 0])                  # episode ends after t=2\n",
    "\n",
    "advantages = compute_gae(rewards, values, dones=dones, gamma=0.99, lam=0.95)\n",
    "print(\"advantages:\", advantages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094ee8a",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589ed77",
   "metadata": {},
   "source": [
    "Experiment: (Thought experiment or optional coding) Consider an LLM fine-tuned with PPO on a reward model that highly values verbosity. Describe how the generated outputs might drift if $\\beta$ is not high enough. How would you detect reward hacking in practice (e.g., the model finds loopholes in the reward)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d714a",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed3930",
   "metadata": {},
   "source": [
    "If PPO is optimizing a reward model that strongly prefers verbosity and longer answers. We can diagnose with the following ways:\n",
    "\n",
    "1. Inspect extremes: review top-scoring and bottom-scoring samples\n",
    "2. Held-out evals: run fixed benchmarks / prompt suites not used in the RL/RM loop (true hold-out), watching for regressions in instruction-following, factuality, calibration, and length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1602b83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
