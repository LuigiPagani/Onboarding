{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa418ae",
   "metadata": {},
   "source": [
    "# Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053a091",
   "metadata": {},
   "source": [
    "Implement a LinUCBAgent class in Python. Use efficient updates (e.g. Sherman-Morrison formula) to achieve $O(d^2)$ per-step updates instead of naive $O(d^3)$. Test it on a simulation with a few arms and measure regret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b38df",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### **Assumptions**\n",
    "\n",
    "1. Linear reward model: \n",
    "\n",
    "For each round $t$, each arm $a$ has a feature vector:\n",
    "$$\n",
    "x_{t,a}\\in\\mathbb{R}^d\n",
    "$$\n",
    "There exists an **unknown** parameter $\\theta_\\star\\in\\mathbb{R}^d$ such that the **conditional mean reward** is linear:\n",
    "$$\n",
    "\\mathbb{E}[r_t(a)\\mid x_{t,a}] = x_{t,a}^\\top \\theta_\\star\n",
    "$$\n",
    "Observed reward is:\n",
    "$$\n",
    "r_t = x_{t,a_t}^\\top\\theta_\\star + \\eta_t\n",
    "$$\n",
    "\n",
    "2. **Noise assumption:**\n",
    "\n",
    "$\\eta_t$ is zero-mean, **conditionally sub-Gaussian**:\n",
    "$$\n",
    "\\mathbb{E}[\\eta_t\\mid \\mathcal{F}_{t-1}] = 0,\\quad \\eta_t \\text{ is } \\sigma\\text{-sub-Gaussian}\n",
    "$$\n",
    "\n",
    "3. **Bounded features:**\n",
    "\n",
    "Assume feature vectors have bounded norm:\n",
    "$$\n",
    "\\|x_{t,a}\\|_2 \\le L\n",
    "$$\n",
    "Without this, uncertainty can blow up.\n",
    "\n",
    "4. **Bounded parameter:** \n",
    "\n",
    "Often assume:\n",
    "$$\n",
    "\\|\\theta_\\star\\|_2 \\le S\n",
    "$$\n",
    "This is not strictly necessary but allows to define the confidence radius $\\beta_t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b57b5",
   "metadata": {},
   "source": [
    "## Math Prerequisites:\n",
    "\n",
    "1. **The Sherman–Morrison formula:**\n",
    "\n",
    "For an invertible matrix $A \\in \\mathbb R^{d\\times d}$ and vectors $u,v \\in \\mathbb R^d$,\n",
    "\n",
    "$$\n",
    "(A + u v^\\top)^{-1} = A^{-1} - \\frac{A^{-1} u v^\\top A^{-1}}{1 + v^\\top A^{-1} u}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e2a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True theta_star: [0.5 0.3 0.1]\n",
      "\n",
      "Selected arm: 1\n",
      "Feature vector of chosen arm: [4. 5. 6.]\n",
      "Observed reward: r = [4. 5. 6.] @ [0.5 0.3 0.1] + 0.1 = 4.200\n",
      "\n",
      "Updated theta_hat: [0.21538462 0.26923077 0.32307692]\n",
      "True theta_star:   [0.5 0.3 0.1]\n",
      "Difference:        [-0.28461538 -0.03076923  0.22307692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from linucb import LinUCBAgent\n",
    "except ImportError:  \n",
    "    from Day_1.linucb import LinUCBAgent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup: 2 arms, each with 3 features\n",
    "    d, K = 3, 2\n",
    "    agent = LinUCBAgent(d=d, alpha=1.0, lam=1.0)\n",
    "\n",
    "    # True unknown parameter (what we want to discover)\n",
    "    theta_star = np.array([0.5, 0.3, 0.1])\n",
    "    print(f\"True theta_star: {theta_star}\")\n",
    "\n",
    "    # Contexts for 2 arms (concrete numbers)\n",
    "    X = np.array([\n",
    "        [1.0, 2.0, 3.0],  # arm 0 features\n",
    "        [4.0, 5.0, 6.0]   # arm 1 features\n",
    "    ])\n",
    "\n",
    "    # Step 1: Select an arm\n",
    "    a = agent.select_arm(X)\n",
    "    print(f\"\\nSelected arm: {a}\")\n",
    "    print(f\"Feature vector of chosen arm: {X[a]}\")\n",
    "\n",
    "    # Step 2: Calculate reward from environment: r = x^T theta_star + noise\n",
    "    noise = 0.1  # small noise\n",
    "    r = X[a] @ theta_star + noise\n",
    "    print(f\"Observed reward: r = {X[a]} @ {theta_star} + {noise} = {r:.3f}\")\n",
    "\n",
    "    # Step 3: Update agent with chosen arm's context and reward\n",
    "    agent.update(X[a], r)\n",
    "\n",
    "    # Step 4: Check updated parameter estimate\n",
    "    theta_hat = agent.theta_hat()\n",
    "    print(f\"\\nUpdated theta_hat: {theta_hat}\")\n",
    "    print(f\"True theta_star:   {theta_star}\")\n",
    "    print(f\"Difference:        {theta_hat - theta_star}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0037c",
   "metadata": {},
   "source": [
    "\n",
    "## Problem\n",
    "Derive the regret bound $R_T = O(d\\sqrt{T})$ (more precisely $\\tilde O(d\\sqrt T)$) for a linear contextual/linear bandit using LinUCB/OFUL-style analysis. Explain:\n",
    "*   the role of confidence ellipsoids,\n",
    "*   why dimension $d$ worsens worst-case regret,\n",
    "*   how to choose $\\beta_t$ to get a uniform high-probability guarantee, and why $\\beta_t$ is nondecreasing.\n",
    "\n",
    "### 1. Model, notation, and estimator\n",
    "At each round $t=1,\\dots,T$:\n",
    "1.  A set of arm feature vectors $\\{x_{t,a}\\in\\mathbb R^d: a\\in\\mathcal A\\}$ is observed.\n",
    "2.  An arm $a_t$ is selected, and the reward is observed\n",
    "    $$\n",
    "    r_t = x_{t,a_t}^\\top\\theta_\\star + \\eta_t,\n",
    "    $$\n",
    "    where $\\theta_\\star\\in\\mathbb R^d$ is unknown.\n",
    "\n",
    "Define the played feature vector $x_t := x_{t,a_t}$.\n",
    "\n",
    "\n",
    "**Ridge design matrix and estimator**\n",
    "$$\n",
    "V_t := \\lambda I + \\sum_{s=1}^{t-1} x_s x_s^\\top,\n",
    "\\qquad\n",
    "\\hat\\theta_t := V_t^{-1}\\sum_{s=1}^{t-1} x_s r_s.\n",
    "$$\n",
    "Because $\\lambda>0$, $V_t\\succ 0$ and $V_t^{-1}$ exists.\n",
    "\n",
    "### 2. Confidence ellipsoids and $\\beta_t$ (probability guarantee)\n",
    "Define the (random) confidence ellipsoid:\n",
    "$$\n",
    "\\mathcal C_t := \\{ \\theta:\\ \\|\\theta-\\hat\\theta_t\\|_{V_t}\\le \\beta_t \\},\n",
    "\\quad \\|u\\|_{V}:=\\sqrt{u^\\top V u}.\n",
    "$$\n",
    "\n",
    "### What \"$\\theta_\\star\\in\\mathcal C_t$\" means (frequentist)\n",
    "$\\theta_\\star$ is fixed; $\\mathcal C_t$ is random (depends on data).\n",
    "$\\Pr(\\theta_\\star\\in\\mathcal C_t)\\ge 1-\\delta$ means: across repeated runs, the constructed set contains $\\theta_\\star$ at least $1-\\delta$ fraction of the time.\n",
    "\n",
    "We want a single event\n",
    "$$\n",
    "\\mathcal E := \\{ \\forall t\\le T:\\ \\theta_\\star\\in\\mathcal C_t \\}\n",
    "$$\n",
    "with $\\Pr(\\mathcal E)\\ge 1-\\delta$. We do not multiply per-step probabilities (events are not independent). Instead we use a union bound:\n",
    "$$\n",
    "\\Pr(\\exists t\\le T:\\ \\theta_\\star\\notin\\mathcal C_t)\n",
    "=\\Pr\\Big(\\bigcup_{t=1}^T E_t^c\\Big)\n",
    "\\le \\sum_{t=1}^T \\Pr(E_t^c),\n",
    "\\quad E_t:=\\{\\theta_\\star\\in\\mathcal C_t\\}.\n",
    "$$\n",
    "\n",
    "A standard valid choice of $\\beta_t$\n",
    "A self-normalized concentration inequality for linear regression with sub-Gaussian martingale noise implies that for any fixed $\\delta\\in(0,1)$, with probability at least $1-\\delta$,\n",
    "$$\n",
    "\\forall t\\le T:\\quad \\|\\hat\\theta_t-\\theta_\\star\\|_{V_t}\\le\n",
    "\\sigma\\sqrt{2\\log\\frac{1}{\\delta} + \\log\\frac{\\det(V_t)}{\\det(\\lambda I)}}\n",
    "+ \\sqrt{\\lambda}S.\n",
    "$$\n",
    "Thus the following choice is valid:\n",
    "$$\n",
    "\\boxed{\n",
    "\\beta_t(\\delta)\n",
    "=\n",
    "\\sigma\\sqrt{2\\log\\frac{1}{\\delta} + \\log\\frac{\\det(V_t)}{\\det(\\lambda I)}}\n",
    "+ \\sqrt{\\lambda}S.\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "### Why $\\beta_t$ is nondecreasing\n",
    "$V_{t+1}=V_t+x_t x_t^\\top\\succeq V_t \\Rightarrow \\det(V_{t+1})\\ge \\det(V_t)\\Rightarrow \\log\\det(V_t)$ is nondecreasing.\n",
    "\n",
    "### 3. LinUCB rule and role of confidence ellipsoids\n",
    "Define the UCB score for any arm $a$:\n",
    "$$\n",
    "\\mathrm{UCB}_t(a):= x_{t,a}^\\top\\hat\\theta_t + \\beta_t\\|x_{t,a}\\|_{V_t^{-1}},\n",
    "\\quad \\|x\\|_{V^{-1}}:=\\sqrt{x^\\top V^{-1}x}.\n",
    "$$\n",
    "LinUCB chooses\n",
    "$$\n",
    "a_t\\in\\arg\\max_{a\\in\\mathcal A}\\mathrm{UCB}_t(a).\n",
    "$$\n",
    "\n",
    "**Why this is the right bonus:** the ellipsoid gives a closed-form bound on how much $x^\\top\\theta$ can change when $\\theta$ varies within $\\mathcal C_t$. Specifically,\n",
    "$$\n",
    "\\max_{\\theta\\in\\mathcal C_t} x^\\top\\theta\n",
    "= x^\\top\\hat\\theta_t + \\beta_t\\|x\\|_{V_t^{-1}}.\n",
    "$$\n",
    "So LinUCB is \"optimistic over plausible parameters.\"\n",
    "\n",
    "\n",
    "### 4. Define regret\n",
    "Let the optimal arm at time $t$ be\n",
    "$$\n",
    "a_t^\\star\\in\\arg\\max_{a} x_{t,a}^\\top\\theta_\\star.\n",
    "$$\n",
    "Instantaneous regret:\n",
    "$$\n",
    "\\Delta_t := x_{t,a_t^\\star}^\\top\\theta_\\star - x_{t,a_t}^\\top\\theta_\\star.\n",
    "$$\n",
    "Cumulative regret:\n",
    "$$\n",
    "R_T := \\sum_{t=1}^T \\Delta_t.\n",
    "$$\n",
    "Work on the high-probability event $\\mathcal E=\\{\\forall t\\le T:\\theta_\\star\\in\\mathcal C_t\\}$.\n",
    "\n",
    "**First inequality:** from $\\theta_\\star\\in\\mathcal C_t$ to an upper bound on $x^\\top\\theta_\\star$\n",
    "**Claim:** On $\\theta_\\star\\in\\mathcal C_t$, for any vector $x$,\n",
    "$$\n",
    "x^\\top\\theta_\\star \\le x^\\top\\hat\\theta_t + \\beta_t\\|x\\|_{V_t^{-1}}.\n",
    "\\tag{A}\n",
    "$$\n",
    "**Proof:**\n",
    "Because $\\theta_\\star\\in\\mathcal C_t$, we have\n",
    "$$\n",
    "\\|\\theta_\\star-\\hat\\theta_t\\|_{V_t}\\le\\beta_t.\n",
    "\\tag{C}\n",
    "$$\n",
    "Write\n",
    "$$\n",
    "x^\\top\\theta_\\star = x^\\top\\hat\\theta_t + x^\\top(\\theta_\\star-\\hat\\theta_t).\n",
    "$$\n",
    "Now apply Cauchy–Schwarz:\n",
    "Thus\n",
    "$$\n",
    "x^\\top(\\theta_\\star-\\hat\\theta_t)\\le \\beta_t\\|x\\|_{V_t^{-1}}.\n",
    "$$\n",
    "Plugging back gives (A).\n",
    "\n",
    "Apply (A) with $x=x_{t,a_t^\\star}$:\n",
    "$$\n",
    "x_{t,a_t^\\star}^\\top\\theta_\\star\n",
    "\\le x_{t,a_t^\\star}^\\top\\hat\\theta_t + \\beta_t\\|x_{t,a_t^\\star}\\|_{V_t^{-1}}.\n",
    "\\tag{A($\\star$)}\n",
    "$$\n",
    "\n",
    "### 4.2 Second inequality: LinUCB maximization (\"optimism\")\n",
    "By definition of $a_t$ as the maximizer of $\\mathrm{UCB}_t(\\cdot)$,\n",
    "$$\n",
    "x_{t,a_t^\\star}^\\top\\hat\\theta_t + \\beta_t\\|x_{t,a_t^\\star}\\|_{V_t^{-1}}\n",
    "\\le\n",
    "x_{t,a_t}^\\top\\hat\\theta_t + \\beta_t\\|x_{t,a_t}\\|_{V_t^{-1}}.\n",
    "\\tag{B}\n",
    "$$\n",
    "Combine (A($\\star$)) and (B):\n",
    "$$\n",
    "x_{t,a_t^\\star}^\\top\\theta_\\star\n",
    "\\le\n",
    "x_{t,a_t}^\\top\\hat\\theta_t + \\beta_t\\|x_{t,a_t}\\|_{V_t^{-1}}.\n",
    "\\tag{D}\n",
    "$$\n",
    "Subtract $x_{t,a_t}^\\top\\theta_\\star$ from both sides:\n",
    "$$\n",
    "\\Delta_t\n",
    "\\le\n",
    "x_{t,a_t}^\\top(\\hat\\theta_t-\\theta_\\star) + \\beta_t\\|x_{t,a_t}\\|_{V_t^{-1}}.\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "### 4.3 Bound the estimation-error term by the same bonus\n",
    "**Deterministic inequality (always true):**\n",
    "For any $x$ and any vector $u$,\n",
    "$$\n",
    "x^\\top u \\le \\|x\\|_{V_t^{-1}}\\cdot\\|u\\|_{V_t}.\n",
    "\\tag{CS-V}\n",
    "$$\n",
    "Proof is identical to the one used above (insert $V_t^{\\pm1/2}$ and apply Euclidean Cauchy–Schwarz).\n",
    "\n",
    "**Use the confidence event:**\n",
    "On $\\mathcal E$, $\\|\\hat\\theta_t-\\theta_\\star\\|_{V_t}\\le\\beta_t$. So with $x=x_{t,a_t}$ and $u=\\hat\\theta_t-\\theta_\\star$,\n",
    "$$\n",
    "x_{t,a_t}^\\top(\\hat\\theta_t-\\theta_\\star)\n",
    "\\le\n",
    "\\|x_{t,a_t}\\|_{V_t^{-1}}\\cdot\\|\\hat\\theta_t-\\theta_\\star\\|_{V_t}\n",
    "\\le\n",
    "\\beta_t\\|x_{t,a_t}\\|_{V_t^{-1}}.\n",
    "\\tag{2a}\n",
    "$$\n",
    "Plug into (1):\n",
    "$$\n",
    "\\Delta_t \\le 2\\beta_t\\|x_{t,a_t}\\|_{V_t^{-1}}.\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "### 4.4 Sum over time and replace $\\beta_t$ by $\\beta_T$\n",
    "Let $x_t=x_{t,a_t}$. Summing (2):\n",
    "$$\n",
    "R_T = \\sum_{t=1}^T \\Delta_t\n",
    "\\le 2\\sum_{t=1}^T \\beta_t\\|x_t\\|_{V_t^{-1}}.\n",
    "$$\n",
    "Because $\\beta_t$ is nondecreasing, $\\beta_t\\le \\beta_T$ for all $t\\le T$, hence\n",
    "$$\n",
    "R_T \\le 2\\beta_T\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}.\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "## 5) Control $\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}$ via elliptical potential\n",
    "This part is deterministic. It assumes only:\n",
    "1.  $V_{t+1}=V_t+x_t x_t^\\top$,\n",
    "2.  $\\lambda>0$ so $V_t\\succ0$.\n",
    "\n",
    "### 5.1 Determinant recursion (matrix determinant lemma)\n",
    "Matrix determinant lemma:\n",
    "$$\n",
    "\\det(V_t + x_t x_t^\\top)=\\det(V_t)\\big(1+x_t^\\top V_t^{-1}x_t\\big).\n",
    "$$\n",
    "So\n",
    "$$\n",
    "\\det(V_{t+1})=\\det(V_t)\\big(1+s_t\\big),\n",
    "\\quad s_t:=x_t^\\top V_t^{-1}x_t=\\|x_t\\|_{V_t^{-1}}^2.\n",
    "$$\n",
    "Telescoping products:\n",
    "$$\n",
    "\\frac{\\det(V_{T+1})}{\\det(V_1)}=\\prod_{t=1}^T (1+s_t).\n",
    "$$\n",
    "Taking logs and using $V_1=\\lambda I$:\n",
    "$$\n",
    "\\log\\frac{\\det(V_{T+1})}{\\det(\\lambda I)}\n",
    "= \\sum_{t=1}^T \\log(1+s_t).\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "### 5.2 Elliptical Potential Inequality (from $\\log(1+u)\\ge u/2$ on $[0,1]$)\n",
    "\n",
    "**General bound**: For any $s_t \\ge 0$,\n",
    "$$\n",
    "\\sum_{t=1}^T \\min\\{1,s_t\\}\\ \\le\\ 2\\sum_{t=1}^T \\log(1+s_t)\n",
    "=2\\log\\frac{\\det(V_{T+1})}{\\det(\\lambda I)}.\n",
    "\\tag{5a}\n",
    "$$\n",
    "\n",
    "**Simplifying condition**: If we enforce $\\lambda \\ge L^2$, then for all $t$ we have $s_t \\le 1$. Here's why:\n",
    "\n",
    "- Since $V_t = \\lambda I + \\sum_{s=1}^{t-1} x_s x_s^\\top \\succeq \\lambda I$, matrix inversion reverses the PSD order: $V_t^{-1} \\preceq (\\lambda I)^{-1} = \\frac{1}{\\lambda}I$.\n",
    "- Using the feature bound $\\|x_t\\|_2 \\le L$:\n",
    "  $$\n",
    "  s_t = x_t^\\top V_t^{-1} x_t \\le \\frac{\\|x_t\\|_2^2}{\\lambda} \\le \\frac{L^2}{\\lambda} \\le 1.\n",
    "  $$\n",
    "\n",
    "**Consequence**: With $\\lambda \\ge L^2$, $\\min\\{1,s_t\\}=s_t$, so (5a) becomes:\n",
    "$$\n",
    "\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}^2 = \\sum_{t=1}^T s_t \\le 2\\log\\frac{\\det(V_{T+1})}{\\det(\\lambda I)}.\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "*(Not assuming $\\lambda \\ge L^2$? Keep $\\min\\{1,s_t\\}$; it doesn't affect the $\\tilde O(d\\sqrt{T})$ final rate.)*\n",
    "\n",
    "### 5.3 Cauchy–Schwarz to convert sum of squares to sum\n",
    "Let $a_t=\\|x_t\\|_{V_t^{-1}}$. Then\n",
    "$$\n",
    "\\left(\\sum_{t=1}^T a_t\\right)^2 \\le \\left(\\sum_{t=1}^T 1\\right)\\left(\\sum_{t=1}^T a_t^2\\right)=T\\sum_{t=1}^T a_t^2,\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}\n",
    "\\le\n",
    "\\sqrt{T\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}^2}.\n",
    "\\tag{CS}\n",
    "$$\n",
    "Combine (CS) with (5):\n",
    "$$\n",
    "\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}\n",
    "\\le\n",
    "\\sqrt{2T\\log\\frac{\\det(V_{T+1})}{\\det(\\lambda I)}}.\n",
    "\\tag{6a}\n",
    "$$\n",
    "\n",
    "## 6) Bound the log-det by $d\\log(1+TL^2/\\lambda)$\n",
    "Start from\n",
    "$$\n",
    "V_{T+1}=\\lambda I+\\sum_{t=1}^T x_t x_t^\\top\n",
    "=\\lambda\\left(I+\\frac{1}{\\lambda}\\sum_{t=1}^T x_t x_t^\\top\\right).\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "\\log\\frac{\\det(V_{T+1})}{\\det(\\lambda I)}\n",
    "= \\log\\det\\left(I+A\\right),\n",
    "\\quad\n",
    "A:=\\frac{1}{\\lambda}\\sum_{t=1}^T x_t x_t^\\top\\succeq 0.\n",
    "$$\n",
    "Let eigenvalues of $A$ be $\\mu_1,\\dots,\\mu_d\\ge 0$. Then\n",
    "$$\n",
    "\\log\\det(I+A)=\\sum_{i=1}^d \\log(1+\\mu_i).\n",
    "$$\n",
    "Use AM–GM on $1+\\mu_i$:\n",
    "$$\n",
    "\\prod_{i=1}^d (1+\\mu_i)\\le\n",
    "\\left(\\frac{1}{d}\\sum_{i=1}^d(1+\\mu_i)\\right)^d\n",
    "= \\left(1+\\frac{1}{d}\\sum_{i=1}^d \\mu_i\\right)^d.\n",
    "$$\n",
    "Take logs and note $\\sum_i\\mu_i=\\mathrm{tr}(A)$:\n",
    "$$\n",
    "\\log\\det(I+A)\\le d\\log\\left(1+\\frac{\\mathrm{tr}(A)}{d}\\right).\n",
    "$$\n",
    "Now\n",
    "$$\n",
    "\\mathrm{tr}(A)=\\frac{1}{\\lambda}\\sum_{t=1}^T \\mathrm{tr}(x_t x_t^\\top)\n",
    "=\\frac{1}{\\lambda}\\sum_{t=1}^T \\|x_t\\|_2^2\n",
    "\\le \\frac{TL^2}{\\lambda}.\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "\\log\\frac{\\det(V_{T+1})}{\\det(\\lambda I)}\n",
    "\\le d\\log\\left(1+\\frac{TL^2}{\\lambda d}\\right)\n",
    "\\le d\\log\\left(1+\\frac{TL^2}{\\lambda}\\right).\n",
    "\\tag{7}\n",
    "$$\n",
    "Plug (7) into (6a):\n",
    "$$\n",
    "\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}\n",
    "\\le\n",
    "\\sqrt{2T\\cdot d\\log\\left(1+\\frac{TL^2}{\\lambda}\\right)}.\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "## 7) Final regret bound\n",
    "Combine (3) and (8):\n",
    "$$\n",
    "R_T\n",
    "\\le\n",
    "2\\beta_T\\sqrt{2T\\cdot d\\log\\left(1+\\frac{TL^2}{\\lambda}\\right)}.\n",
    "\\tag{9}\n",
    "$$\n",
    "Using the explicit $\\beta_T$ (e.g. from $(\\beta)$ and the same log-det bound),\n",
    "$$\n",
    "\\beta_T\n",
    "= \\sigma\\sqrt{2\\log\\frac{1}{\\delta} + \\log\\frac{\\det(V_T)}{\\det(\\lambda I)}}\n",
    "+ \\sqrt{\\lambda}S\n",
    "\\le\n",
    "\\sigma\\sqrt{2\\log\\frac{1}{\\delta} + d\\log\\left(1+\\frac{TL^2}{\\lambda}\\right)}\n",
    "+ \\sqrt{\\lambda}S.\n",
    "$$\n",
    "Hence (9) implies\n",
    "$$\n",
    "R_T = \\tilde O(d\\sqrt T)\n",
    "$$\n",
    "with probability at least $1-\\delta$ (or $1-\\delta$ uniformly over time depending on whether $\\delta$ or $\\delta/T$ / anytime $\\delta_t$ is used).\n",
    "This is the requested $O(d\\sqrt T)$ up to logs.\n",
    "\n",
    "## 8) Role of confidence ellipsoids (what they do in the proof)\n",
    "The confidence ellipsoid provides two indispensable ingredients:\n",
    "\n",
    "1.  A uniform statement about parameter error:\n",
    "    $$\n",
    "    \\|\\hat\\theta_t-\\theta_\\star\\|_{V_t}\\le \\beta_t\n",
    "    \\quad(\\text{with high probability}).\n",
    "    $$\n",
    "2.  A conversion from parameter uncertainty to reward uncertainty for any arm feature $x$:\n",
    "    $$\n",
    "    x^\\top\\theta_\\star \\le x^\\top\\hat\\theta_t + \\beta_t\\|x\\|_{V_t^{-1}}.\n",
    "    $$\n",
    "\n",
    "This is exactly what yields the UCB exploration bonus and enables the \"optimism chain.\"\n",
    "Without the ellipsoid geometry, the clean $\\|x\\|_{V_t^{-1}}$ term is not available, and its sum is not directly controllable via the determinant argument.\n",
    "\n",
    "## 9) Why dimension $d$ worsens worst-case regret\n",
    " \n",
    "The dimension $d$ appears in the regret bound in two key places:\n",
    " \n",
    "1. **Sum of exploration bonuses**: The sum $\\sum_{t=1}^T \\|x_t\\|_{V_t^{-1}}$ is bounded by $O(\\sqrt{Td\\log T})$ via the log-determinant argument. The factor of $d$ arises because $\\log\\det(V_T) - \\log\\det(\\lambda I)$ scales as $d\\log(1 + TL^2/\\lambda)$.\n",
    " \n",
    "2. **Confidence width $\\beta_T$**: The confidence parameter $\\beta_T$ contains a term $\\sqrt{d\\log(\\cdot)}$ from the self-normalized martingale bound.\n",
    " \n",
    " **Intuition**: In higher dimensions, there are more directions in parameter space that need to be explored. Each new direction adds uncertainty that must be resolved through data. With $d$ parameters to estimate, more observations are required to pin down $\\theta_\\star$ in all directions, leading to larger regret.\n",
    " \n",
    "Mathematically, the eigenvalues of $V_t^{-1}$ decay more slowly when $d$ is large (observations are \"spread thinner\" across dimensions), so the exploration bonuses $\\|x_t\\|_{V_t^{-1}}$ remain larger for longer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844db799",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9643c07",
   "metadata": {},
   "source": [
    "Application: Consider a simplified Sqwish scenario with 3 prompt variants. Formulate it as a contextual bandit: define the context (e.g. user embedding), actions (prompts), and reward (e.g. conversion vs no conversion). How would UCB decide which prompt to show early on versus after gathering data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052dff2",
   "metadata": {},
   "source": [
    "## Solution: Sqwish as a Contextual Bandit\n",
    "\n",
    "### Formulation\n",
    "\n",
    "**Context** $x_t \\in \\mathbb{R}^{d_{user}}$: User embedding/features available before showing a prompt (e.g., user profile, session info, device, time of day).\n",
    "\n",
    "**Actions** $a \\in \\{0, 1, 2\\}$: The 3 prompt variants to choose from.\n",
    "\n",
    "**Reward** $r_t \\in [0, R_{\\max}]$: Continuous spend (e.g., dollars spent after seeing the prompt), clipped to the range $[0, R_{\\max}]$ (negative values become 0, values above $R_{\\max}$ are capped).\n",
    "\n",
    "> Note: Clipping spend into a bounded range $[0, R_{\\max}]$ makes the environment no longer perfectly linear (both the lower bound at 0 and upper bound at $R_{\\max}$ introduce non-linearity).\n",
    "\n",
    "### Feature construction (user + one-hot prompt)\n",
    "\n",
    "For each prompt $a$, build a combined feature vector:\n",
    "$$\n",
    "x_{t,a} = [\\text{user\\_features}; \\text{onehot}(a)] \\in \\mathbb{R}^{d_{user} + 3}\n",
    "$$\n",
    "\n",
    "Example with $d_{user}=4$ and 3 prompts:\n",
    "- Prompt 0: $x_{t,0} = [u_1, u_2, u_3, u_4, 1, 0, 0]$\n",
    "- Prompt 1: $x_{t,1} = [u_1, u_2, u_3, u_4, 0, 1, 0]$\n",
    "- Prompt 2: $x_{t,2} = [u_1, u_2, u_3, u_4, 0, 0, 1]$\n",
    "\n",
    "This gives the model the ability to learn different conversion rates per prompt, plus user-prompt interactions.\n",
    "\n",
    "### How UCB decides: early vs late\n",
    "\n",
    "**Early (few observations):**\n",
    "- $A^{-1}$ is large (close to $(1/\\lambda) I$) $\\Rightarrow$ uncertainty bonus $\\alpha\\sqrt{x^\\top A^{-1} x}$ is high.\n",
    "- UCB will **explore**: try prompts that haven't been tested much for similar users.\n",
    "- Even if a prompt has lower estimated mean, its high uncertainty can make it win.\n",
    "\n",
    "**Later (many observations):**\n",
    "- $A^{-1}$ shrinks $\\Rightarrow$ uncertainty bonus becomes small.\n",
    "- UCB **exploits**: picks the prompt with best predicted conversion $x^\\top \\hat\\theta$.\n",
    "- Different users can get different \"best\" prompts (personalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True theta_star: [ 0.2   0.1  -0.1   0.05  0.1   0.3   0.6 ]\n",
      "Prompt biases (last 3 values): prompt0=0.1, prompt1=0.3, prompt2=0.6\n",
      "------------------------------------------------------------\n",
      "After    5 rounds: selections = {0: 2, 1: 3, 2: 0}, cumulative regret = 3.25\n",
      "After   10 rounds: selections = {0: 2, 1: 3, 2: 5}, cumulative regret = 3.47\n",
      "After   25 rounds: selections = {0: 2, 1: 3, 2: 20}, cumulative regret = 4.27\n",
      "After   50 rounds: selections = {0: 2, 1: 3, 2: 45}, cumulative regret = 8.06\n",
      "After  100 rounds: selections = {0: 2, 1: 4, 2: 94}, cumulative regret = 22.58\n",
      "After  500 rounds: selections = {0: 2, 1: 4, 2: 494}, cumulative regret = 96.01\n",
      "After 1000 rounds: selections = {0: 2, 1: 4, 2: 994}, cumulative regret = 183.89\n",
      "------------------------------------------------------------\n",
      "Final theta_hat: [ 0.16   0.087 -0.093  0.029  0.028  0.146  0.631]\n",
      "True theta_star: [ 0.2   0.1  -0.1   0.05  0.1   0.3   0.6 ]\n",
      "Total cumulative regret: 183.89\n"
     ]
    }
   ],
   "source": [
    "# --- Sqwish simulation: 3 prompts, user features + one-hot ---\n",
    "\n",
    "def build_X(user_features: np.ndarray, n_prompts: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build feature matrix X (K x d) from user features + one-hot prompt ID.\n",
    "    \n",
    "    Args:\n",
    "        user_features: (d_user,) array of user features\n",
    "        n_prompts: number of prompt variants\n",
    "    \n",
    "    Returns:\n",
    "        X: (n_prompts, d_user + n_prompts) feature matrix\n",
    "    \"\"\"\n",
    "    d_user = len(user_features)\n",
    "    X = np.zeros((n_prompts, d_user + n_prompts), dtype=np.float64)\n",
    "    for a in range(n_prompts):\n",
    "        X[a, :d_user] = user_features      # user features\n",
    "        X[a, d_user + a] = 1.0             # one-hot for prompt a\n",
    "    return X\n",
    "\n",
    "\n",
    "def simulate_sqwish(n_rounds: int = 50, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Run a simple Sqwish simulation with 3 prompts.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Setup\n",
    "    d_user = 4      # user embedding dimension\n",
    "    n_prompts = 3   # number of prompt variants\n",
    "    d = d_user + n_prompts  # total feature dimension\n",
    "    \n",
    "    # True unknown parameter theta_star\n",
    "    # First d_user dims: user feature weights\n",
    "    # Last 3 dims: prompt-specific biases (prompt 2 is best)\n",
    "    theta_star = np.array([0.2, 0.1, -0.1, 0.05,   # user weights\n",
    "                           0.1, 0.3, 0.6])         # prompt biases: prompt 2 > 1 > 0\n",
    "    \n",
    "    agent = LinUCBAgent(d=d, alpha=1.0, lam=1.0)\n",
    "    \n",
    "    selections = {0: 0, 1: 0, 2: 0}  # count selections per prompt\n",
    "    cumulative_regret = 0.0          # track regret\n",
    "    regrets = []                      # regret history\n",
    "    \n",
    "    print(f\"True theta_star: {theta_star}\")\n",
    "    print(f\"Prompt biases (last 3 values): prompt0={theta_star[d_user]:.1f}, \"\n",
    "          f\"prompt1={theta_star[d_user+1]:.1f}, prompt2={theta_star[d_user+2]:.1f}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for t in range(n_rounds):\n",
    "        # Sample a random user\n",
    "        user_features = rng.normal(size=d_user)\n",
    "        \n",
    "        # Build X: one row per prompt\n",
    "        X = build_X(user_features, n_prompts)\n",
    "        \n",
    "        # Expected reward per prompt (oracle knows theta_star)\n",
    "        expected_rewards = X @ theta_star  # shape (n_prompts,)\n",
    "\n",
    "        # Select prompt using UCB\n",
    "        a = agent.select_arm(X)\n",
    "        selections[a] += 1\n",
    "\n",
    "        # --- Realized regret (simulation-only) ---\n",
    "        # Real regret uses realized rewards:\n",
    "        noise_vec = rng.normal(0, 0.5, size=n_prompts)\n",
    "        raw_spend_all = expected_rewards + noise_vec\n",
    "\n",
    "        max_spend = 10.0\n",
    "        r_all = np.clip(raw_spend_all, 0.0, max_spend).astype(np.float64)\n",
    "        r = float(r_all[a])\n",
    "\n",
    "        instant_regret = float(np.max(r_all) - r)\n",
    "        cumulative_regret += instant_regret\n",
    "        regrets.append(cumulative_regret)\n",
    "\n",
    "        # Update agent with the observed (chosen-arm) realized reward\n",
    "        agent.update(X[a], r)\n",
    "        \n",
    "        # Print progress at checkpoints\n",
    "        if t + 1 in [5, 10, 25, 50, 100, 500, 1000]:\n",
    "            print(f\"After {t+1:4d} rounds: selections = {dict(selections)}, \"\n",
    "                  f\"cumulative regret = {cumulative_regret:.2f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Final theta_hat: {agent.theta_hat().round(3)}\")\n",
    "    print(f\"True theta_star: {theta_star}\")\n",
    "    print(f\"Total cumulative regret: {cumulative_regret:.2f}\")\n",
    "    \n",
    "    return agent, selections, regrets\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "agent, selections, regrets = simulate_sqwish(n_rounds=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}